[
  {
    "objectID": "src/ML/theory/statistics.html",
    "href": "src/ML/theory/statistics.html",
    "title": "통계학 이론",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "통계학 이론"
    ]
  },
  {
    "objectID": "src/ML/theory/statistics.html#베이지안-통계-bayesian-probabilities",
    "href": "src/ML/theory/statistics.html#베이지안-통계-bayesian-probabilities",
    "title": "통계학 이론",
    "section": "1 베이지안 통계 (Bayesian Probabilities)",
    "text": "1 베이지안 통계 (Bayesian Probabilities)\n지금까지 우리는 확률을 무작위성(randomness) 과 반복적인 사건(repeated events) 이라는 고전적(classica) 혹은 빈도적(frequencies) 이라고 불리는 관점에서 봤다. 이제 우리는 확률을 이용하여 불확실성을 정량화하는 Bayesian 관점을 학습할 것이다.\n커브 피팅 혹은 모델링을 생각하자. 즉 측정된 값 \\(\\mathcal{D}=\\{t_1,\\ldots,\\,t_n\\}\\) 을 통해 측정값을 가장 잘 기술하는 함수 \\(y(\\boldsymbol{x};\\,\\boldsymbol{w})\\) 를 결정한다고하자. 여기서 \\(\\boldsymbol{w}\\) 는 모델 매개변수이다. 빈도주의적 입장에서 \\(\\boldsymbol{w}\\) 는 확률이 아닌 우리가 알 내야 하는 값이 된다. 이에 대한 베이즈 정리는 다음과 같다. \\[\np(\\boldsymbol{w}\\,|\\, \\mathcal{D})=\\dfrac{p(\\mathcal{D}\\,|\\,\\boldsymbol{w}) \\cdot p(\\boldsymbol{w})}{p(\\mathcal{D})}.\n\\]\n여기서 \\(p(\\mathcal{D}\\,|\\,\\boldsymbol{w})\\) 를 가능도 혹은 우도 (likelihood function) 라고 하고 \\(p(\\boldsymbol{w})\\) 를 사전 확률 분포 (prior distribution) 라고 한다. \\(p(\\mathcal{D})\\) 는 정규화 상수(normalization constant) 이다. \\(p(\\mathcal{D})\\) 는 실험 결과에 따라 정해지는 확률이기 때문에 실험이 종료된 상황에서는 상수일 뿐이다.\n\n\n1.1 고전적/빈도주의적 입장\n고전적 입장에서는 \\(p(\\boldsymbol{w})=1\\) 이다. 따라서 \\(p(\\boldsymbol{w}|\\mathcal{D})\\) 를 최대화 하는 것은 \\(p(\\mathcal{D}|\\boldsymbol{w})\\) 를 최대화 하는 것이다. 보통 기계학습에서 에러 함수 \\(L(\\boldsymbol{w})=-\\ln p(\\mathcal{D}|\\boldsymbol{w})\\) 이므로 에러 함수를 최소화 하는 것은 가능도를 최대로 하는 것과 동치이다.\n\n\n\n1.2 베이지언적 입장\n매개변수 \\(\\boldsymbol{w}\\) 는 고정된 값이 아닌 확률로 표현되는 값이다. 데이터를 보기 전에 \\(p(\\boldsymbol{w})\\) 에 대해 임시로 정한다. \\(\\mathcal{D}=\\{t_1,\\ldots,\\,t_N\\}\\) 는 \\(p(\\mathcal{D}\\,|\\,\\boldsymbol{w})\\) 에 반영된다.\n\n빈도주의적이든 베이즈적이든 \\(p(\\mathcal{D}|\\boldsymbol{w})\\) 가 중심적인 역할을 하지만 이에 대한 두 입장의 견해는 매우 다르다. 빈도주의 입장에서는 \\(\\boldsymbol{w}\\) 는 고정된 매개변수 이며 그 값과 에러는 \\(\\mathcal{D}\\) 의 분포를 고려하여 얻어진다. 그러나 베이즈주의적 입장에서는 유일한 \\(\\mathcal{D}\\) 가 존재하며 매개변수 \\(\\boldsymbol{w}\\) 가 확률 분포 \\(p(\\boldsymbol{w})\\) 로서 표현된다.\n널리 사용되는 빈도주의자들의 estimator는 최대 가능도 혹은 최대 우도 (maximum likelihood) 이다. 이 입장에서는 \\(p(\\boldsymbol{w})=1\\) 이므로 \\(p(\\mathcal{D}\\,|\\,\\boldsymbol{w})\\) 를 최대화하면 자연스럽게 \\(p(\\boldsymbol{w}\\,|\\,\\mathcal{D})\\) 가 최대화 된다. ML 에서는 \\(-\\ln p(D\\,|\\,\\boldsymbol{w})\\) 를 error function 이라 한다. 따라서 likelihood 를 최대화 하는것은 error function 을 최소화 하는 것이다.\n예를 들어 동전을 던졌을 때 앞면이 나올 확률을 \\(q\\) 라 하자. 세번의 동전을 던져 셋 다 앞면이 나왔을 때, 빈도주의적 접근에 의하면, Likelihood function 은 \\[\np(\\text{3 up}\\,|\\,q)=q^3\n\\] 이므로 \\(p(\\text{3 up}|q)\\) 를 최대화 하는 것은 \\(q=1\\) 이다. 이것은 매우 극단적인 결과이다.\n그런데 베이지언에서는 \\(\\boldsymbol{w}\\) 에 받아들일만 한 사전확률분포 \\(p(\\boldsymbol{w})\\) 을 부여하므로 덜 극단적인 결론에 도달할 수 있다.\n베이지언에 대한 가장 일반적인 비판중의 하나는 사전확률분포 \\(p(\\boldsymbol{w})\\) 를 선택할 때 수학적인 편리성이나 편견에 의해 결과가 왜곡 될 수 있다는 것이다. 이러한 주관성을 개선하기 위해 소위 non-informative priors 가 도입되기도 한다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "통계학 이론"
    ]
  },
  {
    "objectID": "src/ML/theory/statistics.html#the-gaussian-distribution-normal-distribution",
    "href": "src/ML/theory/statistics.html#the-gaussian-distribution-normal-distribution",
    "title": "통계학 이론",
    "section": "2 The Gaussian Distribution (Normal Distribution)",
    "text": "2 The Gaussian Distribution (Normal Distribution)\n평균 (mean) \\(\\mu\\) 와 분산 \\(\\sigma^2\\) 에 대한 1차원 가우시안 분포 \\(\\mathcal{N}(x\\mid \\mu,\\,\\sigma^2)\\) 는 다음과 같다. \\[\n\\mathcal{N} (x\\mid \\mu,\\,\\sigma^2) = \\dfrac{1}{\\sigma \\sqrt{2\\pi }} \\exp \\left[-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right]\n\\tag{1}\\]\n가우시안 분포 \\(\\mathcal{N}(x\\mid \\mu,\\,\\sigma^2)\\) 는 다음과 같은 성질을 갖는다.\n\\[\n\\begin{align}\n\\mathcal{N}&(x\\mid \\mu,\\,\\sigma^2)  \\ge 0\\,,\\\\\n\\int_{-\\infty}^\\infty &\\mathcal{N}(x\\mid \\mu,\\,\\sigma^2)\\, dx = 1,\\, \\\\\n\\mathbb{E}[x] &=\\int_{-\\infty}^\\infty x\\, \\mathcal{N}(x\\mid \\mu,\\,\\sigma^2)\\,dx=\\mu\\;, \\\\\n\\mathbb{E}[x^2] &= \\int_{-\\infty}^\\infty x^2 \\mathcal{N}(x\\mid \\mu,\\,\\sigma^2)\\,dx=\\mu^2+\\sigma^2\\;,\\\\\n\\operatorname{var}[f] &=\\mathbb{E}[x^2]-\\left(\\mathbb{E}[x]\\right)^2=\\sigma^2 \\;.\n\\end{align}\n\\tag{2}\\]\n\\(\\mathbb{R}^\\mathcal{D}\\) 에서 평균 \\(\\boldsymbol{\\mu}\\) 와 공분산 \\(\\boldsymbol{\\Sigma}\\) 를 갖는 가우스 분포는 다음과 같다.\n\\[\n\\mathcal{N}(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma}) = \\dfrac{1}{(2\\pi)^{\\mathcal{D}/2}}\\dfrac{1}{\\left|\\boldsymbol{\\Sigma}\\right|^{1/2}} \\exp \\left[-\\dfrac{1}{2} (\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma} (\\boldsymbol{x}-\\boldsymbol{\\mu})\\right]\n\\tag{3}\\]\n\n\n2.1 1-변수 가우스분포에서의 \\(\\mu\\)와 \\(\\sigma^2\\) 의 추정 - 최대 가능도\n스칼라 변수 \\(x\\) 에 대해 \\(N\\) 번 측정한 것을 \\(\\boldsymbol{x}=\\begin{bmatrix}x_1 &\\ldots &x_N\\end{bmatrix}^T\\) 라 하자. 이 관측은 평균이 \\(\\mu\\) 이며 분산이 \\(\\sigma^2\\) 인 가우시안 분포를 따르는 변수에 대한 각각 독립적인 측정이라고 하자.\n우선 \\(N\\) 측정에서 \\(\\boldsymbol{x}\\) 가 관측될 확률은 \\[\np(\\boldsymbol{x}\\mid \\mu,\\,\\sigma^2)= \\prod_{n=1}^N \\mathcal{N}(x_n\\mid \\mu,\\,\\sigma^2)\n\\tag{4}\\]\n이며 likelihood function for the Gaussian (가우시안 가능도 함수)이라 불리운다.\n어쨋든, 식 4 의 우도함수를 최대화하는 \\(\\mu,\\,\\sigma^2\\) 를 정하자. 계산의 편의를 위해 로그함수를 사용한다. \\[\n\\ln p(\\boldsymbol{x}\\mid \\mu,\\,\\sigma^2)= - \\dfrac{1}{2\\sigma^2}\\sum_{n=1}^N (x_n-\\mu)^2-\\dfrac{N}{2} \\ln \\sigma^2 - \\dfrac{N}{2} \\ln 2\\pi\n\\]\n이 때 \\(p (\\boldsymbol{x}\\mid \\mu,\\,\\sigma^2)\\) 를 최대화 하는 \\(\\mu\\) 와 \\(\\sigma^2\\) 를 \\(\\mu_{ML},\\,\\sigma_{ML}^2\\) 라 할 때 다음과 같다.\n\\[\n\\begin{align}\n\\mu_{ML} &= \\dfrac{1}{N}\\sum_{n=1}^N x_n \\;,\\\\\n\\sigma_{ML}^2 &= \\dfrac{1}{N} \\sum_{n=1}^N (x_n-\\mu_{ML})^2\\;\n\\end{align}\n\\tag{5}\\]\n\n\n편항\n위와 같은 최대 가능도로부터 얻어진 분산은 원래 분포의 분산보다 작은데 하는데 이런 현상을 편향(bias)이라 한다. 표본의 평균과 분산의 기대값은 다음과 같다. \\[\n\\begin{align}\n\\mathbb{E}[\\mu_{ML}]& =\\mu \\\\\n\\mathbb{E}\\left[\\sigma_{ML}^2\\right] & =\\left(\\dfrac{N-1}{N}\\right)\\sigma^2.\n\\end{align}\n\\tag{6}\\]\n식 (1.58)에서 보듯이 \\(\\mathbb{E}\\left[\\sigma^2_{ML}\\right]&lt;\\sigma^2\\) 이다. 따라서 아래와 같이 정의된 \\(\\widetilde{\\sigma\\,}^2\\) 는 samples 로 부터 추정한 모집단의 분산과 같다. (즉 unbiased.) 이를 표본분산이라 한다. \\[\n\\widetilde{\\sigma\\,}^2 = \\dfrac{N}{N-1}\\sigma_{ML}^2 = \\dfrac{1}{N-1} \\sum_{n=1}^N\\left(x_n-\\mu_{ML}\\right)^2\n\\tag{7}\\]\n\\(N\\to \\infty\\) 일 때 \\(\\sigma_{ML}^2 \\to \\sigma^2\\) 임은 쉽게 알 수 있다. 실제로 \\(N\\) 이 작지만 않으면 큰 문제는 되지 않는다.\n\n\n\n\n2.2 Curve Fitting Revisited\n\\(N\\) 개의 입력 변수 \\(\\boldsymbol{x} = \\begin{bmatrix} x_1 &\\ldots & x_N \\end{bmatrix}^T\\) 와 표적값 \\(\\boldsymbol{t}=\\begin{bmatrix} t_1 &\\ldots &t_N\\end{bmatrix}^T\\) 사이에 다항식 \\(t=y(x;\\boldsymbol{w})=w_0 + w_1x+ \\cdots +w_nx^n\\) 의 관계를 가정한다. 표적값의 불확도를 확률분포로서 표현하자. 이를 위해 주어진 \\(x\\) 에 대해 표적값의 확률은 \\(y(\\boldsymbol{x},\\,\\boldsymbol{w})\\) 를 중심으로 분산이 \\(\\beta^{-1}\\) 인 가우시안분포를 따른다고 가정한다. 즉,\n\\[\np(t\\mid x,\\,\\boldsymbol{w},\\, \\beta)=\\mathcal{N}(t\\mid y(x,\\,\\boldsymbol{w}),\\,\\beta^{-1})\n\\tag{8}\\]\n이다.\n훈련 데이터 \\(\\{\\boldsymbol{x},\\,\\boldsymbol{t}\\}\\) 를 이용하여 미지의 매개변수 \\(\\boldsymbol{w}\\) 와 \\(\\beta\\) 를 결정하자. 그렇다면 가능도 함수는 다음과 같이 주어진다. \\[\np(\\boldsymbol{t}\\mid \\boldsymbol{x},\\,\\boldsymbol{w},\\,\\beta)=\\prod_{n=1}^N \\mathcal{N}(t_n\\mid y(x_n,\\, \\boldsymbol{w}),\\, \\beta^{-1}).\n\\tag{9}\\]\n앞서와 같이 \\(\\ln\\) 을 취하면\n\\[\n\\ln p(\\boldsymbol{t}\\mid \\boldsymbol{x},\\, \\boldsymbol{w},\\,\\beta)= -\\dfrac{\\beta}{2} \\sum_{n=1}^N \\left[y(x_n,\\,\\boldsymbol{w})-t_n\\right]^2+\\dfrac{N}{2} \\ln \\beta - \\dfrac{N}{2} \\ln (2\\pi)\n\\tag{10}\\]\n이 된다.\n\n식 10 로부터 고정된 \\(\\beta\\) 에 대해 \\(p(\\boldsymbol{t}\\mid \\boldsymbol{x},\\, \\boldsymbol{w},\\,\\beta)\\) 를 최대화 하는 것과 \\(\\displaystyle \\dfrac{1}{2}\\sum_{n=1}^N \\left[ y(x_n,\\,\\boldsymbol{w})-t_n\\right]^2\\) 를 최소화하는 것, 즉 제곱합 오차를 최소화 하는것은 동치라는 것을 알 수 있다. 이 \\(\\boldsymbol{w}\\) 를 \\(\\boldsymbol{w}_{ML}\\)이라 하자.\n또한 \\(\\beta\\) 에 대해 미분하여 \\(p\\) 를 최대화 하는 \\(\\beta\\) 를 찾아 \\(\\beta_{ML}\\) 이라 하면, \\[\n\\dfrac{1}{\\beta_{ML}}=\\dfrac{1}{N}\\sum_{n=1}^N \\left[ y(x_n,\\, \\boldsymbol{w}_{ML})-t_n\\right]^2\n\\tag{11}\\]\n이다. \\(\\beta\\) 역시 \\(\\boldsymbol{w}_{ML}\\) 이 결정된 상황에서 제곱합 오차를 최소화 할 때 확률을 최대화 하도록 결정된다.\n\n이제 우리는 주어진 데이터로부터 가장 잘 예측할 수 있는 확률 분포를 다음과 갇이 얻는다.\n\\[\np(t\\mid x,\\,\\boldsymbol{w}_{ML},\\, \\beta_{ML})=\\mathcal{N}(t\\mid  y(x,\\,\\boldsymbol{w}_{ML}),\\,\\beta_{ML}^{-1})\n\\tag{12}\\]\n\n\n베이즈 통계를 위한 공식\n\\[\n\\begin{align}\np(a|x)&=\\sum_y p(a| x,\\,y)\\, p(y| x)  \\tag{B1} \\\\\np(a|x,\\,y) &= \\dfrac{p(x|a,\\,y)\\, p(a|y)} {p(y|x)}  \\tag{B2}\n\\end{align}\n\\]\n\n\n(증명). (\\(B1\\)) \\[\n\\begin{aligned}\n\\sum_y p (a\\,|\\, x,\\,y)\\, p(y\\mid x)&=\\sum_y \\dfrac{p(a,\\,x,\\,y)}{p(x,y)} \\cdot \\dfrac{p(x,\\,y)}{p(x)} =\\sum_y \\dfrac{p(a,\\,x,\\,y)}{p(x)} \\\\\n&=\\dfrac{1}{p(x)}\\sum_{y}p(a,\\,x,\\,y) = \\dfrac{p(a,\\,x)}{p(x)} \\\\\n&=p(a|x)\n\\end{aligned}\n\\]\n(\\(B2\\)) \\[\n\\begin{aligned}\n\\dfrac{p(x|a,\\,y)\\, p(a|y)}{p(y|x)} &= \\dfrac{p(a,\\,x,\\,y)}{p(a,\\,y)} \\cdot \\dfrac{p(a,\\,y)}{p(y)} \\cdot \\dfrac{p(y)}{p(x,\\,y)}=\\dfrac{p(a,\\,x,\\,y)}{p(x,\\,y)}=p(a|x,\\,y)\n\\end{aligned}\n\\]\n\n\n\n이제 Bayesian 접근법을 좀 알아보자. 즉 \\(\\boldsymbol{w}\\) 에 대한 사전 분포 \\(p(\\boldsymbol{w})\\) 에 대한 것이다. \\(\\boldsymbol{w}\\) 가 아래와 같은 분포를 따른다고 하자.\n\\[\np(\\boldsymbol{w}|\\alpha)=\\mathcal{N}(\\boldsymbol{w}|\\boldsymbol{0},\\,\\alpha^{-1}\\boldsymbol{1}_{M+1})=\\left(\\dfrac{\\alpha}{2\\pi}\\right)^{(M+1)/2} \\exp \\left(-\\dfrac{\\alpha}{2}\\boldsymbol{w}^T \\boldsymbol{w}\\right)\n\\tag{13}\\]\n여기서 \\(M\\) 은 다항식의 차수이며 이며 따라서 \\(\\boldsymbol{w}\\) 는 \\(M+1\\) 개의 성분을 가진다. \\(\\alpha\\) 와 같이 모델 파라메터의 분포를 제어하는 변수를 초매개변수(hyperparameters) 라 한다.\n베이즈 정리로 부터, \\[\n[\\boldsymbol{w} \\text{ 에 대한 사후 확률}] \\propto [\\text{가능도}]\\times[\\boldsymbol{w}\\text{ 의 사전 확률 분포}]\n\\]\n임을 알고 있으므로,\n\\[\np(\\boldsymbol{w}\\,|\\,\\boldsymbol{x},\\,\\boldsymbol{t},\\,\\alpha,\\,\\beta) \\propto p(\\boldsymbol{t}\\,|\\,\\boldsymbol{x},\\,\\boldsymbol{w},\\,\\beta)\\cdot p(\\boldsymbol{w}\\,|\\,\\alpha)\n\\tag{14}\\]\n이다. 식 14 에 \\(-\\ln\\) 을 취하고 식 10, 식 12 를 대입하면, 사후확률을 극대화 하는 \\(\\boldsymbol{w}\\) 는 다음 식을 최소화 하는 것 \\(\\boldsymbol{w}\\) 이다.\n\\[\n\\dfrac{\\beta}{2}\\sum_{n=1}^N \\{y(x_n,\\,\\boldsymbol{w})-t_n\\}^2+\\dfrac{\\alpha}{2} \\boldsymbol{w}^T \\boldsymbol{w}.\n\\tag{15}\\]\n즉 베이지안에서 사후확률분포를 최대화하는 것은 정규화된 제곱합 오차 함수를 최소화 하는 것과 동등하다.\n\n\n\n2.3 Bayesian Curve Fitting\n앞서 우리는 사전확률분포 \\(p(\\boldsymbol{w}|\\alpha)\\) 에 대한 추정을 포함시켰지만, \\(\\boldsymbol{w}\\) 에 대한 point estimate 이므로 이것은 제대로 된 베이지안 처리가 아니다. 제대로 된 베이지언 처리는 확률에 대한 합과 곱의 규칙들을 일관되게 적용해야 하며, 이는 \\(\\boldsymbol{w}\\) 에 대한 모든 값에 대해 적분해야 함을 의미한다. 이러한 marginalizations 가 패턴 인식에서의 베이지언 방법의 핵심이다.\n\n일단 \\(\\alpha,\\,\\beta\\) 를 고정시키고 (편의를 위해 식에서는 일단 빼자.), test set \\(\\{\\boldsymbol{x},\\,\\boldsymbol{t}\\}\\) 만을 생각하자. 베이지안 방법은\n\\[\np(t\\,|\\,x,\\,\\mathbf{x},\\,\\mathbf{t})=\\int p(t\\mid x,\\,\\mathbf{w})\\, p(\\mathbf{w}\\mid\\mathbf{x},\\,\\mathbf{t})\\,d\\mathbf{w}\n\\tag{16}\\]\n을 생각한다. 여기서 \\(p(t\\mid x,\\,\\boldsymbol{w})\\) 는 식 식 8 에 나와 있으며 \\(p(\\boldsymbol{w}\\mid \\boldsymbol{x},\\,\\boldsymbol{t})\\) 는 사후확률분포이다. (식 14 을 보라.)\n뒤에 보겠지만, curve fitting example 과 같은 문제에서 이 사후확률분포 은 Gaussian 이며 해석적으로 계산 할 수 있다. 비슷하게 식 16 도 해석적으로 적분될 수 있으며 그 결과는 아래와 같은 가우시한 형태로 주어진다.\n\\[\np(t\\mid x,\\,\\boldsymbol{x},\\,\\boldsymbol{t}) =\\mathcal{N}(t\\mid m(x),\\, s^2(x))\n\\]\n여기서 평균 \\(m(x)\\) 와 분산 \\(s^2(x)\\) 는 다음과 같다. \\[\n\\begin{aligned}\nm(x) &=\\beta \\phi(x)^T \\boldsymbol{S} \\sum_{n=1}^N \\boldsymbol{\\phi} (x_n) t_n\\\\\ns^2(x) &=\\beta^{-1}+ \\boldsymbol{\\phi}(x)^T\\boldsymbol{S}\\boldsymbol{\\phi}(x) \\\\\n\\end{aligned}\n\\]\n여기서 행렬 \\(\\boldsymbol{S}\\) 는 다음과 같고 \\(\\boldsymbol{\\phi}(x) = \\begin{bmatrix} x^0 & \\cdots & x^M\\end{bmatrix}^T\\) 이다.\n\\[\n\\begin{align}\n\\boldsymbol{S}^{-1} &= \\alpha \\boldsymbol{I} + \\beta \\sum_{n=1}^N \\boldsymbol{\\phi}(x_n) \\boldsymbol{\\phi}(x_n)^T\n\\end{align}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "통계학 이론"
    ]
  },
  {
    "objectID": "src/ML/theory/statistics.html#model-selection",
    "href": "src/ML/theory/statistics.html#model-selection",
    "title": "통계학 이론",
    "section": "3 1.3. Model Selection",
    "text": "3 1.3. Model Selection\n\n최소자승법을 이용한 Polynomial curve fitting 에서 보았듯이 best generalization을 주는 최적의 다항식의 order \\(M\\) 이 존재한다. 다항식의 order는 모델에서 free parameters의 갯수를 제어한다. Regularization 을 사용하면 regularization coefficient \\(\\lambda\\) 는 모델의 유효 복잡도(effiective complexcity) 를 통제한다.\n실제 응용에서 우리는 이러한 parameters 들을 결정해야 하며 이렇게 하는 주요 목적은 새로운 데이터에 대한 최소의 predictive performance를 얻기 위함이다. 또한 이렇게 complexicity parameters 에 대한 적당한 값을 찾는 것 뿐만 아니라, 특정 목표에 적합한 모델을 찾기 위해 다양한 모델을 고려할 필요가 있다.\nMLA (maximum likelihood approach) 에서 보았듯이 training set 에 대한 performance 가 다른 데이터에 대한 예측력을 보장해주지 않는다. (overfitting). 만약 데이터가 많다면 가용한 데이터중 일부를 다양한 모델을 학습시키거나, 주어진 모델에 대해 complexicity parameters 를 다양한 범위에서 학습시키는데 사용하고 이것을 독립적인 데이터를 사용하여 predictive performance를 비교하여 수 도 있다. 이렇게 학습데이터와 독립적으로 사용되는 데이터를 validation set 이라 한다. 이렇게 수차례 반복한 다음에 test set 이라 불리는 별도의 독립적인 데이터를 사용하여 최종적으로 평가할 수도 있다.\n보통은 training과 testing에 사용될 수 있는 데이터가 부족한데, 이 경우 좋은 모델을 만들기 위해 training에 가능한 많은 데이터를 사용하고자 할 수 있다. 그러나 만약 validation set이 부족하면 it will give a relatively noisy estimate of predictive performance. 이 딜레마에 대한 해결방법중 하나로 cross validation 방법이 있다.\n\nCross Validation\n\n전체 데이터를 \\(S\\) 개의 group으로 나눈다. \\(S\\) 개의 training group 으로 각 training group 마다 \\(S-1\\) 개의 데이터 그룹을 training set으로 나머지 하나를 validation set으로 사용한다.\nTraining group 마다 각자의 모델 (혹은 별도의 parameters set) 을 사용하므로 computationally expensive 하다. 또한 하나의 모델에 대한 다수의 complexcity parameter 를 갖게 될 수 있다. 이런 조합들을 탐색하다보면 최악의 경우 training run 이 parameter 갯수의 지수승으로 증가할수도 있다.!!!\n우리는 더 좋은 접근법을 사용해야 한다. 이상적으로 이 접근법은 training data 에 의존해야 하며, 한번의 training run을 통해 비교 할 수 있는 다수의 hyperparameters와 model types 를 허용해야 하는데….\n이를 위해 training data 에만 의존하며 over fitting에 의한 bias로부터 자유로운 성능 척도를 찾아야 한다.\n역사적으로 복잡한 모델에서의 over fitting을 보상하는 penalty term을 추가함으로서 maximum likelihood의 bais를 교정하고자 하는 다양한 ‘information criteria’ 가 제안되었다. 예를 들어 Akaike information criterion (AIC) 의 경우 \\[\n\\ln p(\\mathcal{D}\\mid \\boldsymbol{w}_{ML})-M\n\\] 을 최대화 하는 모델을 선택한다. 여기서 \\(p(\\mathcal{D}\\mid \\boldsymbol{w}_{ML})\\) 은 best-fit log likelihood 이며 \\(M\\) 은 모델에서 adjustable 한 parameter의 갯수이다. 이의 변형으로서 Bayesian information criterion 이 있는데 이는 section 4.4.1 에서 소개될 것이다. 이러한 criteria는 model parameter의 불확실성을 고려하지 않으며, 실제적으로는 과하게 간단한 모델을 선호한다.\n따라서 우리는 section 3.4 에서 fully Bayesian approach 로 전환할 것이며 이러한 complexity penalty 가 자연스럽고 원칙적인 방법으로 발생하는지 볼 것이다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "통계학 이론"
    ]
  },
  {
    "objectID": "src/ML/theory/statistics.html#the-curse-of-dimensionality",
    "href": "src/ML/theory/statistics.html#the-curse-of-dimensionality",
    "title": "통계학 이론",
    "section": "4 1.4 The Curse of Dimensionality",
    "text": "4 1.4 The Curse of Dimensionality\n\nOil examples은 그냥 읽으면 되고..\n우리가 다루고자 하는 입력 데이터가 고차원의 데이터 (\\(\\mathcal{D}-\\dim\\))라고 가정해보자. 다항식 근사에서 order \\(3\\) 까지 전개해보면, \\[\ny(\\boldsymbol{x},\\,\\boldsymbol{w})=w_0+\\sum_{i=1}^\\mathcal{D} w_i x_i + \\sum_{i=1}^\\mathcal{D}\\sum_{j=1}^\\mathcal{D} w_{ij}x_ix_j + \\sum_{i=1}^\\mathcal{D}\\sum_{j=1}^\\mathcal{D} \\sum_{k=1}^\\mathcal{D} w_{ijk}x_i x_j x_k \\tag{1.74}\n\\] 이다. \\(\\mathcal{D}\\) 에 따라 3차항의 계수의 갯수는 \\(\\mathcal{D}^3\\) 개 만큼 증가하는 것처럼 보인다.(실제로는 interchange symmetry 로 인해 이것보다는 작지만 그래도 \\(\\mathcal{D}\\gg M\\) 일 경우는 \\(\\mathcal{D}^M\\) 와 같이 증가한다. see exercise 1.16. 이것도 아주 급격하게 증가하는 것이다. )\n\\({D}\\) 차원의 구를 생각하자. \\({D}\\) 가 커질수록 구의 대부분의 부피는 표면에 분포한다. \\({D}\\) 차원에서 반경 \\(r\\) 인 구의 부피 \\(V_D(r)=K_D r^D\\) 이다 여기에 작은 \\(0&lt;\\epsilon\\ll 1\\) 을 생각하면 구 표면의 두께 \\(\\epsilon\\) 만큼의 껍질의 부피와 \\(D\\) 차원에서의 unit sphere 의 부피의 비는, \\[\n\\dfrac{V_D(1)-V_D(1-r)}{V_D(1)}=1-(1-\\epsilon)^D\n\\] 임을 안다. \\({D}\\) 가 커질 수록 작은 \\(\\epsilon\\) 에서의 값이 크다.\n\\(D\\) 차원 가우시안 분포에서 이 데이터를 polar coordinate 로 바꾸어 보자. 차원이 늘어날수록 \\(p(r)\\) 에서 가장 높은 확률을 가진 값이 점점 커진다. 이는 고차원 구에서 대부분의 부피가 spherical shell에 위치한다는 앞의 논리와 상응한다.\n차원의 저주는 저차원에서의 직관이 고차원에서도 통용되지 않는 경우가 많음을 의미한다. 이 차원의 저주는 패턴 인식의 응용에 있어서 중요한 문제를 제기하지만 고차원을 다루는 효율적인 테크닉이 부족하거나 없다는 것을 의미하지는 않는다.\n\n고차원의 데이터라도 실제로는 보다 낮은 차원의 특정 영역에 데이터가 제한되어 있는 경우가 흔하며,\n실제 데이터는 전형적으로 어떤 smoothness properties 를 (최소한 국소적으로라도) 가지고 있는 경우가 많다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "통계학 이론"
    ]
  },
  {
    "objectID": "src/ML/theory/optimization.html",
    "href": "src/ML/theory/optimization.html",
    "title": "최적화",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "최적화"
    ]
  },
  {
    "objectID": "src/ML/theory/optimization.html#최적화",
    "href": "src/ML/theory/optimization.html#최적화",
    "title": "최적화",
    "section": "1 최적화",
    "text": "1 최적화\n\n1.1 최적화의 기본 요소\n\n3가지 기본 요소\n\n변수 (Decesion variable or unknown) \\(\\boldsymbol{x}\\in \\mathbb{R}^n\\)\n목적함수 (objective function) \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\)\n제약 조건 (Constraint) \\(g_i:\\mathbb{R}^n \\to \\mathbb{R}\\), \\(i=1,\\ldots,\\,m\\) 에 대해 \\(g_i(\\boldsymbol{x})\\le 0\\) and/or \\(h_j:\\mathbb{R}^n \\to \\mathbb{R},\\, j=1,\\ldots,\\,k\\) 에 대해 \\(h_j(\\boldsymbol{x})=0\\)\n\n 과정\n\n모델링 : 목적함수, 변수, 제약조건을 찾고 확인한다.\n모델을 만든 후 최적화 알고리즘을 사용하여 해를 구한다.\n\n\n\n\n1.2 최적화의 수학 모델\n수학적 표준 모델\n\n제약조건을 만족하는 \\(\\mathcal{C}\\subset \\mathbb{R}^n\\) 를 가능해 영역(feasible region) 이라고 한다. 즉 \\(\\mathcal{C}\\) 는 아래와 같다. \\[\n\\mathcal{C} = \\{\\boldsymbol{x}\\in \\mathbb{R}^n : g_i(\\boldsymbol{x})\\le 0,\\,i=1,\\ldots,\\,m\\}.\n\\]\n\\(\\displaystyle \\boldsymbol{x}^\\ast = \\argmin_{\\boldsymbol{x}\\in \\mathcal{C}} f(\\boldsymbol{x})\\) 를 찾는다. 이 \\(\\boldsymbol{x}^\\ast\\) 를 최적해 (optimal solution) 이라고 한다.\n\\(\\{g_i\\}\\) 와 같은 제약조건이 없다면 unconstrained 라고 하며, 그렇지 않다면 constrained 라고 한다.\n\n 등가 변환\n\n우리가 구하고자 하는 문제가 \\(f(\\boldsymbol{x})\\) 를 최소화 하는 것이 아닌 최대로 하는 \\(\\boldsymbol{x}\\) 를 찾는 문제라면 \\(-f(\\boldsymbol{x})\\) 를 최소화 하는 문제이다.\nconstraint 가 \\(g_i(\\boldsymbol{x}) \\ge 0\\) 이라면 \\(-g_i(\\boldsymbol{x})\\le 0\\) 으로 바꿀 수 있다. \\(g_i (\\boldsymbol{x})=0\\) 이라면 \\(g_i(\\boldsymbol{x}) \\le 0\\) 와 \\(-g_i(\\boldsymbol{x})\\le 0\\) 을 동시에 만족하는 조건으로 바꿀 수 있다.\n즉 수학적 표준 모델은 상당히 표면적으로 보이는 것보다 훨씬 넓은 범위의 문제를 포괄한다.\n\n\n\n\n1.3 전역적 최적화와 국소적 최적화\n함수 \\(f(x)\\) 가 아래 그림과 같다고 하자.\n\n\n\n\n\n\n그림 1: 전역적 최소와 국소적 최소\n\n\n\n전체 영역에서의 최소점은 \\(x_G\\) 이며 이를 전역적 최소점(global minimum) 이라고 한다. 그리고 \\(x_L\\) 같이 어떤 근방에서의 최소점을 국소적 최소점(local minimum) 이라고 한다. 함수 \\(f(x)\\) 가 단 하나의 국소적 최소점을 갖는다는 보장이 없으며, 또한 많은 알고리즘은 전역적 최소점이 아닌 국소적 최소점을 찾는다. \\(f(x)\\) 가 두번 미분 가능할 경우 국소적 최소점은 보통 미분이 \\(0\\) 이고 이차미분이 양수인 점이다. 다변수의 경우 \\(\\nabla f(\\boldsymbol{x})=\\boldsymbol{0}\\) 이며 헤시안 행렬 \\(\\boldsymbol{H}_f\\) 가 positive definite 한 경우이다.\n\n\n\n1.4 볼록 최적화 (Convex optimization)\n\n\n\n\n\n\nconvex function\n\\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) 이 다음을 만족하면 convect function 이라 한다. \\[\n\\forall \\theta\\in [0,\\,1],\\, \\boldsymbol{x},\\, \\boldsymbol{y}\\in \\mathbb{R}^n \\implies f(\\theta \\boldsymbol{x}+(1-\\theta)\\boldsymbol{y}) \\le \\theta f(\\boldsymbol{x}) + (1-\\theta)f(\\boldsymbol{y})\n\\]\nconvex set\n\\(A\\subset\\mathbb{R}^n\\) 이 다음을 만족하면 convex set 이라 한다.\n\\[\n\\forall \\theta\\in [0,\\,1],\\, \\boldsymbol{x},\\, \\boldsymbol{y}\\in A \\implies \\theta \\boldsymbol{x}+(1-\\theta)\\boldsymbol{y}\\in A\n\\]\n\n\n\n\n\n목적함수가 convex function 이고 feasible region 이 convex set 인 경우의 최적화를 convex optimization 이라고 한다.\nConvex optimization 의 모든 국소적 해는 전역적 해이다.\nCVX (Matlab), CVXPY (Python)",
    "crumbs": [
      "AI & ML",
      "이론",
      "최적화"
    ]
  },
  {
    "objectID": "src/ML/theory/optimization.html#unconstrained-convex-optimizaiton-문제",
    "href": "src/ML/theory/optimization.html#unconstrained-convex-optimizaiton-문제",
    "title": "최적화",
    "section": "2 Unconstrained convex optimizaiton 문제",
    "text": "2 Unconstrained convex optimizaiton 문제\n\n\\(\\nabla_\\boldsymbol{x}f(\\boldsymbol{x}^\\ast)=\\boldsymbol{0}\\) 인 \\(\\boldsymbol{x}^\\ast\\) 를 찾는 문제이다.\n\n\\[\n\\nabla_\\boldsymbol{x} f := \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_n}\\end{bmatrix}\n\\tag{1}\\]\n이와 유사하게\n\\[\n\\dfrac{df}{d\\boldsymbol{x}} := \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_1} & \\cdots & \\dfrac{\\partial f}{\\partial x_n}\\end{bmatrix}\n\\tag{2}\\]\n로 정의한다. 즉 \\(\\nabla_{\\boldsymbol{x}}f\\) 는 열벡터이며 \\(\\dfrac{d f}{d\\boldsymbol{x}}\\) 는 행벡터이다.\n\n\n2.1 Gradient\n\\(\\boldsymbol{x} = \\begin{bmatrix}x_1 & \\cdots & x_n \\end{bmatrix}^T \\in \\mathbb{R}^n\\), \\(\\boldsymbol{a} \\in \\mathbb{R}^n\\), \\(\\boldsymbol{A}=\\mathcal{M}_{m \\times n}(\\mathbb{R})\\) 일 때 \\(\\boldsymbol{x}\\) 에 대한 스칼라 함수의 gradient 는 다음과 같다.\n\\[\n\\begin{aligned}\n\\nabla_\\boldsymbol{x} (\\boldsymbol{a}^T\\boldsymbol{x}) &= \\boldsymbol{a}\\\\[0.3em]\n\\nabla_\\boldsymbol{x} (\\boldsymbol{x}^T\\boldsymbol{a}) &= \\boldsymbol{a}\\\\[0.3em]\n\\nabla_\\boldsymbol{x} (\\boldsymbol{x}^T\\boldsymbol{x}) &= 2\\boldsymbol{x}\\\\[0.3em]\n\\nabla_{\\boldsymbol{x}} (\\boldsymbol{x}^T\\boldsymbol{Ax}) &= (\\boldsymbol{A}^T +\\boldsymbol{A})\\boldsymbol{x}\n\\end{aligned}\n\\tag{3}\\]\n\n\n\n2.2 직접법\n\n\\(\\nabla_\\boldsymbol{x}f(\\boldsymbol{x}^\\ast)=\\boldsymbol{0}\\) 인 \\(\\boldsymbol{x}^\\ast\\) 를 직접 구한다.\n\n\n이제 몇가지 형태의 convex 함수인 목적함수에 대한 그래디언트 \\(\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x})\\) 와 헤시안 행렬 \\(\\boldsymbol{H}_f\\) 를 구해보자.\n\n\n예제 1 (Affine 형태의 목적함수) \\(f(\\boldsymbol{x}) = \\boldsymbol{a}^T\\boldsymbol{x} + \\boldsymbol{b}\\) 인 경우\n\\[\n\\nabla_\\boldsymbol{x}f(\\boldsymbol{x})=\\boldsymbol{a},\\qquad \\boldsymbol{H}_f = \\boldsymbol{0}.\n\\]\n이다. 따라서 \\(\\boldsymbol{a}=\\boldsymbol{0}\\) 인 특별한 경우가 아니면 최소값이 존재하지 않는다.\n\n\n\n\n\n예제 2 (Quadratic 형태의 목적함수) 대칭행렬 \\(\\boldsymbol{P}\\) 에 대해 \\(f(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{Px} + \\boldsymbol{b}^T\\boldsymbol{x}+c\\) 인 경우\n\\[\n\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x})= 2\\boldsymbol{Px} + \\boldsymbol{b},\\qquad \\boldsymbol{H}_f = 2\\boldsymbol{P}\n\\]\n이다.\n\n\n\n\n\n예제 3 (제곱 형태의 목적함수) 대칭행렬 \\(\\boldsymbol{P}\\) 에 대해 \\(f(\\boldsymbol{x}) = \\|\\boldsymbol{Ax}-\\boldsymbol{b}\\|^2\\) 인 경우\n\\[\nf(\\boldsymbol{x})= \\boldsymbol{x}^T\\boldsymbol{A}^T\\boldsymbol{Ax} - \\boldsymbol{x}^T\\boldsymbol{A}^T\\boldsymbol{b}-\\boldsymbol{b}^T\\boldsymbol{Ax}+\\boldsymbol{b}^T\\boldsymbol{b}\n\\]\n이므로 gradient 와 헤시안 행렬은 다음과 같다.\n\\[\n\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x})= 2\\boldsymbol{A}^T\\boldsymbol{Ax} -2\\boldsymbol{A}^T\\boldsymbol{b},\\qquad \\boldsymbol{H}_f = 2\\boldsymbol{A}^T\\boldsymbol{A}.\n\\]\n이 때 \\(\\boldsymbol{x}^\\ast = (\\boldsymbol{A}^T\\boldsymbol{A})^{-1}\\boldsymbol{A}^T\\boldsymbol{b}\\) 이어야 한다. 이 때 \\((\\boldsymbol{A}^T\\boldsymbol{A})^{-1}\\boldsymbol{A}^T\\) 는 잘 알려진 무어-펜로즈 좌측 유사역행렬(left pseudoinverse matrix) 이다.\n\n\n\n\n\n2.3 경사 하강법(Gradient descent)\n미분 가능한 \\(f(\\boldsymbol{x})\\) 에 대해 \\(\\boldsymbol{x}_0\\) 가 주어졌으며 충분히 작은 값 \\(\\alpha_k\\) 에 대해 \\(\\boldsymbol{x}_{k+1}\\) 이 다음과 같다고 하자.\n\\[\n\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k -\\alpha_k \\nabla_\\boldsymbol{x} f(\\boldsymbol{x}_k)\n\\]\n그렇다면 \\(f(\\boldsymbol{x}_{k+1}) &lt; f(\\boldsymbol{x}_k)\\) 가 될 것이고 이것을 계속 반복해 나가면 \\(f(\\boldsymbol{x})\\) 의 국소적 최저점을 찾을 수 있다. 이것을 경사 하강법(gradient descent method) 이라고 한다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "최적화"
    ]
  },
  {
    "objectID": "src/ML/theory/optimization.html#등식-제약이-주어졌을-때",
    "href": "src/ML/theory/optimization.html#등식-제약이-주어졌을-때",
    "title": "최적화",
    "section": "3 등식 제약이 주어졌을 때",
    "text": "3 등식 제약이 주어졌을 때\n\n3.1 라그랑제 승수법\n목적함수 \\(f(\\boldsymbol{x})\\) 에 대해 \\(h_i(\\boldsymbol{x})=0,\\, (i=1,\\ldots,\\,M)\\) 의 제약조건이 주어졌다고 하자. 이 때\n\\[\n\\begin{aligned}\n\\nabla f(\\boldsymbol{x}^\\ast) &= \\lambda_i \\nabla h_i(\\boldsymbol{x}^\\ast) \\\\[0.3em]\nh_i(\\boldsymbol{x}^\\ast) &= 0.\n\\end{aligned}\n\\]\n을 만족하는 \\(\\boldsymbol{x}^\\ast\\) 는 가능해 영역에서의 \\(f(\\boldsymbol{x})\\) 의 stationary point 이다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "최적화"
    ]
  },
  {
    "objectID": "src/ML/theory/ML.html",
    "href": "src/ML/theory/ML.html",
    "title": "기계 학습",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "기계 학습"
    ]
  },
  {
    "objectID": "src/ML/theory/ML.html#기계-학습의-분류",
    "href": "src/ML/theory/ML.html#기계-학습의-분류",
    "title": "기계 학습",
    "section": "1 기계 학습의 분류",
    "text": "1 기계 학습의 분류\n\n지도 학습(Supervised Learning)\n\n입력 \\(\\boldsymbol{x}\\) 에 대한 정답(label) \\(\\boldsymbol{t}\\) 이 있는 데이터를 학습한다.\n회귀(regression) : 정답으로서 가능한 값이 실수(\\(\\mathbb{R}\\)) 인 경우.\n분류(classification) : 정답으로서 가능한 값이 이산적인 값일 경우.\n\n비지도 학습(Unsupervised Learning)\n\n정답(label)이 없는 데이터를 특징별로 군집화 (clustering) 하거나 데이터의 분포를 추정한다.\n\n강화학습 or 증강학습 (Reinforced Learning)\n\n주어진 데이터가 아닌, 환경과 상호작용을 통해 학습\n주어진 상태(state) 에 행동 (action) 을 취하며, 이에 대한 보상(reward)을 받는다.\n훈련 도중에, 최대 보상을 받도록 정책(policy)를 지속적으로 수정한다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "기계 학습"
    ]
  },
  {
    "objectID": "src/ML/theory/ML.html#함수로서의-기계학습",
    "href": "src/ML/theory/ML.html#함수로서의-기계학습",
    "title": "기계 학습",
    "section": "2 함수로서의 기계학습",
    "text": "2 함수로서의 기계학습\n\n인공지능은 어떤 입력에 대한 출력을 하며 우리는 보통 이런 것을 수학적으로는 함수(function) 라고 부른다.\n기계학습에서의 학습이란 대량의 데이터를 입력하여 이 데이터를 가장 잘 표현하는 하나의 함수를 정하는 것이다. 하나의 데이터에 대해 보통 입력값이 여러개이므로 \\(i\\)-번째 데이터의 입력은 \\(\\boldsymbol{x}_i\\) 로 \\(i\\)-번째 데이터의 label 은 \\(y_i\\) 로 표기한다. 입력값과 label 의 쌍을 \\((\\boldsymbol{x}_i, y_i)\\) 로 표기한다.\n함수를 내부적으로 표현하는데 쓰는 값을 매개변수 (parameter) 라 한다. 예를 들어 \\[\ny=f(\\boldsymbol{x}=(x_1, x_2)) = ax_1+bx_2+c = \\begin{bmatrix} 1 & a & b \\end{bmatrix}\\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}\n\\] 일 경우, 입력값은 \\(\\boldsymbol{x}=\\begin{bmatrix}x_1 & x_2\\end{bmatrix}^T\\), 매개변수는 \\(a, b, c\\), 출력값은 \\(y\\) 이다.\n기계학습을 통해 매개변수를 정해야 함수가 완성된다. 매개변수의 집합을 \\(\\boldsymbol{w}\\) 로 표기하여 함수를 다음과 같이 쓰기도 한다 \\[\ny=f(\\boldsymbol{x} ; \\boldsymbol{w})\n\\]\n함수가 얼마나 데이터를 잘 기술하는지를 평가하는 함수를 손실함수(Loss function) 혹은 비용함수(Cost function) 라고 한다. 대표적인 손실함수로는 신경망(Neural Network) 에서, 회귀의 경우 평균제곱오차 (Mean Square Error, MSE) 함수 와 분류의 경우 교차 엔트로피 오차 (Cross Entropy Error, CEE) 함수가 있다.\n\n\\[\n\\begin{aligned}\n\\text{MSE}(\\boldsymbol{w}) &= \\dfrac{1}{2} \\sum_{i=1}^N \\left(y_i - f(\\boldsymbol{x}_i; \\boldsymbol{w})\\right)^2 \\\\[0.3em]\n\\text{CEE}(\\boldsymbol{w}) &= \\sum_{i=1}^N \\left[y_i \\ln (f(\\boldsymbol{x}_i;\\boldsymbol{w})) - (1-y_i)(1-\\ln (f(\\boldsymbol{x}_i;\\boldsymbol{w})))\\right]\n\\end{aligned}\n\\]\n\n기계학습을 통해 오차함수를 최소화 하는 매개변수들을 찾아 함수를 완성한다.\n손실 함수(Loss Function) \\(L(\\boldsymbol{w})\\) 의 특징\n\n\\(L(\\boldsymbol{w}) ≥ 0\\)\n\\(L(\\boldsymbol{w})\\) 는 미분가능 함수\n최적의 경우 = \\(L(\\boldsymbol{w})\\) 가 최소값이 되는 경우\n따라서 기계학습에서 학습이란 \\(L(\\boldsymbol{w})\\) 가 최소값이 되도록 하는 \\(\\boldsymbol{w}\\) 를 찾는 것이다. (최적화 (optimization))",
    "crumbs": [
      "AI & ML",
      "이론",
      "기계 학습"
    ]
  },
  {
    "objectID": "src/ML/theory/ML.html#통계학의-기본",
    "href": "src/ML/theory/ML.html#통계학의-기본",
    "title": "기계 학습",
    "section": "3 통계학의 기본",
    "text": "3 통계학의 기본\n\n\n3.1 기본 개념\n\n표본 공간 (sample space) \\(\\Omega\\) : 실험/측정에 있어서 가능한 모든 결과값의 집합. \\(\\Omega\\) 의 각 원소들은 각각이 식별 가능하며, 상호 배타적(동시에 발생할 수 없음) 이어야 한다. 특정 결과값 \\(\\omega\\) 는 \\(\\Omega\\) 의 원소이다.\n사건 공간 (evant space) \\(\\mathcal{A}\\) : 실험/측정의 잠재적인 결과의 집합. 당연히 표본 공간 \\(\\Omega\\) 의 부분집합.\n확률 (probability) : \\(A\\in \\mathcal{A}\\) 에 대해 \\(A\\) 의 사건이 발생할 확률을 \\(p(A)\\) 라고 한다. 임의의 \\(A\\in \\mathcal{A}\\) 에 대해 \\(0\\le p(A)\\le 1\\) 이며 \\(\\sum_{A\\in \\Omega} p(A)=1\\) 이다.\n표적 공간 (target space) \\(\\mathcal{T}\\) : 우리가 관심있는 정량화된 값. 서로 구별되는 표적공간의 원소를 상태(state) 라고 한다.\n확률 변수 (random variable) : 표본공간의 성분 \\(\\omega\\) 와 표적공간의 성분 \\(t\\) 를 연결하는 함수 \\(X:\\Omega \\to \\mathcal{T}\\) 가 존재하며 이 \\(X\\) 를 확률변수 라고 한다.\n\n예를 들어 두개의 동전을 던져 이중 몇개의 동전이 앞면이 나오는 지 관심있다고 하자. 앞면을 \\(u\\), 뒷면을 \\(d\\) 라고 하면 \\(\\Omega = \\mathcal{A} = \\{ uu,\\, ud,\\,du,\\,dd\\}\\) 이며 \\(\\mathcal{T}=\\{0,\\,1,\\,2\\}\\) 가 된다. 이제 사건공간이 아닌 표적공간의 부분집합에 대한 확률에 관심을 갖게 된다. 즉 \\(S\\in \\mathcal{T}\\) 에 대해 \\(p(S)\\) 가 우리의 주요 관심사이다.\n표적공간 \\(\\mathcal{T}\\) 가 이산공간일 때 \\(X\\) 를 이산확률변수라고 하고 \\(\\mathbb{R}\\) 과 같이 연속일 때 연속확률변수라고 한다.\n\n\n\n3.2 이산 확률\n\n결합 확률(Joint Probability)\n확률변수 \\(X,\\,Y\\) 에 대해 \\(X\\) 는 \\(x_1,\\ldots,\\,x_M\\) 값을 가질 수 있으며, \\(Y\\) 는 \\(y_1,\\ldots,\\,y_L\\) 값을 가질 수 있다고 하자. 모두 \\(N\\) 번의 시행에서 \\(X=x_i,\\, Y=y_j\\) 가 나온 횟수를 \\(n_{ij}\\) 라 하자. \\(N\\) 번의 시행에서 \\(X=x_i\\) 인 횟수는 \\(c_i\\), \\(Y=y_j\\) 인 횟수는 \\(r_j\\) 라 하자. 즉, \\[\np(X=x_i,\\, Y=y_j)=\\dfrac{n_{ij}}{N},\\quad p(X=x_i)=\\dfrac{c_i}{N},\\quad p(Y=y_j)=\\dfrac{r_j}{N}\\;.\n\\tag{1}\\]\n이다. 이 때,\n\\[\np(X=x_i)=\\sum_{j=1}^L p(X=x_i,\\, Y=y_j)\n\\tag{2}\\]\n이며 (자명하다) 이를 sum rule 이라 한다. 여기서 \\(P(X=x_i)\\) 를 개별 사건의 확률로 주변 확률(marginal probability) 라 하기도 한다.\n\n\n\n조건부 확률(Conditional probability)\n\\(X=x_i\\) 인 상황에서 \\(Y=y_j\\) 인 확률을 \\(p(Y=y_j \\mid X=x_i)\\) 라 쓰며 \\(X=x_i\\) 일 때 \\(Y=y_j\\) 에 대한 조건부 확률(conditional probability) 이라고 하고 다음과 같이 주어진다. \\[\np(Y=y_j\\mid X=x_i)=\\dfrac{n_{ij}}{c_i}\n\\tag{3}\\]\n\n\n\n확률의 곱의 법칙(Product rule of probability)\n\\[\np(X=x_i,\\, Y=y_j)=\\dfrac{n_{ij}}N=\\dfrac{n_{ij}}{c_i}\\dfrac{c_i}{N}=P(Y=y_j\\mid X=x_i)\\cdot p(X=x_i)\\;.\n\\tag{4}\\]\n\n\n\n합과 곱의 규칙\n\\[\n\\begin{aligned}\n\\textbf{sum rule}&\\qquad p(X)=\\sum_Y p(X,\\,Y)\\,, \\\\\n\\textbf{product rule}& \\qquad p(X,\\,Y)=p(Y\\mid X)p(X)\n\\end{aligned}\n\\tag{5}\\]\n\n\n\n베이즈 정리(Bayes’ theorem)\n\\[\np(Y\\mid X)=\\dfrac{p(X\\mid Y) \\, p(Y)}{p(X)}\\;.\n\\tag{6}\\]\nWith sum rule, \\[\nP(X)=\\sum_{Y}p(X \\mid Y)\\, p(Y)\\,.\n\\tag{7}\\]\n\n\n변수의 독립성(Independence of variable)\n확률변수 \\(X,\\,Y\\) 에 때해 \\(p(X,\\,Y)=p(X)\\, p(Y)\\) 일 때 \\(X\\) 와 \\(Y\\) 는 서로 독립적(independent) 이라고 한다. \\(X,\\,Y\\) 가 서로 독립적이면 식 식 5 으로 부터 \\(p(Y|X)=p(Y)\\) 임을 알 수 있다.\n\n\n\n\n3.3 확률 밀도 함수\n표적공간이 연속일 때 확률은 확률밀도함수 \\(p(x)\\) 로 기술된다.\n\n확률밀도함수와 확률\n확률밀도함수 \\(p(x)\\)는 다음 두 조건을 만족해야 한다. \\[\n\\begin{align}\np(x) & \\ge 0\\\\\n\\int_{-\\infty}^\\infty p(x)\\,dx &=1\n\\end{align}\n\\tag{8}\\]\n연속확률변수일 때 \\(x\\in (a,\\,b)\\) 일 확률 \\(p(x\\in (a,\\,b))\\) 는 \\[\np(x\\in (a,\\,b))=\\int_a^b p(x)\\,dx\n\\tag{9}\\] 이다.\n\n\n\n변수의 변환\n\\(x=g(y)\\) 이며 \\(y\\) 에 대한 확률분포를 알고 싶을 때, 이 확률분포를 \\(p_y(y)\\) 라 하면, \\[\np_y(y)=p(x)\\left|\\dfrac{dx}{dy}\\right|=p(g(y))|g'(y)\n\\tag{10}\\] 임을 쉽게 보일 수 있다.\n\n\n\n누적 분포 함수\n\\(P(z)=p(x\\in (-\\infty,\\,z))\\) 를 누적 분포 함수(cumulative distribution function) 이라 하며, \\[\nP(z) = \\int_{-\\infty}^z p(x)\\, dx\n\\tag{11}\\]\n로 정의된다. \\(P'(x)=p(x)\\) 임은 십게 알 수 있다.\n다변수 \\(\\boldsymbol{x}=(x_1,\\ldots,\\,x_D)\\) 에 대한 확률분포는 \\(\\boldsymbol{x}\\) 를 포함하는 infinitesimal volume \\(\\delta \\boldsymbol{x}\\) 에 대해 \\(p(\\boldsymbol{x})\\,\\delta \\boldsymbol{x}\\) 로 주어지며 다음과 같은 성질을 만족한다.\n\\[\n\\begin{aligned}\np(\\boldsymbol{x}) & \\ge 0  \\\\\n\\int p(\\boldsymbol{x})\\,d\\boldsymbol{x}&= 1\n\\end{aligned}\n\\tag{12}\\]\n연속적인 변수, 이산적인 변수 모두에 대한 확률 분포함수를 probability density function 이라 하기도 하고, 이산적인 변수에 대해서 probability mass function 이라고 구분하여 부르기도 한다.\nSum rule과 Bayes’ theorem 을 생각하면 다음이 성립함을 알 수 있다. \\[\n\\begin{aligned}\np(x) &= \\int p(x,\\,y)\\, dy \\\\\np(x,\\,y)&=p(y| x)\\,p(x)\n\\end{aligned}\n\\tag{13}\\]\n\n\n\n\n3.4 기댓값과 공분산\n\n기댓값\n확률변수 \\(X\\) 에 대한 확률분포가 \\(p(x)\\) 일 때 \\(x\\) 에 대한 함수 \\(f(x)\\) 의 평균값을 \\(f\\) 에 대한 기댓값(expectation) 이라 하며 \\(\\mathbb{E}[f]\\) 로 표기하고 다음과 같다. \\[\n\\begin{align}\n\\mathbb{E}[f]&:=\\sum_x p(x) f(x) &&\\text{for descrete distribution,}\\\\\n&:=\\int  p(x) f(x)\\, dx& &\\text{for continuous distribution.}\n\\end{align}\n\\tag{14}\\]\n\\(N\\) 개의 sample 이 주어졌을 때 기댓값은 다음과 같이 근사 될 수 있다. \\[\n\\mathbb{E}[f] \\approx \\dfrac{1}{N} \\sum_{i=1}^N f(x_n).\n\\tag{15}\\]\n식 14 의 두 식은 식 15 의 \\(N \\to \\infty\\) 극한과 동일하다.\n다변수 확률분포에서 특정 변수에 대한 기댓값은 \\(\\mathbb{E}_x [f(x,\\,y)]\\) 와 같이 표기하며 다음과 같다. \\[\n\\mathbb{E}_x [f(x,\\,y)]=\\sum_x p (x,\\,y) f(x,\\,y) =\\int p(x,\\,y) f(x,\\,y)\\, dx\n\\tag{16}\\]\n\n\n\n조건부 기댓값\n\\(p(x\\,|\\,y)\\) 에 대한 \\(f(x)\\) 의 기댓값은 다음과 같다. \\[\n\\mathbb{E}_x [f \\mid y\\,]= \\sum_x p(x\\,|\\, y)\\, f(x)\n\\tag{17}\\]\n\n\n\n분산\n\\(f(x)\\) 에 대한 분산(variance) \\(\\text{Var}[f]\\) 는 다음과 같이 정의된다. \\[\n\\begin{aligned}\n\\operatorname{Var}[f] & := \\mathbb{E}\\left[(f(x)-\\mathbb{E}[f(x)])^2\\right] \\\\[0.3em]\n&=\\mathbb{E}[f(x)^2]-(\\mathbb{E}[f(x)])^2\\;\n\\end{aligned}\n\\tag{18}\\]\n이다. 변수 \\(x\\) 자체에 대한 분산 \\(\\text{Var}[x]\\) 는 다음과 같다. \\[\n\\operatorname{Var}[x]=\\mathbb{E}[x^2]-\\mathbb{E}[x]^2.\n\\tag{19}\\]\n\n\n\n공분산\n아래와 같이 정의되는 \\(\\text{Cov}[x,\\,y]\\) 를 \\(X,\\,Y\\) 에 대한 공분산(covariance) 라고 한다. \\[\n\\begin{align}\n\\operatorname{Cov}[x,\\,y]&:= \\mathbb{E}_{x,\\,y} \\left[(x-\\mathbb{E}[x]) (y-\\mathbb{E}[y])\\right] \\\\\n&=\\mathbb{E}_{x,\\,y}[xy] -\\mathbb{E}[x] \\mathbb{E}[y].\n\\end{align}\n\\tag{20}\\]\n\\(x,\\,y\\) 가 서로 독립이면 \\(\\operatorname{Cov}[x,\\,y]=0\\) 이다.\n두 확률 변수가 벡터 \\(\\boldsymbol{x},\\, \\boldsymbol{y}\\) 이면 \\[\n\\begin{aligned}\n\\operatorname{Cov}[\\boldsymbol{x},\\, \\boldsymbol{y}]&= \\mathbb{E}_{\\boldsymbol{x},\\, \\boldsymbol{y}}\\left[\\left( \\boldsymbol{x}-\\mathbb{E}[\\boldsymbol{x}]\\right)\\left( \\boldsymbol{y}^T-\\mathbb{E}[\\boldsymbol{y}^T]\\right)\\right] \\\\[0.3em]\n&=\\mathbb{E}_{\\boldsymbol{x},\\,\\boldsymbol{y}}[\\boldsymbol{x}\\boldsymbol{y}^T]-\\mathbb{E}[\\boldsymbol{x}]\\,\\mathbb{E}[\\boldsymbol{y}^T]\n\\end{aligned}\n\\tag{21}\\]\n이다. \\(\\operatorname{Cov}[\\boldsymbol{x}] := \\operatorname{Cov}[\\boldsymbol{x},\\,\\boldsymbol{x}]\\) 로 정의한다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "기계 학습"
    ]
  },
  {
    "objectID": "src/ML/pytorch/intro.html",
    "href": "src/ML/pytorch/intro.html",
    "title": "소개",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "PyTorch",
      "소개"
    ]
  },
  {
    "objectID": "src/ML/pytorch/intro.html#sec-pytorch_introduction",
    "href": "src/ML/pytorch/intro.html#sec-pytorch_introduction",
    "title": "소개",
    "section": "1 시작하기",
    "text": "1 시작하기\n앞으로의 모든 코드에는 아래의 import 가 이미 실행되었다고 가정한다.\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n1.1 데이터셋(Dataset) 과 데이터 로더(Data Loader)\n파이토치에서 데이터에 작업을 할 때 torch.utils.data.DataLoader 와 torch.utils.data.Dataset 을 사용한다. Dataset 샘플과 정답을 저장하는 컨테이너이며 DataLoader 는 Dataset 의 데이터들을 iterable 하게 처리 할 수 있도록 한다. Pytorch 의 Datasets 은 pytorch 에서 제공하는 데이터셋을 설명한다.\nFashionMNIST 데이터셋을 다운로드 받아 보자. 이 FashionMNIST 데이터셋은 가방이나 악세사리와 같은 패션 아이템의 이미지를 학습하기 위한 저해상도 이미지의 모음이다. 우선 학습 데이터를 다운받는다.\n\n\nIn\n\ntraining_data = datasets.FashionMNIST(\n    root=\"/home/asc/torchdata\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\nroot 는 데이터를 다운 받는 디렉토리를 말한다. root 디렉토리에 FashionMNIST 서브디렉토리를 만들고 이 서브디렉토리에 데이터가 저장된다.\n이제 테스트 데이터를 다운받아보자.\n\n\nIn\n\ntest_data = datasets.FashionMNIST(\n    root=\"/home/asc/torchdata\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n데이터를 확인해보자.\n\n\nIn\n\ntraining_data\n\n\n\nOut\n\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: /home/asc/torchdata\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\n\n\n\n\nIn\n\ntest_data\n\n\n\nOut\n\nDataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: /home/ast/torchdata\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\ntraining_data 와 test_data 는 텐서화된 이미지 데이터를 포함한다.\n\n\nIn\n\ntraining_data.data\n\n\n\nOut\n\ntensor([[[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n\n\n배치 사이즈를 정하고 데이터 로더를 생성한다.\n\n\nIn\n\nbatch_size = 64\n\n# 데이터로더를 생성합니다.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\n\n\nOut\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n\n\n이제 FashionMNIST 의 몇몇 아이템들을 보자. \n\n\nIn\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n그림 1: Out\n\n\n\n\n\n\n1.2 사용자 정의 데이터셋과 데이터 로더\n사용자 정의 Dataset 은 클래스 Dataset 클래스를 상속받아 만들며 아래의 세 함수가 정의되어야 한다.\n\n__init__\n__len__\n__getitem__\n\n즉 python 의 list 와 유사하게 행동해야 한다는 의미이다. Pytorch tutorial 의 사용자 정의 Dataset 에서 제시한 샘플 코드는 아래와 같다.\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n\n이제 아주 간단한 Dataset 을 만들어 보자.\n\n\nIn\n\nfrom torch.utils.data import Dataset\n\nclass SimpleCustomDataset(Dataset):\n    def __init__(self, data:str):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\nds = SimpleCustomDataset(\"abcdefghijk\")\ndloader = DataLoader(ds, batch_size=3, shuffle=True)\n\n이제 ds 라는 Dataset 과 dloader 라는 DataLoader 가 만들어졌다. 아래와 같이 batch_size 크기 대로 순회 할 수 있다.\n\n\nIn\n\nfor i in range(4):\n    print(next(iter(dloader)))\n\n\n\nOut\n\n['d', 'i', 'g']\n['f', 'd', 'c']\n['e', 'h', 'c']\n['h', 'c', 'a']\n\n\n\n\n1.3 CUDA\ntorch.cuda.is_available() 함수를 통해 CUDA 를 사용 할 수 있는지 확인 할 수 있다.\n\n\nIn\n\ntorch.cuda.is_available()\n\n\n\nOut\n\nTrue\n\n\nCUDA GPU 가 몇개인지 확인할 수 있다.\n\n\nIn\n\ntorch.cuda.device_count()\n\n\n\nOut\n\n2\n\nCUDA 장치는 0 부터 시작하는 인덱스를 갖는다. 따라서 2개의 CUDA GPU 장치의 인덱스는 0, 1 이며 그 이름은 다음과 같이 확인 할 수 있다.\n\n\nIn\n\nfor i in (0, 1):\n    print(torch.cuda.get_device_name(i))\n\n\n\nOut\n\nNVIDIA RTX A5000\nNVIDIA RTX A5000\n\n\nCUDA 를 사용하기 위해서는 다음과 host 즉 CPU 상에서 텐서를 만든 후 CUDA 로 텐서를 복사할 수 있다.\n\n\nIn\n\na=torch.tensor([[1,2,3,4], [5,6,7,8]])\nprint(\"a.divice = \", a.device)\nb = a.to(cuda0)\nc = a.to(cuda1)\nprint(\"b.divice = \", b.device)\nprint(\"c.divice = \", c.device)\nb\n\n\n\nOut\n\na.divice =  cpu\nb.divice =  cuda:0\nc.divice =  cuda:1\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]], device='cuda:0')\n\n\n혹은 처음부터 CUDA 장치에 텐서를 만들 수 있다.\n\n\nIn\n\nd=torch.tensor([[1,2], [3,4]], device=\"cuda:0\")",
    "crumbs": [
      "AI & ML",
      "PyTorch",
      "소개"
    ]
  },
  {
    "objectID": "src/ML/pytorch/intro.html#텐서",
    "href": "src/ML/pytorch/intro.html#텐서",
    "title": "소개",
    "section": "2 텐서",
    "text": "2 텐서\nnumpy.array 와 같이 torch.tensor 는 pytorch 에서 데이터를 다루는데 사용되는 객체이다. 많은 경우 numpy (혹은 np) 를 torch, array 를 tensor 로 바꾸면 numpy 와 거의 같게 동작한다. pytorch-for-numpy-users 를 참고하라.\nnumpy 의 array 는 cpu 에서 동작하지만 앞서 보였듯이 torch 의 tensor 는 cpu 에서 동작할 수도 있고 cuda 에서 동작할 수도 있다. 이후로 특별한 언급이 없다면 array 는 np.array 이며 tensor 는 torch.tensor 이다.\n\n\n2.1 초기화\nnumpy 의 array 로부터 tensor 를 초기화 할 수 있다.\n\n\nIn\n\nnparr1 = np.array([1,2,3,4])\nthtns1 = torch.from_numpy(nparr1)\nthtns2 = thtns1.to(\"cuda:0\")\n\nnparr1[0]=-1\nprint(nparr1)\nprint(thtns1)\nprint(thtns2)\nprint(\"-----\")\nthtns1[1]=-2\nprint(nparr1)\nprint(thtns1)\nprint(thtns2)\n\n\n\nOut\n\n[-1  2  3  4]\ntensor([-1,  2,  3,  4])\ntensor([1, 2, 3, 4], device='cuda:0')\n-----\n[-1 -2  3  4]\ntensor([-1, -2,  3,  4])\ntensor([1, 2, 3, 4], device='cuda:0')\n\nnparr 을 [1, 2, 3, 4] 로 초기화 하였고 이를 이용하여 cpu tensor thtns1 을 초기화하고 다시 이를 tensor.to() 함수를 사용하여 cuda tensor thtns2 로 초기화 하였다. torch.from_numpy 로 초기화 할 경우 tensor 와 array 는 동기화되어 하나를 변경하면 나머지 하나도 변화하지만 cuda tensor 인 thtns2 는 어떤 경우에도 변경되지 않는다.\n\n이제 거꾸로 cuda tensor 에서 cpu tensor, 그리고 np.array 로 가보자. tensor.cpu() 는 cuda tensor 로부터 cpu tensor 를 생성하며 tensor.numpy() 함수는 np.array 배열을 만든다. 역시 마찬가지로 이렇게 생성된 array 의 성분을 바꾸면 cpu tensor 는 같이 바뀌지만 cuda tensor 는 변경되지 않는다.\n\n\nIn\n\nts1 = torch.tensor([1,2,3,4], device=\"cuda:0\")\nts2 = ts1.cpu()\nar2 = ts2.numpy()\n\nar2[-1]=0\nprint(ts1)\nprint(ts2)\nprint(ar2)\n\n\n\nOut\n\ntensor([1, 2, 3, 4], device='cuda:0')\ntensor([1, 2, 3, 0])\n[1 2 3 0]\n\n\n\n\n2.2 텐서 타입\n텐서 타입은 텐서 성분의 타입과 텐서가 저장된 위치(cpu or cuda) 에 따라 결정된다. 성분의 타입이 dtype 이며 이에 따른 cpu 와 gpu 에서의 타입은 아래와 같다.\n\n\n\n\n\n\n\n\n\n성분의 타입\ndtype\nCPU tensor 타입\nGPU tensor 타입\n\n\n\n\n32-bit 부동소수\ntorch.float32, torch.float\ntorch.FloatTensor\ntorch.cuda.FloatTensor\n\n\n64-bit 부동소수\ntorch.float64, torch.double\ntorch.DoubleTensor\ntorch.cuda.DoubleTensor\n\n\n16-bit 부동소수\ntorch.float16, torch.half\ntorch.HalfTensor\ntorch.cuda.HalfTensor\n\n\n8-bit 부호 없는 정수\ntorch.uint8\ntorch.ByteTensor\ntorch.cuda.ByteTensor\n\n\n8-bit 정수\ntorch.int8\ntorch.CharTensor\ntorch.cuda.CharTensor\n\n\n16-bit 정수\ntorch.int16 or torch.short\ntorch.ShortTensor\ntorch.cuda.ShortTensor\n\n\n32-bit 정수\ntorch.int32 or torch.int\ntorch.IntTensor\ntorch.cuda.IntTensor\n\n\n64-bit integer\ntorch.int64 or torch.long\ntorch.LongTensor\ntorch.cuda.LongTensor\n\n\nBoolean\ntorch.bool\ntorch.BoolTensor\ntorch.cuda.BoolTensor\n\n\n\n\narray 혹은 tensor 의 dtype 은 생성시 지정할 수도 있고 변경할 수도 있다. numpy 의 astype() 메서드는 사용할 수 없고 torch.as_tensor() 함수를 아래와 같이 사용 할 수 있다.\n\n\nIn\n\nx1 = torch.tensor([1,2,3], dtype=torch.float16)\nprint(x1.dtype)\nx2 = torch.as_tensor(x1, dtype = torch.float32)\nprint(x2.dtype)\n\n\n\nOut\n\ntorch.float16\ntorch.float32\n\n\n\n\n\n\n\n\n기본 타입\n\n\n\nnp.array([1.0, 2.0]) 의 dtype 은 np.float64 이지만 torch.tensor([1.0, 2.0]) 의 dtype 은 cpu, cuda 모두 torch.float32 이다. 특별히 명시적으로 dtype 을 변환시키지 않고 array 와 tensor 사이의 변환은 dtype 을 유지한다.\n성분이 정수일 경우 array 와 tensor 모두 기본적인 dtype 은 int64 이다.\n\n\n\n\n\n2.3 tensor 를 생성할 때의 파라미터들\n\ndata : 텐서를 생성할 때 텐서의 성분이 되는 list, array 등의 데이터\ndtype : 텐서 성분 타입\ndevice : cpu, cuda, ‘cuda:0’ 등\nrequires_grad : True, False 중 하나의 값을 가지며 torch.autograd 에 의한 텐서의 gradient 추적할지 여부가 결정된다.\npin_memory : device=cpu 일 경우만 가능. CPU 에서 CUDA 로의 전송이 빠르다.\n\n\n\nIn\n\nts1 = torch.tensor(data = [1, 2, 2, 3], dtype = torch.float32, device=\"cpu\", requires_grad=True, pin_memory=True)\nprint(ts1)\n\n\n\nOut\n\ntensor([1., 2., 2., 3.], requires_grad=True)\n\n\n\n\n2.4 _ 로 끝나는 메서드\n다음 코드를 보자.\n\n\nIn\n\np1 = torch.tensor([-1.0, 2.0])\np2 = p1.abs()\nq1 = torch.tensor([-1.0, 2.0])\nq1.abs_()\nprint(\"p1 = \", p1)\nprint(\"p2 = \", p2)\nprint(\"q1 = \", q1)\n\n\n\nOut\n\np1 =  tensor([-1.,  2.])\np2 =  tensor([1., 2.])\nq1 =  tensor([1., 2.])\n\np1.abs() 는 p1 의 각 성분의 절대값으로 이루어진 텐서를 반환하지만 p1 을 바꾸지는 않는다. 그러나 q1.abs_() 는 q1 의 각 성분을 원래 성분의 절대값으로 바꾼다. Pytorch 에서 다른 것이 다 같고 맨 끝에 _ 가 붙지 않은 메서드와 붙은 메서드 가 있다면 _ 가 붙은 메서드는 결과값을 새로운 tensor 로 반환하는 것이 아니라 원래의 텐서를 변경한다. 이것을 In place 연산이라고 한다. _ 가 마지막에 붙지 않은 메서드는 연산 결과를 새로운 텐서로 반환한다.\n\n\n\n2.5 View 와 tensor.contiguous()\n\n참고자료 : tensor view\n\n예를 들어 텐서의 축 순서를 바꾸는 tensor.transpose() 는 원래의 텐서의 데이터로부터 새로운 데이터를 생성하지 않고 원본 텐서에 데이터를 참조하면서 그것이 보여지거나 연산되는 방식을 바꾼다. 이렇게 원본 텐서를 참조하면서 모양, 기능, 출력 등만 바뀌는 것을 View 라고 한다. 한 텐서가 다른 텐서의 View 라면 둘 중 하나의 성분을 바꾸면 나머지 하나도 당연히 같이 바뀌게 된다.\n이렇게 텐서의 내용을 복사 변경하지 않고 View 를 지원하는 메서드는 tensor.transpose(), tensor.expand(), tensor.diagonal() 등 다수가 있다. 기본적으로 tensor[1:-1:2, 2:] 와 같은 슬라이싱도 View 이다.\ntensor.is_contiguous() 메서드는 텐서가 View 인지 아닌지를 반환하며 tensor.contiguous() 메서드는 원본 텐서의 데이터로부터 원하는 모양과 기능을 가진 새로운 데이터셋을 가진 텐서를 만든다.\n\n\nIn\n\nt0 = torch.tensor([1,2,3,4,5.0])\nt1 = t0[1:-1:2]\nprint(t1.is_contiguous())\nt1=t1.contiguous() \nprint(t1.is_contiguous())\n\n\n\nOut\n\nFalse\nTrue\n\n\n\n\n2.6 텐서를 복사하는 세가지 방법\nTensor p0 에 대해\n\np1 = p0.detatch() : p1 는 p0 와 데이터를 공유하며 둘중 하나의 성분을 바꿔도 나머지에 반영되지만 p1.requires_grad 는 False 이다.\np2 = p0.clone() : 내용을 복사하여 하나를 변경해도 다른 하나에 반영되지 않는다.\np3 = p0.data : 사용하지 말것. pytorch 0.4.0 reliase note 의 What about .data 섹션 참고.\n\n\n\n\n2.7 Tensor class reference\n\nTensor class reference",
    "crumbs": [
      "AI & ML",
      "PyTorch",
      "소개"
    ]
  },
  {
    "objectID": "src/topics/socket.html#소켓-통신",
    "href": "src/topics/socket.html#소켓-통신",
    "title": "TCP/UDP 통신",
    "section": "1 소켓 통신",
    "text": "1 소켓 통신\n현재의 많은 장비들은 소켓 통신을 사용하여 데이터와 정보를 주고받는다. 예전의 장비들은 RS232 나 RS485 같은 직렬 통신이나 다른 다양한 통신 방법을 사용하는 경우가 많았지만 최근의 장비들은 소켓 통신을 통해 데이터를 전송하는 경우가 많다. 여기서는 소켓 통신에 대해 짧게 알아보고 실제 구현해 보기로 하자.\n\n\n소켓\n소켓은 같은 컴퓨터의 프로세스 사이에서, 혹은 서로 다른 컴퓨터(혹은 장비)의 프로세스 사이에서 통신을 수행할 때의 종단점 (end point)이다. 같은 컴퓨터의 프로세스 사이의 소켓에는 unix domain socket 이 있으며 다른 컴퓨터 프로세스 사이에서의 통신에는 Datagram socket 과 Streaming socket 이 있다. 각각의 네트워크 하드웨어는 사양과 내부 명령어가 다르지만 운영체제 수준에서 이를 추상화하여 서로 통신할 수 있도록 한 것이 소켓이다. 여기서는 서로 다른 컴퓨터의 프로세스간 통신에 대해서만 다루기로 한다.\n\n\n\n스트림 소켓\n전화와 같이 믿을 수 있는 양방향 통신을 제공한다. 즉 데이터를 주고받는 소켓의 양쪽이 성립된 상태에서 한쪽(서버)에서 다른 한쪽(클라이언트)으로의 연결을 초기화하고, 연결이 생성된 후에는 어느 쪽에서든 다른 쪽으로 통신할 수 있다. 데이터를 송신하고 나서 이 데이터가 실제로 도착했는지도 즉각 확인할 수 있다. 보통 전송 제어 프로토콜(Transmission Control Protocol, TCP)이라 불리는 표준 통신 프로토콜을 사용하며 이 외에도 SCTP(Stream Control Transmission Protocol) 나 DCCP (Datagram Congestion Control Protocol) 가 사용되지만 여기서는 다루지 않기로 한다. 컴퓨터 네트워크에서 데이터는 보통 패킷이라는 단위로 전송되는데, TCP는 패킷이 오류 없이 순서대로 도착하도록 설계되었다. 웹서버, 메일서버, 각 클라이언트 애플리케이션 모두는 TCP와 스트림 소켓을 사용한다.\n\n\n\n데이터그램 소켓\n데이터그램 소켓의 연결은 단방향이고 신뢰할 수 없다. IP 와 포트번호를 특정하여 보내지만 수신을 확인하지 않는다. 또한 데이터가 순서대로 전송된다고 보장할 수도 없다. 사용자 데이터그램 프로토콜(User Datagram Protocol, UDP)이라는 표준 프로토콜을 사용한다. TCP 에 비해 단순하고 간단하며 부하가 적고 빠른 방법이다. 패킷 손실이 허용되기도 하며 네트워크 게임이나 음악/동영상 스트리밍에서 자주 쓰인다. UDP 를 통해 신뢰성 있는 데이터 통신을 하고 싶다면 직접 패킷을 통해 구현해야 한다.\n\n\n\n서버와 클라이언트\n여기서는 접속을 기다리는 것이 서버(server), 기다리는 서버에 접근하는 것이 클라이언트(client) 라고 한다.",
    "crumbs": [
      "주제별",
      "TCP/UDP 통신"
    ]
  },
  {
    "objectID": "src/topics/socket.html#tcp-와-udp-를-이용한-간단한-에코-서버와-클라이언트의-저차원-구현",
    "href": "src/topics/socket.html#tcp-와-udp-를-이용한-간단한-에코-서버와-클라이언트의-저차원-구현",
    "title": "TCP/UDP 통신",
    "section": "2 TCP 와 UDP 를 이용한 간단한 에코 서버와 클라이언트의 저차원 구현",
    "text": "2 TCP 와 UDP 를 이용한 간단한 에코 서버와 클라이언트의 저차원 구현\n여기서는 Python 으로 간단한 서버와 클라이언트를 구현한다. 가장 기본적인 저차원 구현은 C 언어를 이용한 구현이지만 여기서는 이 C 구현과 가장 비슷하며 저차원 구현을 체험해 볼 수 있는 Python 구현을 통해 TCP 와 UDP 통신을 알아보기로 하자.\n\nTCP 통신\n서버가 대기하고 클라이언트와의 접속이 성립하면 클라이언트는 사용자로부터의 입력을 서버에게 보낸다. 서버는 클라이언트로부터 받은 메시지를 출력하고 그 메시지 끝에 # 을 붙여 클라이언트에게 보낸다. 클라이언트는 # 이 붙은 메시지를 받고 출력한다. 사용자로부터 -1 을 받으면 서버는 역시 # 이 붙은 메시지 -1# 을 클리이언트로 보내고 종료하며, 클라이언트 역시 종료한다.\nPython 으로 구현하지만 여기서의 방식은 소켓 통신의 기본적인 즉 low-level 방식으로 거의 모든 언어에서 기본적으로 지원한다. 물론 실제로는 이 방식보다는 이 방식을 좀 더 쓰기 편하게 만든 방식을 쓸 수도 있지만 기본 동작을 일단 확인하기 위해 low-level 방식으로 구현해보자.\n소켓 통신을 위해서는 socket 모듈을 임포트 해야 하고 소켓 객체를 구현해야 한다. 서버에는 두개의 소켓이 필요하다. 하나는 접속을 받아들이고 연결을 위한 역할을 수행하는 소켓(이하 연결 소켓)이며 다른 하나는 클라이언트와 데이터를 주고받기 위한 소캣(이하 서버 소켓)이다. 클라이언트는 데이터를 주고받는 소켓(이하 클라이언트 소켓) 하나만 있으면 된다. 연결 소켓, 서버소켓, 클라이언트 소켓은 편의상 지은 이름이며 다른 곳에서는 다른 이름으로 사용될 수 있다.\n\nTCP 서버\n일단 서버쪽에서 연결 소켓을 성립시켜야 한다. 연결 소켓을 위해서는 서버 IP 와 포트 번호가 필요하다. IP 는 \"123.123.123.12\" 형식의 문자열이며 포트 번호는 0 부터 65535 번까지의 정수이다. IP 와 포트번호는 Internet Assigned Numbers Authority (IANA) 에서 관리하는데 포트 번호중의 일부는 특별한 용도로 지정되어 있다. 자세한 것은 List of TCP and UDP port numbers 를 참고하라. 보통 49152 에서 65535 번호는 자유롭게 사용 할 수 있다.\n\n연결 소켓은 IP 와 포트번호, 그리고 프로토콜로 구성된다. 우선\nimport socket\n을 통해 소캣 모듈을 사용할 수 있도록 한다. 이후 아래와 같이 소켓 인스턴스를 만들고 프로토콜과 옵션을 정한다.\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nsocket.socket 의 첫번째 인자로는 다음이 사용될 수 있다.\n\nsocket.AF_INET 은 IPv4 즉 4 개의 8비트 정수로 지정된 IP 번호를 사용한는 것을 의미한다. 보퉁 많이 사용하는 123.123.123.12 형식이다.\nsocket.AF_INET6 는 IPv6 주소체계를 사용한다는 것을 의미하며 16비트 정수 4개를 사용한다.\nsocket.AF_UNIX 는 네트웍 통신이 아닌 프로세스간의 통신에 대해 사용한다.\n\nsocket.socket 의 두번째 인자로는 다음이 사용될 수 있다.\n\nsocket.SOCK_STREAM 은 TCP 통신 규약을 사용한다는 것을 의미한다.\nsocket.SOCK_DGRAM 은 UDP 통신 규약을 사용한다는 것을 의미한다.\n\n\nsocket.setsockopt 함수는 소켓의 옵션을 지정한다. 첫번째 인자는 프로토콜 레벨이며 두번째 인자는 프로토콜 레벨에서의 옵션 이름, 세번째 인자부터는 앞의 두 인자에 따른 설정값 등이 온다. 자세한 내용은 setsockopt 서브루틴 을 참고하라. 여기서 socket.SO_REUSEADDR 옵션의 옵션값을 1 로 설정하면 클라이언트 소켓을 닫은 후 같은 IP 와 포트번호로 연결할 수 있다.\n\nserver_socket.bind((host, port))\nserver_socket.listen(0)\nclient_soc, addr = server_socket.accept()\nbind((host, port)) 는 연결 소켓에 IP 번호(host) 와 포트(port) 를 지정한다. 이제 listen() 을 통해 클라이언트로부터 접속 요청 대기를 시작한다. listen 함수는 최대 대기 큐의 갯수를 인자로 받을 수 있며, 인자가 없을 경우 자동으로 할당된다.\naccept() 함수는 연결을 시도한 클라이언트와 통신 할 수 있는 소켓(client_soc), 즉 서버 소켓과 연결된 클라이언트의 (IP 주소, 포트) 튜플을 반환한다. 이 서버 소켓을 통해 클라이언트와 통신한다. 이제 서버의 코드를 보자.\n# TCP 서버 프로그램 \nimport socket, time\n\nhost, port = 'localhost', 43333\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nserver_socket.bind((host, port))\nserver_socket.listen()\n\nprint('TCP 서버 시작')\n\nclient_soc, addr = server_socket.accept()\n\nprint('연결된 client (IP, port):', addr)\n\n# 접속 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    try:\n        data = client_soc.recv(1024)\n    except ConnectionResetError:\n        print('ConnectionResetError')\n        connection_retained = False\n    finally:    \n        msg = data.decode() # 읽은 데이터 디코딩\n        print('받은 메시지 :', msg)\n        client_soc.sendall((msg+\"#\").encode(encoding='utf-8')) # 에코메세지 클라이언트로 보냄\n\n        # 클라이언트로부터 받은 메시지가 \"-1\" 이면 접속을 종료시킨다.\n        if msg == \"-1\":\n            connection_retained = False\n            print(\"to be closed\")        \n            client_soc.close()\n        \n    time.sleep(0.2)\n    \nserver_socket.close() # 사용했던 서버 소켓을 닫아줌 \n앞서 말했듯이 클라이언트로부터 -1 메시지를 받기 전까지는 계속 클라이언트와 통신해야 한다. 이를 위해 일단 connection_retianed = True 로 두고 이 변수값이 True 인 상황에서는 계속 주고 받도록 한다. 그러나 클라이언트로부터 -1 을 받으면 connection_refused = False 가 되며 while 루프를 벗어나고 소켓이 종료되고 (server_socket.close()) 프로그램도 종료한다.\n\n\n\nTCP 클라이언트\n클라이언트는 서버보다 간단한다.\nimport socket\nserver_ip, server_port = 'localhost', 43333\nsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n서버에 접속하기 위해서는 서버의 IP 와 포트 번호를 알야야 한다(server_ip, server_port). TCP 통신이므로 소켓은 server 의 연결소켓과 같이 설정한다. 이제 연결을 시도한다\nsocket.connect((server_ip, server_port))\n서버와 마찬가지로 연결 유지를 확인하기 위한 변수 connetion_retained 가 존재한다. 코드는 아래와 같다.\n# TCP 클라이언트 프로그램\nimport socket, time\n\nserver_ip, server_port = 'localhost', 43333\nsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nsocket.connect((server_ip, server_port))\nsocket.settimeout(0.1)\n\n# 연결 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    msg = input('msg:') # 서버로 보낼 msg 입력\n    \n    if msg == \"-1\":\n        connection_retained = False\n    socket.sendall(msg.encode(encoding='utf-8'))\n\n    # 서버가 에코로 되돌려 보낸 메시지를 클라이언트가 받음\n    try:\n        data = socket.recv(1024)\n    except TimeoutError:\n        print('TimeoutError')\n    finally:\n        msg = data.decode() # 읽은 데이터 디코딩\n        print('서버로부터 받은 메시지 :', msg)\nsocket.close()\nsocket.recv(1024) 는 서버로부터 최대 1024 바이트를 는다는 의미이다. 그러나 이 함수 실행 후 지정된 시간동안 응답이 없을 경우 TimeoutError 가 발생하며 소캣이 종료된다. 응답대기시간은 socket.settimeout() 함수를 통해 지정하며 인자로 초 단위의 시간을 입력한다. 인자가 없으면 무한정 기다린다.\n여기서는 응답 대기 시간이 길더라도 문제가 없지만 예를 들어 여러 서버로부터 데이터를 받는 경우를 생각해보자. 여기서 응답을 대기한다면 다른 작업을 수행 할 수가 없으며, 이를 위해 응답 대기시간을 짧게 잡고 다른 작업을 수행한 후 다시 메시지를 기다리는 것이 좋다.\n\n\n\n\nUDP 통신\nUDP 통신과 TCP 통신의 차이는 다음과 같다.\n\n소켓 타입이 socket.SOCK_STREAM 이 아닌 socket.SOCK_DGRAM 이다.\n연결 소켓이 없으며 서버소켓이 직접 bind 된다.\nTCP 에서는 recv, send 를 통해 데이터를 주고받지만 UDP 에서는 recvfrom, sendto 함수를 사용한다. 앞서 사용한 sendall 함수는 기본 함수인 send 를 python socket 모듈에서 확장한 것이다.\nrecvfrom 함수를 통해 데이터와 데이터를 보낸 주소를 얻으며, 이 주소가 원하는 주소가 않을 경우 무시한다.\nsendto 함수의 인자로는 데이터와 데이터를 받을 IP, 포트번호의 튜플을 전달한다.\n\n\n소스코드를 보라.\n\n\nUDP 서버\n# UDP 서버 프로그램\nimport socket, time\n\nhost, port = 'localhost', 43333\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nserver_socket.bind((host, port))\n# server_socket.listen()\n\nprint('UDP 서버 시작')\n\n#client_soc, addr = server_socket.accept()\naddr = (None, None)\n\nprint('연결된 client (IP, port):', addr)\n\n# 접속 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    try:\n        data, addr = server_socket.recvfrom(1024)\n    except ConnectionResetError:\n        print('ConnectionResetError')\n        connection_retained = False\n    finally:    \n        msg = data.decode() # 읽은 데이터 디코딩\n        print(addr, '받은 메시지 :', msg)\n        server_socket.sendto((msg+\"#\").encode(encoding='utf-8'), addr) # 에코메세지 클라이언트로 보냄\n\n        # 클라이언트로부터 받은 메시지가 \"-1\" 이면 접속을 종료시킨다.\n        if msg == \"-1\":\n            connection_retained = False\n            print(\"to be closed\")        \n            # client_soc.close()\n        \n    time.sleep(0.2)\n    \nserver_socket.close() # 사용했던 서버 소켓을 닫아줌 \n\n\n\nUDP 클라이언트\n# UDP 클라이언트 프로그램\nimport socket, time\n\nserver_addr = ('localhost', 43333)\nsocket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n# socket.connect((server_ip, server_port))\nsocket.settimeout(0.1)\n\n# 연결 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    msg = input('msg:') # 서버로 보낼 msg 입력\n    \n    if msg == \"-1\":\n        connection_retained = False\n    socket.sendto(msg.encode(encoding='utf-8'),server_addr)\n\n    # 서버가 에코로 되돌려 보낸 메시지를 클라이언트가 받음\n    try:\n        data, addr = socket.recvfrom(1024)\n    except TimeoutError:\n        print('TimeoutError')\n    finally:\n        msg = data.decode() # 읽은 데이터 디코딩\n        print('서버로부터 받은 메시지 :', msg)\nsocket.close()",
    "crumbs": [
      "주제별",
      "TCP/UDP 통신"
    ]
  },
  {
    "objectID": "src/tools/unixLinux/git.html#팁들",
    "href": "src/tools/unixLinux/git.html#팁들",
    "title": "Git 관련",
    "section": "1 팁들",
    "text": "1 팁들\n\n1.1 터미널에서 git 관련 alias 정리\n우연히 terminal 에서 alias 를 쳤을 때 git 관련 alias 가 아주 많이 지정되어 있는 것을 발견했다. 혹시나 명령어를 잘못 입력해서 git 가 꼬일 수 있기 때문에 이를 제거하고 싶었다. 방법은 .zshrc 에서\nplugins=(git)\n를 찾아 주석처리(#) 해준다.",
    "crumbs": [
      "Tools",
      "Unix/Linux 관련",
      "Git 관련"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_wavelet.html",
    "href": "src/tools/tikz/tikz_wavelet.html",
    "title": "tikz Wavelet",
    "section": "",
    "text": "\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2]\n\n    \\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n    \n    \\draw[color=black, thick, domain=-1:0, variable = \\t]   plot ({\\t}, {(1+\\t)});\n    \\draw[color=black, thick, domain=0:1, variable = \\t]   plot ({\\t}, {(1-\\t)});\n    \\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-a$};\n    \\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$a$};\n    \n    \\begin{scope}[xshift=3.5cm] \n    \n    \\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n    \n    \\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, { (2 * sin((\\t * 180 / pi) /2)/((\\t)))^2 });\n    \\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n    \\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 1: Wavelet-1\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n    \\draw[-{stealth}] (-1.2, 0) -- (1.22,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n    \n    \\draw[color=black, thick, domain=-6:0, variable = \\t]   plot ({\\t/5}, {exp(\\t)});\n    \\draw[color=black, thick, domain=0:6, variable = \\t]   plot ({\\t/5}, {exp(-\\t)});\n    \\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-5a$};\n    \\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$5a$};\n    \\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$1$};\n    \n    \\begin{scope}[xshift=2.8cm] \n    \n    \\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n    \n    \\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, {1/(\\t*\\t + 1)});\n    \\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$2/a$};\n    \\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n    \\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 2: Wavelet-2\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n    \\draw[-{stealth}] (-0.5, 0) -- (1.7,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$\\psi_H(t)$};\n    \n    \\draw[very thick] (-0.5, 0) -- (0, 0) -- (0, 1) -- (0.5, 1) -- (0.5, -1) -- (1, -1)-- (1, 0) -- (1.5, 0);\n    \\node[] at (0, 0) [below left] {$0$};\n    \\node[] at (0.5, 0) [below left] {$\\frac{1}{2}$};\n    \\node[] at (1, 0) [below left] {$1$};\n    \\draw[] (0.1, 1) -- (-0.1, 1) node[left] {$1$};\n    \\node[] at (0, -1) [left] {$-1$};\n    \n    \\begin{scope}[xshift=3.3cm] \n    \n    \\draw[-{stealth}] (-1.2, -1) -- (1.2,-1) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,-1.0) -- (0,1.2) node[above] {$|\\hat{\\psi}_H(\\omega)|$};\n    \n    \\draw[color=black, very thick, domain=0.01:40, samples=100, variable = \\t]   plot ({\\t/40}, {(2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n    \\draw[color=black, very thick, domain=-40:-0.01, samples=100, variable = \\t]   plot ({\\t/40}, {(-2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n    \\draw[] (0.0, -0.96) -- (0.0, -1.04 ) node[below] {$0$};\n    \\draw[] (0.314, -0.96) -- (0.314, -1.04 ) node[below] {$4\\pi$};\n    \\draw[] (0.628, -0.96) -- (0.628, -1.04 ) node[below] {$8\\pi$};\n    \\draw[] (0.942, -0.96) -- (0.942, -1.04 ) node[below] {$12\\pi$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 3: Haar wavelet\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=3]\n\n    \\draw[-{stealth}] (-0.5, 0) -- (2.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.7) -- (0,0.7) node[above] {$(\\psi_H \\ast \\phi)(t)$};\n    \n    \\draw[very thick] (-0.5, 0) -- (0, 0) -- (0.5, 0.5) -- (1.5, -0.5) -- (2, 0) -- (2.4, 0);\n    \\node[] at (0, 0) [below left] {$0$};\n    \\draw [] (0.5, 0.03) -- (0.5, -0.03) node[below] {$\\frac{1}{2}$};\n    \\draw [] (1, 0.03) -- (1, -0.03) node[below] {$1$};\n    \\draw [] (1.5, 0.03) -- (1.5, -0.03) node[below] {$\\frac{3}{2}$};\n    \\draw [] (2, 0.03) -- (2, -0.03) node[below] {$2$};\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 4: convolution of Haar\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a=1, b=0}(t)$};\n    \\draw[color=black, very thick, domain=-1:1, samples=100, variable = \\t]   plot ({\\t}, {exp(-(4*\\t)^2) * cos(\\t*180/pi*10)});\n    \n    \n    \\begin{scope}[xshift=2.3cm] \n    \n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&gt;1, b&gt;0} (t)$};\n    \n    \\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(8*(\\t-0.2))^2) * cos(2*(\\t-0.2)*180/pi*10)});\n    \n    \\end{scope}\n    \n    \\begin{scope}[xshift=4.6cm] \n    \n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&lt;1, b&gt;0}(t)$};\n    \n    \\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(2*(\\t-0.2))^2) * cos(0.5*(\\t-0.2)*180/pi*10)});\n    \n    \\end{scope}\n    \n    \n    \\end{tikzpicture}\n\\end{document}\n \n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\psi_{1, 0}(t)$};\n\\draw[color=black, very thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/6}, {(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw (-0.289, -0.02)-- (-0.289, 0.02) node[above, scale=0.8] {$-\\sqrt{3}$};\n\\draw (0.289, -0.02)-- (0.289, 0.02) node[above, scale=0.8] {$\\sqrt{3}$};\n\n\\begin{scope}[xshift=2.3cm] \n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\hat{\\psi}_{1, 0} (t)$};\n\n\\draw[color=black, very thick, domain=-7:7, samples=200, variable = \\t]   plot ({\\t/7}, {\\t*\\t*exp(-\\t*\\t/2)});\n\n\\draw (-0.202, 0.02)-- (-0.202, -0.02) node[below, scale=0.8] {$-\\sqrt{2}$};\n\\draw (0.202, 0.02)-- (0.202, -0.02) node[below, scale=0.8] {$\\sqrt{2}$};\n\\end{scope}\n    \n\\end{tikzpicture}\n\\end{document}\n \n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-6.3, 0) -- (6.3,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-4) -- (0,6) node[above] {$$};\n\\draw[color=black, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, {3*(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw[color=red, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(2)/sqrt(3) * (1-((\\t+2)/1.5)^2)*exp(-(((\\t+2)/1.5)^2)/2)});\n\n\\draw[color=teal, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(4)* (1-((\\t-sqrt(2))*4)^2)*exp(-(((\\t-sqrt(2))*4)^2)/2)});\n\n\\node [left] at (-0.1, 3) {$\\psi_{1, 0}(\\omega)$};\n\\node [left, red] at (-3, 1) {$\\psi_{\\frac{3}{2}, -2}(\\omega)$};\n\\node [right, teal] at (1.5, 4) {$\\psi_{\\frac{1}{4}, -\\sqrt{2}}(\\omega)$};\n\n\\foreach \\x in {1,...,5}\n{\n    \\draw (\\x, 0.1)-- (\\x, -0.1) node[below, scale=0.8] {$\\x$};\n    \\draw (-\\x, 0.1)-- (-\\x, -0.1) node[below, scale=0.8] {$-\\x$};\n}\n\n\\end{tikzpicture}\n\\end{document}\n \n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-0.5, 0) -- (8,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,4) node[above] {$\\omega$};\n\n\\foreach \\x in {0,...,2}\n{\n    \\draw (2*\\x +2 , 0.1)-- (2*\\x +2, 0.0) node[below, scale=0.8] {$b_\\x + a_\\x t_m$};\n    \\draw (0.1, \\x +1)-- (0.0, \\x +1) node[left, scale=0.8] {$\\omega_m/a_\\x$};\n    \\draw[dashed] (2*\\x+2, 0.0) -- (2*\\x+2, \\x+1) -- (0, \\x+1);\n}\n\n\\draw [thick, red] (1, 0.7) -- (3, 0.7) -- (3, 1.3) -- (1, 1.3) -- cycle;\n\\node [above, red, scale=0.8] at (2, 1.3) {$2a_0 \\sigma_t$};\n\\node [right, red, scale=0.8] at (3, 1.0) {$\\dfrac{2}{a_0}\\sigma_\\omega$};\n\n\\draw [thick, blue] (3.5, 1.6) -- (4.5, 1.6) -- (4.5, 2.4) -- (3.5, 2.4) -- cycle;\n\\node [above, blue, scale=0.8] at (4, 2.4) {$2a_1 \\sigma_t$};\n\\node [right, blue, scale=0.8] at (4.5, 2.0) {$\\dfrac{2}{a_1}\\sigma_\\omega$};\n\n\\draw [thick, teal] (5.8, 2) -- (5.8, 4) -- (6.2, 4) -- (6.2, 2) -- cycle;\n\\node [above, teal, scale=0.8] at (6, 4) {$2a_2 \\sigma_t$};\n\\node [right, teal, scale=0.8] at (6.2, 3.0) {$\\dfrac{2}{a_2}\\sigma_\\omega$};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 5: Wavelet resolution\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n    \\draw[-{stealth}] (0, 0) -- (8.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,4) node[above] {$\\omega$};\n    \n    \\draw [thick] (0, 0.5) -- (8, 0.5);\n    \\draw [thick] (0, 1.5) -- (8, 1.5);\n    \\draw [thick] (0, 3.5) -- (8, 3.5);\n    \n    \n    \\filldraw[black] (0, 0) circle (1pt) node [below, scale=0.7] {$n=0$};\n    \\filldraw[black] (4, 0) circle (1pt) node [below, scale=0.7] {$n=1$};\n    \\filldraw[black] (8, 0) circle (1pt) node [below, scale=0.7] {$n=2$};\n    \\filldraw[black] (0, 0) circle (1pt) node [left, scale=0.7] {$m=0$};\n    \\filldraw[black] (0, 0.5) circle (1pt) node [left, scale=0.7] {$m=-1$};\n    \\filldraw[black] (0, 1.5) circle (1pt) node [left, scale=0.7] {$m=-2$};\n    \\filldraw[black] (0, 3.5) circle (1pt) node [left, scale=0.7] {$m=-3$};\n    \n    \\foreach \\x in {0,...,4}\n    {\n    \\draw [thick] (2*\\x, 0) -- (2*\\x, 3.5);\n    \n    }\n    \n    \n    \\foreach \\x in {0,...,8}\n    {\n    \\draw [thick] (\\x, 0.5) -- (\\x, 3.5);\n    }\n    \n    \\foreach \\x in {0,...,15}\n    {\n    \\draw [thick] (\\x/2, 1.5) -- (\\x/2, 3.5);\n    }\n    \n    \\draw[{stealth}-{stealth}] (7.5, 3.7) -- (8.0,3.7);\n    \\node[above] at (7.75, 3.7) {$\\sigma_t$};\n    \n    \\draw[{stealth}-{stealth}] (8.2, 1.5) -- (8.2,3.5);\n    \\node[right] at (8.2, 2.5) {$\\sigma_\\omega$};\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 6: Wavelet resolution-2",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz Wavelet"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_wavelet.html#wavelets",
    "href": "src/tools/tikz/tikz_wavelet.html#wavelets",
    "title": "tikz Wavelet",
    "section": "",
    "text": "\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2]\n\n    \\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n    \n    \\draw[color=black, thick, domain=-1:0, variable = \\t]   plot ({\\t}, {(1+\\t)});\n    \\draw[color=black, thick, domain=0:1, variable = \\t]   plot ({\\t}, {(1-\\t)});\n    \\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-a$};\n    \\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$a$};\n    \n    \\begin{scope}[xshift=3.5cm] \n    \n    \\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n    \n    \\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, { (2 * sin((\\t * 180 / pi) /2)/((\\t)))^2 });\n    \\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n    \\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 1: Wavelet-1\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n    \\draw[-{stealth}] (-1.2, 0) -- (1.22,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n    \n    \\draw[color=black, thick, domain=-6:0, variable = \\t]   plot ({\\t/5}, {exp(\\t)});\n    \\draw[color=black, thick, domain=0:6, variable = \\t]   plot ({\\t/5}, {exp(-\\t)});\n    \\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-5a$};\n    \\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$5a$};\n    \\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$1$};\n    \n    \\begin{scope}[xshift=2.8cm] \n    \n    \\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n    \n    \\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, {1/(\\t*\\t + 1)});\n    \\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$2/a$};\n    \\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n    \\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 2: Wavelet-2\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n    \\draw[-{stealth}] (-0.5, 0) -- (1.7,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$\\psi_H(t)$};\n    \n    \\draw[very thick] (-0.5, 0) -- (0, 0) -- (0, 1) -- (0.5, 1) -- (0.5, -1) -- (1, -1)-- (1, 0) -- (1.5, 0);\n    \\node[] at (0, 0) [below left] {$0$};\n    \\node[] at (0.5, 0) [below left] {$\\frac{1}{2}$};\n    \\node[] at (1, 0) [below left] {$1$};\n    \\draw[] (0.1, 1) -- (-0.1, 1) node[left] {$1$};\n    \\node[] at (0, -1) [left] {$-1$};\n    \n    \\begin{scope}[xshift=3.3cm] \n    \n    \\draw[-{stealth}] (-1.2, -1) -- (1.2,-1) node[right] {$\\omega$};\n    \\draw[-{stealth}] (0,-1.0) -- (0,1.2) node[above] {$|\\hat{\\psi}_H(\\omega)|$};\n    \n    \\draw[color=black, very thick, domain=0.01:40, samples=100, variable = \\t]   plot ({\\t/40}, {(2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n    \\draw[color=black, very thick, domain=-40:-0.01, samples=100, variable = \\t]   plot ({\\t/40}, {(-2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n    \\draw[] (0.0, -0.96) -- (0.0, -1.04 ) node[below] {$0$};\n    \\draw[] (0.314, -0.96) -- (0.314, -1.04 ) node[below] {$4\\pi$};\n    \\draw[] (0.628, -0.96) -- (0.628, -1.04 ) node[below] {$8\\pi$};\n    \\draw[] (0.942, -0.96) -- (0.942, -1.04 ) node[below] {$12\\pi$};\n    \n    \\end{scope}\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 3: Haar wavelet\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 100, scale=3]\n\n    \\draw[-{stealth}] (-0.5, 0) -- (2.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.7) -- (0,0.7) node[above] {$(\\psi_H \\ast \\phi)(t)$};\n    \n    \\draw[very thick] (-0.5, 0) -- (0, 0) -- (0.5, 0.5) -- (1.5, -0.5) -- (2, 0) -- (2.4, 0);\n    \\node[] at (0, 0) [below left] {$0$};\n    \\draw [] (0.5, 0.03) -- (0.5, -0.03) node[below] {$\\frac{1}{2}$};\n    \\draw [] (1, 0.03) -- (1, -0.03) node[below] {$1$};\n    \\draw [] (1.5, 0.03) -- (1.5, -0.03) node[below] {$\\frac{3}{2}$};\n    \\draw [] (2, 0.03) -- (2, -0.03) node[below] {$2$};\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 4: convolution of Haar\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a=1, b=0}(t)$};\n    \\draw[color=black, very thick, domain=-1:1, samples=100, variable = \\t]   plot ({\\t}, {exp(-(4*\\t)^2) * cos(\\t*180/pi*10)});\n    \n    \n    \\begin{scope}[xshift=2.3cm] \n    \n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&gt;1, b&gt;0} (t)$};\n    \n    \\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(8*(\\t-0.2))^2) * cos(2*(\\t-0.2)*180/pi*10)});\n    \n    \\end{scope}\n    \n    \\begin{scope}[xshift=4.6cm] \n    \n    \\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&lt;1, b&gt;0}(t)$};\n    \n    \\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(2*(\\t-0.2))^2) * cos(0.5*(\\t-0.2)*180/pi*10)});\n    \n    \\end{scope}\n    \n    \n    \\end{tikzpicture}\n\\end{document}\n \n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\psi_{1, 0}(t)$};\n\\draw[color=black, very thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/6}, {(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw (-0.289, -0.02)-- (-0.289, 0.02) node[above, scale=0.8] {$-\\sqrt{3}$};\n\\draw (0.289, -0.02)-- (0.289, 0.02) node[above, scale=0.8] {$\\sqrt{3}$};\n\n\\begin{scope}[xshift=2.3cm] \n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\hat{\\psi}_{1, 0} (t)$};\n\n\\draw[color=black, very thick, domain=-7:7, samples=200, variable = \\t]   plot ({\\t/7}, {\\t*\\t*exp(-\\t*\\t/2)});\n\n\\draw (-0.202, 0.02)-- (-0.202, -0.02) node[below, scale=0.8] {$-\\sqrt{2}$};\n\\draw (0.202, 0.02)-- (0.202, -0.02) node[below, scale=0.8] {$\\sqrt{2}$};\n\\end{scope}\n    \n\\end{tikzpicture}\n\\end{document}\n \n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-6.3, 0) -- (6.3,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-4) -- (0,6) node[above] {$$};\n\\draw[color=black, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, {3*(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw[color=red, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(2)/sqrt(3) * (1-((\\t+2)/1.5)^2)*exp(-(((\\t+2)/1.5)^2)/2)});\n\n\\draw[color=teal, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(4)* (1-((\\t-sqrt(2))*4)^2)*exp(-(((\\t-sqrt(2))*4)^2)/2)});\n\n\\node [left] at (-0.1, 3) {$\\psi_{1, 0}(\\omega)$};\n\\node [left, red] at (-3, 1) {$\\psi_{\\frac{3}{2}, -2}(\\omega)$};\n\\node [right, teal] at (1.5, 4) {$\\psi_{\\frac{1}{4}, -\\sqrt{2}}(\\omega)$};\n\n\\foreach \\x in {1,...,5}\n{\n    \\draw (\\x, 0.1)-- (\\x, -0.1) node[below, scale=0.8] {$\\x$};\n    \\draw (-\\x, 0.1)-- (-\\x, -0.1) node[below, scale=0.8] {$-\\x$};\n}\n\n\\end{tikzpicture}\n\\end{document}\n \n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-0.5, 0) -- (8,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,4) node[above] {$\\omega$};\n\n\\foreach \\x in {0,...,2}\n{\n    \\draw (2*\\x +2 , 0.1)-- (2*\\x +2, 0.0) node[below, scale=0.8] {$b_\\x + a_\\x t_m$};\n    \\draw (0.1, \\x +1)-- (0.0, \\x +1) node[left, scale=0.8] {$\\omega_m/a_\\x$};\n    \\draw[dashed] (2*\\x+2, 0.0) -- (2*\\x+2, \\x+1) -- (0, \\x+1);\n}\n\n\\draw [thick, red] (1, 0.7) -- (3, 0.7) -- (3, 1.3) -- (1, 1.3) -- cycle;\n\\node [above, red, scale=0.8] at (2, 1.3) {$2a_0 \\sigma_t$};\n\\node [right, red, scale=0.8] at (3, 1.0) {$\\dfrac{2}{a_0}\\sigma_\\omega$};\n\n\\draw [thick, blue] (3.5, 1.6) -- (4.5, 1.6) -- (4.5, 2.4) -- (3.5, 2.4) -- cycle;\n\\node [above, blue, scale=0.8] at (4, 2.4) {$2a_1 \\sigma_t$};\n\\node [right, blue, scale=0.8] at (4.5, 2.0) {$\\dfrac{2}{a_1}\\sigma_\\omega$};\n\n\\draw [thick, teal] (5.8, 2) -- (5.8, 4) -- (6.2, 4) -- (6.2, 2) -- cycle;\n\\node [above, teal, scale=0.8] at (6, 4) {$2a_2 \\sigma_t$};\n\\node [right, teal, scale=0.8] at (6.2, 3.0) {$\\dfrac{2}{a_2}\\sigma_\\omega$};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 5: Wavelet resolution\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n    \\draw[-{stealth}] (0, 0) -- (8.5,0) node[right] {$t$};\n    \\draw[-{stealth}] (0,0) -- (0,4) node[above] {$\\omega$};\n    \n    \\draw [thick] (0, 0.5) -- (8, 0.5);\n    \\draw [thick] (0, 1.5) -- (8, 1.5);\n    \\draw [thick] (0, 3.5) -- (8, 3.5);\n    \n    \n    \\filldraw[black] (0, 0) circle (1pt) node [below, scale=0.7] {$n=0$};\n    \\filldraw[black] (4, 0) circle (1pt) node [below, scale=0.7] {$n=1$};\n    \\filldraw[black] (8, 0) circle (1pt) node [below, scale=0.7] {$n=2$};\n    \\filldraw[black] (0, 0) circle (1pt) node [left, scale=0.7] {$m=0$};\n    \\filldraw[black] (0, 0.5) circle (1pt) node [left, scale=0.7] {$m=-1$};\n    \\filldraw[black] (0, 1.5) circle (1pt) node [left, scale=0.7] {$m=-2$};\n    \\filldraw[black] (0, 3.5) circle (1pt) node [left, scale=0.7] {$m=-3$};\n    \n    \\foreach \\x in {0,...,4}\n    {\n    \\draw [thick] (2*\\x, 0) -- (2*\\x, 3.5);\n    \n    }\n    \n    \n    \\foreach \\x in {0,...,8}\n    {\n    \\draw [thick] (\\x, 0.5) -- (\\x, 3.5);\n    }\n    \n    \\foreach \\x in {0,...,15}\n    {\n    \\draw [thick] (\\x/2, 1.5) -- (\\x/2, 3.5);\n    }\n    \n    \\draw[{stealth}-{stealth}] (7.5, 3.7) -- (8.0,3.7);\n    \\node[above] at (7.75, 3.7) {$\\sigma_t$};\n    \n    \\draw[{stealth}-{stealth}] (8.2, 1.5) -- (8.2,3.5);\n    \\node[right] at (8.2, 2.5) {$\\sigma_\\omega$};\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 6: Wavelet resolution-2",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz Wavelet"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_functions.html",
    "href": "src/tools/tikz/tikz_functions.html",
    "title": "tikz function plots in Quarto",
    "section": "",
    "text": "\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw[very thin,color=gray] (-0.1,-0.1) grid (4.2,5.2);\n    \n    \\draw[-&gt;] (-0.6, 0) -- (4.2,0) node[right] {$x$};\n    \\draw[-&gt;] (0,-0.6) -- (0,5.2) node[above] {$y$};\n    \n    \\foreach \\x in {1,...,8}\n    {\n      \\draw[thin] (\\x / 2, 0.05) -- (\\x /2, -0.05);\n      }\n    \n    \\foreach \\x in {2, 4, 6, 8}\n    {\n      \\node[below]  at (\\x /2 , -0.05) {$\\x$};\n      }\n    \n    \\foreach \\y in {1,...,9}\n    {\n      \\draw[thin] (0.05 , \\y / 2) -- (-0.05 , \\y /2);\n      }\n    \n    \\foreach \\y in {2, 4, 6, 8, 10}\n    {\n      \\node[left]  at (-0.05, \\y / 2) {$\\y$};\n      }\n    \n    \\filldraw[black] (0.5,1.1 /2) circle (2pt);\n    \\filldraw[black] (2/2, 1.65 /2) circle (2pt);\n    \\filldraw[black] (3/2, 3.43/2) circle (2pt);\n    \\filldraw[black] (4/2, 4.02/2) circle (2pt);\n    \\filldraw[black] (5/2, 4.58/2) circle (2pt);\n    \\filldraw[black] (6/2, 5.78/2) circle (2pt);\n    \\filldraw[black] (7/2, 7.32/2) circle (2pt);\n    \n    \\draw[thick, dashed, red] (0, 0) -- (3.8, 3.8);\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 1: 선형회귀\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[domain=-1:1, samples = 100, scale=3]\n\n    \\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$x$};\n    \\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$y$};\n    \\node[left, scale=0.8] at (0, 1) {$1$};\n    \\node[left, scale=0.8] at (0, -1) {$-1$};\n    \\node[below, scale=0.8] at (-1, 0) {$-1$};\n    \\node[below, scale=0.8] at (1, 0) {$1$};\n    \n    \\foreach \\x in {-5,...,5}\n    {\n      \\draw[thin] (\\x / 5, 0.02) -- (\\x /5, -0.02);\n      }\n    \n    \\foreach \\y in {-5,...,5}\n    {\n      \\draw[thin] (0.02 , \\y / 5) -- (-0.02 , \\y /5);\n      }\n    \n    \\draw[color=black]   plot (\\x, \\x) ;\n    \\node[above, black] at (0.7, 0.73) {$T_1(x)$};\n    \\draw[color=blue]   plot (\\x, 2 * \\x * \\x - 1);\n    \\node[below right, blue] at (0, -1) {$T_2(x)$};\n    \\draw[color=red]   plot (\\x, 4 * \\x * \\x * \\x - 3* \\x);\n    \\node[above, red] at (-0.5, 1) {$T_3(x)$};\n    \\draw[color=teal]   plot (\\x, 8 * \\x * \\x * \\x * \\x - 8* \\x * \\x + 1);\n    \\node[right, teal] at (0.2, 0.8) {$T_4(x)$};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 2: 체비셰프 다항식\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[domain=-1:1, samples = 100, scale=3]\n\n    \\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$x$};\n    \\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$y$};\n    \\node[left, scale=0.8] at (0, 1) {$1$};\n    \\node[left, scale=0.8] at (0, -1) {$-1$};\n    \\node[below, scale=0.8] at (-1, 0) {$-1$};\n    \\node[below, scale=0.8] at (1, 0) {$1$};\n    \n    \\foreach \\x in {-5,...,5}\n    {\n      \\draw[thin] (\\x / 5, 0.02) -- (\\x /5, -0.02);\n      }\n    \n    \\foreach \\y in {-5,...,5}\n    {\n      \\draw[thin] (0.02 , \\y / 5) -- (-0.02 , \\y /5);\n      }\n    \n    \\draw[color=black]   plot (\\x, 1.5 * \\x * \\x - 0.5 ) ;\n    \\node[above, black] at (-0.7, 0.73) {$P_2(x)$};\n    \\draw[color=blue]   plot (\\x, 2.5 * \\x * \\x * \\x - 1.5 * \\x);\n    \\node[above, blue] at (-0.5, 0.4) {$P_3(x)$};\n    \\draw[color=red]   plot (\\x, 35/8 * \\x * \\x * \\x *\\x - 30 / 8 * \\x *\\x + 3/8 );\n    \\node[red] at (-0.5, -0.5) {$P_4(x)$};\n    \\draw[color=teal]   plot (\\x, 63/8 * \\x * \\x * \\x * \\x * \\x - 70/8* \\x * \\x * \\x + 15/8 * \\x);\n    \\node[right, teal] at (0.2, 0.5) {$P_5(x)$};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 3: 르장드르 다항식",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz function plots in Quarto"
    ]
  },
  {
    "objectID": "src/tools/Cpp/vcpkg.html#소개",
    "href": "src/tools/Cpp/vcpkg.html#소개",
    "title": "vcpkg",
    "section": "1 소개",
    "text": "1 소개\n\nMicrosoft가 오픈소스로 제공하는 C++용 패키지 관리자.\n원조의 설명서 : vcpkg 설명서",
    "crumbs": [
      "Tools",
      "C/C++ 관련",
      "vcpkg"
    ]
  },
  {
    "objectID": "src/posts/index.html",
    "href": "src/posts/index.html",
    "title": "The Notebook",
    "section": "",
    "text": "첫번째 글\n\n\n\n\n\n\n\n\n\n\n\nJulia_KAERI\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "src/gpu/cuda/04_hardware_implementation.html",
    "href": "src/gpu/cuda/04_hardware_implementation.html",
    "title": "Hardware Implementation",
    "section": "",
    "text": "NVIDIA GPU 아키텍처는 다중 스레드 스트리밍 멀티프로세서(SM)의 확장 가능한 배열을 중심으로 구축되어 있다. 호스트 CPU 의 CUDA 프로그램이 커널 그리드를 호출하면, 그리드의 블록들이 실행 가능한 용량을 가진 멀티프로세서에 할당된다. 스레드 블록의 스레드들은 하나의 멀티프로세서에서 동시에 실행되며, 여러 스레드 블록이 하나의 멀티프로세서에서 동시에 실행될 수 있다. 스레드 블록이 종료되면, 빈 멀티프로세서에서 새로운 블록이 실행된다.\n멀티프로세서는 수백 개의 스레드를 동시 실행하도록 설계되었다. 이렇게 많은 스레드를 관리하기 위해 SIMT(Single-Instruction, Multiple-Thread)라는 독창적인 아키텍처를 사용하며, 이에 대해서는 SIMT 아키텍처(SIMT Architecture) 에서 설명합니다. 명령어들은 파이프라인으로 처리되어, 단일 스레드 내의 명령어 수준 병렬성뿐만 아니라, 하드웨어의 동시 멀티스레딩을 통한 광범위한 스레드 수준 병렬성도 활용\u001f다. 하드웨어 멀티스레딩에 대한 자세한 내용은 하드웨어 멀티스레딩(Hardware Multithreading)에서 설명한다. CPU 코어와 달리, 명령어는 순서대로 발행되며 브랜치 예측(branch prediction) 이나 추측 실행(speculative execution)은 없다.\nSIMT 아키텍처와 하드웨어 멀티스레딩 은 모든 디바이스에 공통적으로 적용되는 스트리밍 멀티프로세서의 아키텍처 특징을 설명합니다. 각각 Compute Capability 5.x, Compute Capability 6.x 및 Compute Capability 7.x 에 대한 세부 사항은 compute capability 5.x, 6.x, 7.x에서 제공됩니다.\nNVIDIA GPU 아키텍처는 리틀 엔디언 표현을 사용한다."
  },
  {
    "objectID": "src/gpu/cuda/04_hardware_implementation.html#sec-cuda_simt_archtecture",
    "href": "src/gpu/cuda/04_hardware_implementation.html#sec-cuda_simt_archtecture",
    "title": "Hardware Implementation",
    "section": "1 SIMT 아키텍쳐",
    "text": "1 SIMT 아키텍쳐"
  },
  {
    "objectID": "src/gpu/cuda/04_hardware_implementation.html#sec-cuda_hardware_multithreading",
    "href": "src/gpu/cuda/04_hardware_implementation.html#sec-cuda_hardware_multithreading",
    "title": "Hardware Implementation",
    "section": "2 하드웨어 멀티스레딩(Hardware Multithreading)",
    "text": "2 하드웨어 멀티스레딩(Hardware Multithreading)"
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html",
    "href": "src/gpu/cuda/02_programming_model.html",
    "title": "Programming Model",
    "section": "",
    "text": "CUDA C++는 프로그래머가 커널이라는 C++ 함수를 정의할 수 있도록 하여 C++를 확장한 것.\n이 함수는 호출되면 N개의 서로 다른 CUDA 스레드에 의해 병렬로 N번 실행된다. 반면 일반 C++ 함수는 한 번만 실행된다.\n\n\n간단한 커널의 예는 아래와 같다.\n// Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation with N threads\n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ...\n}",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#커널",
    "href": "src/gpu/cuda/02_programming_model.html#커널",
    "title": "Programming Model",
    "section": "",
    "text": "CUDA C++는 프로그래머가 커널이라는 C++ 함수를 정의할 수 있도록 하여 C++를 확장한 것.\n이 함수는 호출되면 N개의 서로 다른 CUDA 스레드에 의해 병렬로 N번 실행된다. 반면 일반 C++ 함수는 한 번만 실행된다.\n\n\n간단한 커널의 예는 아래와 같다.\n// Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation with N threads\n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ...\n}",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_thread_hierarchy",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_thread_hierarchy",
    "title": "Programming Model",
    "section": "2 스레드 계층 구조 : 스레드 \\(\\rightarrow\\) 블럭 \\(\\rightarrow\\) 그리드",
    "text": "2 스레드 계층 구조 : 스레드 \\(\\rightarrow\\) 블럭 \\(\\rightarrow\\) 그리드\n\nthreadIdx 는 3차원 벡터로 1, 2, 3 차원 스레드 인덱스를 이용하여 스레드를 특정할 수 있다.\n스레드 블럭(Thread block) : 1, 2 혹은 3차원의 스레드 인덱스의 집합\n\n\n\n스레드 인덱스와 스레드 ID\n\n1차원 블럭 : ID = 인덱스\n2차원 블럭 : 블럭 사이즈가 \\((D_x, D_y)\\) 일 경우 스레드 인덱스 \\((x,\\,y)\\) 에 대한 \\(\\text{ID} = x+yD_x\\).\n3차원 블럭 : 블걱 사이즈가 \\((D_x, D_y, D_z)\\) 일 경우 스레드 인덱스 \\((x,\\,y,\\,z)\\) 에 대한 \\(\\text{ID} = x+yD_x + zD_x D_y\\).\n\n\n\n\n블럭당 스레드 수 제한 : 블럭의 모든 스레드는 같은 SM(streaming multiprocess) 코어 안에 있어야 하며 그 코어의 제한된 메모리 자원을 공유한다. 현재의 GPU 에서 하나의 블럭은 최대 1024 개의 스레드를 포함 할 수 있다.\n커널은 같은 형상의 다수의 스레드 블럭에서 실행되며, 따라서 총 스레드 수는 블럭수 와 블럭당 스레드 수의 곱이다.\n그리드 (Grid) 는 블럭의 집합이며, 1, 2 혹은 3차원 인덱스를 갖는다. 한 그리드의 스레드 블럭의 갯수는 처리하고자 하는 데이터의 크기에 의해 지정되며, 보통 시스템의 프로세서 수 보다 크다.\n\n\n\n\n\n\n\n\n그림 1: Grid of Thread Blocks\n\n\n\n\n행렬곱에 대한 코드와 실행에 대한 아래의 코드를 보자.\n// Kernel definition\n__global__ void MatAdd(float A[N][N], float B[N][N],\nfloat C[N][N])\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i &lt; N && j &lt; N)\n        C[i][j] = A[i][j] + B[i][j];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);\n    ...\n}\n\n&lt;&lt;&lt;...&gt;&gt;&gt; 구문에서 지정된 블록당 스레드 수와 그리드당 블록 수는 int 또는 dim3 타입이다. 2차원 블록 또는 그리드는 위의 예와 같이 지정할 수 있다.\n그리드 내의 각 블록은 커널 내에서 내장된 blockIdx 변수를 통해 액세스할 수 있는 1차원, 2차원 또는 3차원 고유 인덱스로 식별할 수 있다. 스레드 블록의 차원은 커널 내에서 내장된 blockDim 변수를 통해 액세스할 수 있다.\n위 코드의 경우 이 경우 임의적이기는 하지만 16x16(256개 스레드)의 스레드 블록 크기가 일반적인 선택이다. 그리드는 행렬 요소당 하나의 스레드를 가질 수 있을 만큼 충분한 블록으로 생성된다. 단순화를 위해 이 예에서는 각 차원의 그리드당 스레드 수가 해당 차원의 블록당 스레드 수로 균등하게 나누어진다고 가정하지만 반드시 그럴 필요는 없다.\n스레드 블록은 독립적으로 실행될 수 있어야 한다. 병렬 또는 직렬로 어떤 순서로든 실행할 수 있어야 한다. 이러한 독립성으로 인해 앞장의 그림 3 에서 볼 수 있듯이 스레드 블록을 모든 코어 수에 걸쳐 어떤 순서로든 예약할 수 있으므로 프로그래머는 코어 수에 따라 확장되는 코드를 작성할 수 있다.\n블록 내의 스레드는 일부 공유 메모리를 통해 데이터를 공유하고 실행을 동기화하여 메모리 액세스를 조정함으로써 협력할 수 있다. 더 정확하게 말하면 __syncthreads() 내장 함수를 호출하여 커널에서 동기화 지점을 지정할 수 있다. __syncthreads() 는 어떤 허락이 떨어지기 전까지 블록의 모든 스레드가 진행을 멈추고 기다려야 하는 장벽 역할을 한다. 공유 메모리 에는 공유 메모리를 사용하는 예를 제시한다. __syncthreads() 외에도 Cooperative Groups API는 풍부한 스레드 동기화 기본 요소를 제공한다.\n효율적인 협력을 위해 공유 메모리는 각 프로세서 코어 근처의 저지연 메모리(L1 캐시와 매우 유사)가 될 것으로 예상되고 __syncthreads() 는 가벼울 것으로 예상된다.\n\n\n\nThread Block Cluseter\nNVIDIA Compute Capability 9.0 부터 CUDA 프로그래밍 모델은 스레드 블록으로 구성된 스레드 블록 클러스터라는 선택적 계층 구조를 도입한다. 스레드 블록의 스레드가 스트리밍 멀티프로세서에서 공동 스케줄링되는 것과 유사하게 클러스터의 스레드 블록도 GPU의 GPU 처리 클러스터(GPC)에서 공동 스케줄링된다.\n스레드 블록과 유사하게 클러스터도 스레드 블록 클러스터 그리드에서 설명한 대로 1차원, 2차원 또는 3차원으로 구성된다. 클러스터의 스레드 블록 수는 사용자가 정의할 수 있으며 클러스터의 최대 8개 스레드 블록이 CUDA에서 이식 가능한 클러스터 크기로 지원된다. 8개의 멀티프로세서를 지원하기에는 너무 작은 GPU 하드웨어 또는 MIG 구성에서는 최대 클러스터 크기가 그에 따라 줄어들게된다. 8개 이상의 스레드 블록 클러스터 크기를 지원하는 더 큰 구성뿐만 아니라 이러한 더 작은 구성을 식별하는 것은 아키텍처에 따라 다르며 cudaOccupancyMaxPotentialClusterSize API를 사용하여 쿼리할 수 있다.\n\n\n\n\n\n\n그림 2: Grid of Thread Block Clusters\n\n\n\n\n\n\n\n\n\n노트\n\n\n\nIn a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the Cluster Group API.\n\n\n\n스레드 블록 클러스터는 __cluster_dims__(X,Y,Z) 를 사용하는 컴파일러 시간 커널 속성을 사용하거나 CUDA 커널 실행 API cudaLaunchKernelEx 를 사용하여 커널에서 활성화할 수 있다. 아래 예는 컴파일러 시간 커널 속성을 사용하여 클러스터를 실행하는 방법을 보여준다. 커널 속성을 사용하는 클러스터 크기는 컴파일 시간에 고정되고 그런 다음 고전적인 &lt;&lt;&lt; , &gt;&gt;&gt; 를 사용하여 커널을 실행할 수 있다. 커널이 이런 컴파일 시간의 클러스터 크기를 사용하는 경우 커널을 실행할 때 클러스터 크기를 수정할 수 없다.\n// Kernel definition\n// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension\n__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)\n{\n\n}\n\nint main()\n{\n    float *input, *output;\n    // Kernel invocation with compile time cluster size\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n\n    // The grid dimension is not affected by cluster launch, and is still enumerated\n    // using number of blocks.\n    // The grid dimension must be a multiple of cluster size.\n    cluster_kernel&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(input, output);\n}\n\n스레드 블록 클러스터 크기는 런타임에 설정할 수도 있고, 커널은 CUDA 커널 실행 API cudaLaunchKernelEx 를 사용하여 실행할 수 있다. 아래 코드 예제는 확장 가능한 API를 사용하여 클러스터 커널을 실행하는 방법을 보여준다.\n// Kernel definition\n// No compile time attribute attached to the kernel\n__global__ void cluster_kernel(float *input, float* output)\n{\n\n}\n\nint main()\n{\n    float *input, *output;\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n\n    // Kernel invocation with runtime cluster size\n    {\n        cudaLaunchConfig_t config = {0};\n        // The grid dimension is not affected by cluster launch, and is still enumerated\n        // using number of blocks.\n        // The grid dimension should be a multiple of cluster size.\n        config.gridDim = numBlocks;\n        config.blockDim = threadsPerBlock;\n\n        cudaLaunchAttribute attribute[1];\n        attribute[0].id = cudaLaunchAttributeClusterDimension;\n        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension\n        attribute[0].val.clusterDim.y = 1;\n        attribute[0].val.clusterDim.z = 1;\n        config.attrs = attribute;\n        config.numAttrs = 1;\n\n        cudaLaunchKernelEx(&config, cluster_kernel, input, output);\n    }\n}\ncompute capability 9.0 호환 GPU에서 클러스터의 모든 스레드 블록은 단일 GPU 처리 클러스터(GPC)에서 공동 스케줄링되도록 보장되며 클러스터의 스레드 블록이 Cluster Group API cluster.sync() 를 사용하여 하드웨어가 지원하는 동기화를 수행할 수 있다. Cluster Group은 또한 num_threads() 및 num_blocks() API를 사용하여 스레드 수 또는 블록 수 측면에서 클러스터 그룹 크기를 쿼리하는 멤버 함수를 제공한다. 클러스터 그룹의 스레드 또는 블록 순위는 dim_threads() 및 dim_blocks() API를 사용하여 각각 쿼리할 수 있다.\n클러스터에 속한 스레드 블록은 분산 공유 메모리(Distributed shared memory)에 액세스할 수 있다. 클러스터의 스레드 블록은 분산 공유 메모리의 모든 주소를 읽고, 쓰고, 원자성을 수행할 수 있다. 분산 공유 메모리(Distributed shared memory)에는 분산 공유 메모리에서 히스토그램을 수행하는 예를 제시한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_memory_hierarchy",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_memory_hierarchy",
    "title": "Programming Model",
    "section": "3 메모리 계층 구조",
    "text": "3 메모리 계층 구조\n\n\n\n\n\n\n그림 3: Memory Hierarchy\n\n\n\nCUDA 스레드는 그림 3 에서 설명한 대로 실행 중에 여러 메모리 공간에서 데이터에 액세스할 수 있다. 각 스레드에는 개별적인 로컬 메모리가 있다. 각 스레드 블록에는 블록의 모든 스레드에서 볼 수 있고 블록과 동일한 수명을 가진 공유 메모리가 있다. 스레드 블록 클러스터의 스레드 블록은 서로의 공유 메모리에서 읽기, 쓰기 및 아토믹 연산을 수행할 수 있다. 모든 스레드는 동일한 전역 메모리(Global memory)에 액세스할 수 있다.\n또한 모든 스레드에서 액세스할 수 있는 읽기 전용 메모리 공간이 추가로 존재한다 - 상수 메모리 공간과 텍스처 메모리 공간. 전역, 상수 및 텍스처 메모리 공간은 다양한 메모리 사용에 맞게 최적화되어 있다(장치 메모리 액세스 참조). 텍스처 메모리는 또한 일부 특정 데이터 형식에 대해 다양한 주소 지정 모드와 데이터 필터링을 제공한다(텍스처 및 표면 메모리 참조).\n글로벌, 상수 및 텍스처 메모리 공간은 동일한 애플리케이션에서 커널을 시작할 때에도 지속된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_heterogeneous_programming",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_heterogeneous_programming",
    "title": "Programming Model",
    "section": "4 이기종 프로그래밍 (Heterogeneous Programming)",
    "text": "4 이기종 프로그래밍 (Heterogeneous Programming)\n\n\n\n\n\n\n그림 4: Heterogeneous Programming\n\n\n\n\n\n그림 4 에서 알 수 있듯이 CUDA 프로그래밍 모델은 C++ 프로그램을 실행하는 호스트에 대한 보조 프로세서로 작동하는 물리적으로 분리된 장치에서 CUDA 스레드가 실행된다고 가정한다. 예를 들자면 커널이 GPU에서 실행되고 나머지 C++ 프로그램이 CPU에서 실행되는 경우이다. 이후 GPU 가 실행되는 분리된 장치를 디바이스라고 부르겠다. (호스트와 디바이스라는 명칭으로 나누는 것은 일종의 관례이다)\nCUDA 프로그래밍 모델은 또한 호스트와 디바이스가 각각 호스트 메모리와 디바이스 메모리라고 하는 DRAM에서 자체적인 별도 메모리 공간을 유지한다고 가정한다. 따라서 프로그램은 CUDA 런타임(프로그래밍 인터페이스에서 설명)에 대한 호출을 통해 커널에서 볼 수 있는 전역, 상수 및 텍스처 메모리 공간을 관리한다. 여기에는 디바이스 메모리 할당 및 할당 해제와 호스트와 디바이스 메모리 간의 데이터 전송이 포함된다.\nUnified Memory 는 호스트와 디바이스 메모리 공간을 연결하는 관리되는 메모리를 제공한다. 관리되는 메모리는 시스템의 모든 CPU와 GPU에서 공통 주소 공간이 있는 단일의 일관된 메모리 이미지로 액세스할 수 있다. 이 기능은 디바이스 메모리에 대한 초과 예약(oversubcription)을 가능하게 하며 호스트와 디바이스에서 데이터를 명시적으로 미러링할 필요성을 없애 애플리케이션 포팅 작업을 크게 단순화할 수 있다. Unified Memory에 대한 소개는 Unified Memory Programming 을 참조하라.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda-asynchronous_simt_programming_model",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda-asynchronous_simt_programming_model",
    "title": "Programming Model",
    "section": "5 비동기적 SIMT 프로그래밍 모델",
    "text": "5 비동기적 SIMT 프로그래밍 모델\nCUDA 프로그래밍 모델에서 스레드는 계산이나 메모리 작업을 수행하기 위한 가장 낮은 수준의 추상화이다. NVIDIA Ampere GPU 아키텍처 기반 장치부터 시작하여 CUDA 프로그래밍 모델은 비동기 프로그래밍 모델을 통해 메모리 작업을 가속시킨다. 비동기 프로그래밍 모델은 CUDA 스레드와 관련하여 비동기 연산의 행동을 정의한다.\n비동기 프로그래밍 모델은 CUDA 스레드 간 동기화를 위한 비동기 배리어 의 동작을 정의한다. 이 모델은 또한 cuda::memcpy_async를 사용하여 GPU에서 계산하는 동안 글로벌 메모리에서 비동기적으로 데이터를 이동하는 방법을 설명하고 정의한다.\n\n\n비동기 연산\n비동기 작업은 한 CUDA 스레드에서 시작하여 다른 스레드에서 실행되는 것처럼 비동기적으로 실행되는 작업으로 정의된다. 잘 구성된 프로그램에서는 하나 이상의 CUDA 스레드가 이 비동기 작업과 동기화된다. 비동기 작업을 시작한 CUDA 스레드는 동기화 스레드에 포함될 필요가 없다.\n이러한 비동기 스레드(as-if 스레드)는 항상 비동기 작업을 시작한 CUDA 스레드와 연결된다. 비동기 작업은 작업 완료를 동기화 하기 위해 동기화 객체를 사용한다. 이러한 동기화 객체는 사용자가 명시적으로 관리하거나(예: cuda::memcpy_async) 라이브러리 내에서 암묵적으로 관리할 수 있다(예: cooperative_groups::memcpy_async).\n동기화 객체는 cuda::barrier 또는 cuda::pipeline 일 수 있다. 이러한 객체는 비동기 Barrier 및 cuda::pipeline 을 사용한 비동기 데이터 복사에서 자세히 설명한다. 이러한 동기화 객체들 다른 스레드 범위(scope)에서 사용할 수 있다. 이 스레드 범위는 비동기 작업과 동기화하기 위해 동기화 객체를 사용할 수 있는 스레드 집합을 정의한다. 다음 표는 CUDA C++에서 사용 가능한 스레드 범위와 각각과 동기화할 수 있는 스레드를 정의한다.\n\n\n\n\n\n\n\n스레드 범위\n동기화 범위\n\n\n\n\ncuda::thread_scope::thread_scope_thread\n비동기 작업을 시작한 CUDA 스레드\n\n\ncuda::thread_scope::thread_scope_block\n시작 스레드와 동일한 스레드 블록 내의 모든 CUDA 스레드 또는 어떤 CUDA 스레드\n\n\ncuda::thread_scope::thread_scope_device\n시작 스레드가 속한 동일한 GPU 디바이스 내의 모든 CUDA 스레드 또는 어떤 CUDA 스레드\n\n\ncuda::thread_scope::thread_scope_system\n시작 스레드가 속한 동일한 시스템 내의 모든 CUDA 스레드 또는 CPU 스레드\n\n\n\n\n이러한 스레드 scope 는 표준 C++ 라이브러리에 대한 확장인 CUDA Standard C++ library 로 구현되었다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#compute-capability",
    "href": "src/gpu/cuda/02_programming_model.html#compute-capability",
    "title": "Programming Model",
    "section": "6 Compute Capability",
    "text": "6 Compute Capability\n디바이스의 compute capability 는 버전 번호로 표현되며, 때로는 “SM 버전” 이라고도 한다. 이 버전 번호는 GPU 하드웨어에서 지원하는 기능을 식별하며, 런타임에 애플리케이션에서 현재 GPU에서 사용할 수 있는 하드웨어 기능 및/또는 명령어를 확인하는 데 사용된다. compute capability 는 주요 개정 번호 X와 부차 개정 번호 Y로 구성되며 X.Y로 표시된다.\n동일한 주요 개정 번호가 있는 장치는 동일한 코어 아키텍처이다. 주요 개정 번호는 NVIDIA Hopper GPU 아키텍처 기반 장치의 경우 9, NVIDIA Ampere GPU 아키텍처 기반 장치의 경우 8, Volta 아키텍처 기반 장치의 경우 7, ​​Pascal 아키텍처 기반 장치의 경우 6, Maxwell 아키텍처 기반 장치의 경우 5, Kepler 아키텍처 기반 장치의 경우 3 이다.\n부차 개정 번호는 코어 아키텍처에 대한 점진적 개선에 해당하며, 새로운 기능이 포함될 수 있다.\nTuring은 컴퓨팅 기능 7.5의 장치에 대한 아키텍처이며 Volta 아키텍처를 기반으로 하는 점진적 업데이트이다.\nCUDA 지원 GPU는 모든 CUDA 지원 장치와 해당 컴퓨팅 기능을 나열한다. Compute Capability 에서는 각 compute capability 의 기술 사양을 제공한다.\n\n\n\n\n\n\n\n경고\n\n\n\n특정 GPU의 컴퓨팅 기능 버전은 CUDA 소프트웨어 플랫폼의 버전인 CUDA 버전(예: CUDA 7.5, CUDA 8, CUDA 9)과 다르다. CUDA 플랫폼은 애플리케이션 개발자가 아직 발명되지 않은 미래의 GPU 아키텍처를 포함하여 여러 세대의 GPU 아키텍처에서 실행되는 애플리케이션을 만드는 데 사용된다. CUDA 플랫폼의 새 버전은 종종 해당 아키텍처의 컴퓨팅 기능 버전을 지원하여 새 GPU 아키텍처에 대한 기본 지원을 추가하지만, CUDA 플랫폼의 새 버전은 일반적으로 하드웨어 세대와 독립적인 소프트웨어 기능도 포함한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html",
    "title": "CUDA.jl 성능 팁",
    "section": "",
    "text": "CUDA.jl 의 Performance Tips 을 요약 & 번역한 것이다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#일반적인-팁",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#일반적인-팁",
    "title": "CUDA.jl 성능 팁",
    "section": "1 일반적인 팁",
    "text": "1 일반적인 팁\n항상 코드 프로파일링부터 시작한다(자세한 내용은 profiling 페이지 참조). 먼저 CUDA.@profile 또는 NSight Systems 를 사용하여 프로그램 전체를 분석하고 핫스팟과 병목지점을 파악해야 한다. 여기에 집중하면서 다음을 수행한다.\n\nCPU와 GPU 간의 데이터 전송을 최소할 것. 불필요한 메모리 사본을 제거하고 다수의 적은 전송을 일괄적인 큰 전송으로 한다.\n문제가 있는 커널 호출을 식별한다. 단일 호출로 처리할 수 있는 수천 개의 커널을 동작시킬 수 있다.\nCPU가 GPU를 바쁘게 유지할 만큼 빠르게 작업을 제출하지 않는 스톨을 찾습니다.\n\n이것으로 충분하지 않고 느리게 실행되는 커널을 식별한 경우 NSight Compute를 사용하여 해당 커널을 자세히 분석해 볼 수 있다. 중요도 순으로 시도해야 할 몇 가지 사항은 다음과 같다.\n\n메모리 액세스를 최적한다. 예를 들어 불필요한 전역 접근(대신 공유 메모리에서 버퍼링)을 피하거나 접근을 병합한다.\n각 스트리밍 멀티프로세서(SM)에서 더 많은 스레드를 실행한다. 이는 레지스터 압력을 낮추거나 공유 메모리 사용을 줄임으로써 달성할 수 있다. 아래 팁은 레지스터 압력을 줄일 수 있는 다양한 방법을 설명한다.\nFloat64 및 Int/Int64 와 같은 64비트 유형 대신 Float32 및 Int32 와 같은 32비트 타입을 사용한다.\n같은 워프의 스레드가 갈라지는 원인이 되는 제어 흐름 사용을 피하세요. 즉, while 또는 for 루프가 전체 워프에서 동일하게 동작하도록 하고, 워프 내에서 갈라지는 if 를 if else 로 바꾼다.\nGPU가 메모리 액세스의 지연 시간을 숨길 수 있도록 계산 강도를 높인다.\n\n\n\nInlining\nInlining 은 레지스터 사용을 줄여 커널 속도를 높일 수 있다. 모든 함수의 인라인을 강제로 실행하려면 @cuda always_inline=true 를 사용하면 된다.\n\n\n\n쓰레드 당 최대 레지스터 개수를 제한한다.\n시작할 수 있는 스레드 수는 부분적으로 커널이 사용하는 레지스터 수에 의해 결정된다. 이는 멀티프로세서의 모든 스레드에서 레지스터가 공유되기 때문이다. 스레드당 최대 레지스터 수를 설정하면 사용되는 레지스터가 줄어들어 스레드 수가 늘어나고 레지스터를 로컬 메모리에 분산시키는 댓가로 성능이 향상될 수 있다. 최대 레지스터 수를 32로 설정하려면 @cuda maxregs=32 라고 한다.\n\n\nFastMath\n일반적인 산술 함수의 고속 버젼을 사용하는 @fastmath 매크로를 사용할 수 있다. 다음의 매크로를 사용하라. @cuda fastmath=true",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#julia-전용-팁",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#julia-전용-팁",
    "title": "CUDA.jl 성능 팁",
    "section": "2 Julia 전용 팁",
    "text": "2 Julia 전용 팁\n\n런타임 예외를 최소화한다\nJulia 에서 많은 일반적인 연산은 런타임에 Error 를 발생시킬 수 있으며, Error 는 종종 분기를 만들고 해당 분기에서 함수를 호출하는데, 이 둘 다 GPU 에서는 느리다. 배열을 인덱싱할 때 @inbounds 를 사용하면 경계 검사로 인한 예외가 제거된다. --check-bounds=yes (Pkg.test의 기본값) 로 코드를 실행하면 항상 경계 검사가 발생한다. LLVM.jl 패키지의 assume 을 사용하여 예외를 제거할 수도 있다. 다음을 보라.\nusing LLVM.Interop\n\nfunction test(x, y)\n    assume(x &gt; 0)\n    div(y, x)\nend\nassume(x &gt; 0) 은 컴파일러에게 0 으로 나누는 에러가 발생하지 않을 것임을 말해준다.\n\n\n\n32 비트 정수\n가능하면 32비트 정수를 하용한다. 레지스터 압력의 일반적인 원인은 32비트만 필요한데 64비트 정수를 사용하는 것이다. 예를 들어, 하드웨어의 인덱스는 32비트 정수이지만 Julia의 리터럴은 Int64 로, blockIdx().x-1 과 같은 표현식은 64비트 정수로 승격된다. 32비트 정수를 사용하려면 1 을 Int32(1) 로 대체하거나 CUDA를 사용하여 실행하는 경우 더 간결하게 1i32 로 대체할 수 있다.\n이것이 얼마나 큰 차이를 만드는지 확인하기 위해 소개 튜토리얼에서 소개한 커널을 사용해 보자.\nusing CUDA, BenchmarkTools\n\nfunction gpu_add3!(y, x)\n    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n    for i = index:stride:length(y)\n        @inbounds y[i] += x[i]\n    end\n    return\nend\ngpu_add3! (generic function with 1 method)\n몇개의 레지스터가 사용되었는지 확인해보자.\nx_d = CUDA.fill(1.0f0, 2^28)\ny_d = CUDA.fill(2.0f0, 2^28)\n\nCUDA.registers(@cuda gpu_add3!(y_d, x_d))\n29\n위의 결과는 기기마다 다를 수 있다. 이제 32 비트 정수를 사용하는 커널은 아래와 같다.\nfunction gpu_add4!(y, x)\n    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n    for i = index:stride:length(y)\n        @inbounds y[i] += x[i]\n    end\n    return\nend\ngpu_add4! (generic function with 1 method)\nCUDA.registers(@cuda gpu_add4!(y_d, x_d))\n28\n따라서 32비트 정수로 전환하여 레지스터 하나를 덜 사용하게 되고, 64비트 정수를 더 많이 사용하는 커널에서는 레지스터 수가 더 크게 감소할 것으로 예상된다.\n\n\n\nStepRange 사용을 피하라\nfor 루프의 이전 커널에서 StepRange 인 index:stride:length(y) 를 순회했다. 안타깝게도 StepRange 를 구성하는 것은 오류가 발생할 수 있고, 단순히 반복하고 싶을 경우에는 불필요한 계산을 포함하기 때문에 느립니다. 대신 다음과 같이 while 루프를 사용하는 것이 더 빠르다.\nfunction gpu_add5!(y, x)\n    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n\n    i = index\n    while i &lt;= length(y)\n        @inbounds y[i] += x[i]\n        i += stride\n    end\n    return\nend\ngpu_add5! (generic function with 1 method)\n벤치마크는\nfunction bench_gpu4!(y, x)\n    kernel = @cuda launch=false gpu_add4!(y, x)\n    config = launch_configuration(kernel.fun)\n    threads = min(length(y), config.threads)\n    blocks = cld(length(y), threads)\n\n    CUDA.@sync kernel(y, x; threads, blocks)\nend\n\nfunction bench_gpu5!(y, x)\n    kernel = @cuda launch=false gpu_add5!(y, x)\n    config = launch_configuration(kernel.fun)\n    threads = min(length(y), config.threads)\n    blocks = cld(length(y), threads)\n\n    CUDA.@sync kernel(y, x; threads, blocks)\nend\nbench_gpu5! (generic function with 1 method)\n@btime bench_gpu4!($y_d, $x_d)\n  76.149 ms (57 allocations: 3.70 KiB)\n@btime bench_gpu5!($y_d, $x_d)\n  75.732 ms (58 allocations: 3.73 KiB)\n이 벤치마크는 이 커널에 대한 성능 이점에서는 미미하지만 StepRange 를 사용할 때 28개의 레지스터가 사용되었다는 점을 상기하면 사용된 레지스터의 양에 큰 차이가 있음을 확인 할 수 있다.\nCUDA.registers(@cuda gpu_add5!(y_d, x_d))\n12",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html",
    "title": "CUDA.jl 배열 처리",
    "section": "",
    "text": "필요한 패키지는 다음과 같다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#cuarray-타입",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#cuarray-타입",
    "title": "CUDA.jl 배열 처리",
    "section": "1 CuArray 타입",
    "text": "1 CuArray 타입\n\n기본 연산\nCUDA.jl 의 기본적인 타입은 CuArray 타입으로 Array 타입과 많은 부분에서 비슷하다. 우선 CPU 에서 만든 배열을 GPU 의 CuArray 타입으로 전환시켜 보자.\ncarr0 = CuArray(rand(Float32, 128, 128))\n결과는 다음과 같다. CPU 에서 128 x 128 랜덤 배열을 만든 후 GPU 에서의 CuArray 타입으로 변환시켰다. 아래로의 많은 줄이 생략되었다.\n128×128 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 0.614408   0.545875   0.716857   …  0.25018    0.304857   0.99422\n 0.432157   0.148661   0.060947      0.0701835  0.15738    0.26359\n\n기본적인 사용법도 눈여겨보라.\ncarr1 = CuArray{Float32}(undef,  1024)\ncarr2 = fill!(copy(carr1), 0f0)\n@test carr2 == CUDA.zeros(Float32, 1024)\n\nCuArray 에 많은 배열 연산을 수행 할 수 있다.\ncarr3 = carr1.^2 + carr2.^2\ncarr4 = map(cos, carr1)\ncarr5 = reduce(+, carr1)\n\nArray 와 같이 논리 연산을 통해 성분을 선택 할 수 있다.\nc1 = CuArray([1,2,3,4,5])\nc2 = c1[[true, false, false, true, true]]\n3-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 1\n 4\n 5\n\n이 외의 중요한 연산을 수행해보자. 모두 Array 에서와 같이 작동한다.\nfindall(isodd, c1)\nfindfirst(isodd, c1)\nfindmin(c1)\n\nreshape, view 와 같은 것도 Array 와 똑같이 동작한다.\nc2 = CuArray{Int32}(collect(1:6))\nc3 = reshape(c2, 2, 3)\nc4 = view(c2, 2:4)\n\n\n\n스칼라 인덱싱\n다음은 Array 에서와 달리 경고를 발생시킨다.\na = CuArray([1])\na[1]+=1\n┌ Warning: Performing scalar indexing on task Task (runnable, started) @0x000075014b772400.\n│ Invocation of getindex resulted in scalar indexing of a GPU array.\n│ This is typically caused by calling an iterating implementation of a method.\n│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n│ and therefore should be avoided.\n│ \n│ If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n│ to enable scalar iteration globally or for the operations in question.\n└ @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:149\n배열의 개별 원소에 인덱스를 이용하여 접근하는 것을 스칼라 인덱싱이라고 한다. Array 에서의 경우는 아무 문제가 없지만 CuArray 의 경우는 위의 경고 메시지에서 나오듯이 큰 성능 하락을 불러일으킬 수 있다. Julia REPL 이나 jupyter 와 같은 상호작용 세션에서는 한번 경고를 발생시키고 수행하며, 이후에는 경고도 없이 수행하지만 실행 프로그램 상에서라면 에러를 발생시킨다. 아래의 코드를 파일로 저장하고 실행시켜보라.\n#! /usr/bin/env julia\nusing CUDA\n\na = CuArray([1])\na[1]=3\n아래와 같은 에러메시지가 출력된다.\nERROR: LoadError: Scalar indexing is disallowed.\nInvocation of setindex! resulted in scalar indexing of a GPU array.\n...\n스칼라 인덱싱을 허용하려면 @arrowscalar 매크롤르 사용한다.\nCUDA.@allowscalar a[1] += 1",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#고급-기능",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#고급-기능",
    "title": "CUDA.jl 배열 처리",
    "section": "2 고급 기능",
    "text": "2 고급 기능\n\n난수 발생\nCUDA 에서 제공하는 난수발생기를 사용 할 수 있다.\nCUDA.randn(Float64, 2, 1)\nCUDA 는 난수발생 모듈인 CURAND 를 포함하며 CURAND 는 lognormal 분포나 푸아송 분포에 대한 난수발생을 제공한다.\nCUDA.rand_logn(Float32, 1, 5; mean=2, stddev=20)\nCUDA.rand_poisson(UInt32, 1, 10; lambda=100)\n\n\n\n선형 대수\nCUDA 에는 자체 선형 대수 모듈인 CUBLAS 가 포함되어 있고 관련 함수들이 Julia 의 표준 선형 대수 모듈인 LinearAlgebra 에 포함되어 있다. 즉 LinearAlgebra 모듈을 통해 CuArray 에 대한 선형 대수 계산을 GPU 에서 수행 할 수 있다.\nC1= CuArray{Float32}([5 -1; -1 4]);\nlu(C1)\nLU{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}, CuArray{Int32, 1, CUDA.DeviceMemory}}\nL factor:\n2×2 CuArray{Float32, 2, CUDA.DeviceMemory}:\n  1.0  0.0\n -0.2  1.0\nU factor:\n2×2 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 5.0  -1.0\n 0.0   3.8\n CUBLAS에 존재하지만 (아직) LinearAlgebra 표준 라이브러리의 고수준 constructs 에 포함되지 않은 연산은 CUBLAS 서브모듈로 직접 접근 할 수 있다. 많은 연산이(예: cublasDdot) 더 상위 수준의 wrapper (예: dot) 를 사용할 수 있으므로 C 래퍼를 직접 호출할 필요가 없다.\nc1 = CuArray{Float32}([1, 2])\nCUBLAS.dot(2, c1, c1)\n5.0f0\n\n\n\nSolver\n선형 시스템의 해를 구하는 LAPACK 유사 기능은 CUDA 에 포함된 CUSOLVER 에 포함되어 있으며 LinearAlgebra 표준 라이브러리의 메서드를 통해서 접근 할 수 있다.\nA = CUDA.rand(3, 3)\nA = A * A'\ncholesky(A)\nCholesky{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}}\nU factor:\n3×3 UpperTriangular{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}}:\n 0.685254  0.386212  0.527034\n  ⋅        0.977934  0.542067\n  ⋅         ⋅        0.182822\n\nA = CUDA.rand(3, 3)\nb = CUDA.rand(3)\nx=A\\b\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n  1.9668096\n  4.2866626\n -1.7602696\nA*x-b\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n  2.9802322f-8\n -2.2351742f-8\n -2.7939677f-9\n\n\n\n희소 행렬\nCUDA 에 포함된 CUSPARSE 라이브러리를 통해 희소행렬을 다룰 수 있다. CUSPARSE 에서 희소행렬은 주로 CuSparseArray 객체를 사용하며 이 객체의 기능은 SparseArrays 패키지를 통해서도 접근 할 수 있다.\nsp1=sprand(10, 0.2)\ncsp1 = CuSparseVector(sp1)\n10-element CuSparseVector{Float64, Int32} with 2 stored entries:\nsparsevec(Int32[1, 2], [0.12468759985600719, 0.5027074361089312], 10)\n\n\n\nFFT\nCUDA 에는 고속 이산 푸리에 변환(FFT) 를 수행하는 CUFFT 가 포함되어 있으며 CUFFT 서브모듈로 접근한다. 사용법은 다음 절의 벤치마크 를 참고하라.\n\n\n\n벤치마크\n2차원 푸리에 변환에 대한 수행 시간을 확인해 보자.\nd1 = rand(Float32, 2048, 2048);\nc1 = CuArray(d1);\n@benchmark fft(d1)\nBenchmarkTools.Trial: 26 samples with 1 evaluation.\n Range (min … max):  115.318 ms … 233.668 ms  ┊ GC (min … max): 0.00% … 1.47%\n Time  (median):     187.126 ms               ┊ GC (median):    1.84%\n Time  (mean ± σ):   195.700 ms ±  36.154 ms  ┊ GC (mean ± σ):  1.59% ± 0.57%\n\n@benchmark CUDA.@sync fft($c1)\nBenchmarkTools.Trial: 6485 samples with 1 evaluation.\n Range (min … max):  433.504 μs …  11.076 ms  ┊ GC (min … max): 0.00% … 18.69%\n Time  (median):     741.453 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   765.796 μs ± 767.649 μs  ┊ GC (mean ± σ):  1.96% ±  1.84%",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#메모리-관리",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#메모리-관리",
    "title": "CUDA.jl 배열 처리",
    "section": "3 메모리 관리",
    "text": "3 메모리 관리\n다음을 보자\na1 = [1.0, 2.0, 3.0]\nca1 = CuArray{Float32}(a1)\na1 은 메모리상의 배열로, a1 에 대한 처리는 CPU 가 담당한다. 우리는 관례를 따라 앞으로 CPU 와 메인 메모리를 합쳐 host 라고 부르기로 하자. ca1 = CuArray{Float32}(a1) 은 a1 의 배열을 Float32 로 바꾸어 GPU 의 메모리로 옮긴다. GPU 와 GPU 의 메모리 등을 합쳐 device 라고 부르자. 굳이 Float32 타입으로 바꾸는 이유는 GPU 에서는 Float32 형식의 부동소수 연산이 Float64 보다 훨씬 빠르기 때문이다. Float64 타입이 무조건 필요한 경우가 아니라면 보통 Float32 를 사용한다. 이렇게 host 에서 device 로 데이터를 옮기는 것을 upload, device 에서 host 로 옮기는 것을 download 라고 하자.\n\n\n타입 보존 upload\nharr = Diagonal([1.0,2,3])\n여기서 harr 은 host 의 Float64 타입 희소행렬이다. 이것을 GPU 로 업로드 할 때\nCuArray(harr)\n라고 하면 CuArray{Float64} 타입 배열이 된다. Float32 타입으로 바꾸기 위해\nCuArray{Float64}(harr)\n라고 해도 되고 CUDA.jl 의 cu 함수를 이용해도 된다.\ncu(harr)\n\n\n\n통합 메모리\nCuArray 생성자와 cu 함수는 기본적으로 GPU에서만 액세스할 수 있는 장치 메모리를 할당한다. CPU 와 GPU 모두 접근할 수 있는 통합 메모리(Unified memory) 가 있으며 장치 드라이버가 데이터의 이동을 관장한다. 즉 필요에 따리 CPU 와 GPU 메모리에 존재하며 전송된다. 1 차원 배열에 대한 통합메모리는 다음과 같이 사용 할 수 있다. 아래에서 carr1, carr2, carr3 는 모두 같다.\narr1 = [1, 2, 3]\ncarr1 = CuArray{Float32, 1, CUDA.UnifiedMemory}(arr1)\ncarr2 = CuVector{Float32, CUDA.UnifiedMemory}(arr1)\ncarr3 = cu(arr1; unified=true)\n2차원 베열에 대한 통합메모리는 다음과 같이 사용 할 수 있다. 역시 cmat1, cmat2, cmat3 는 모두 같다.\nmat1 = [1 2; 3 4]\ncmat1 = CuMatrix{Float32, CUDA.UnifiedMemory}(mat1)\ncmat2 = CuArray{Float32, 2, CUDA.UnifiedMemory}(mat1)\ncmat3 = cu(mat1, unified=true)\n이렇게 하면 CPU 코드를 실행하거나 AbstractArray 로의 폴백(fallback)을 트리거하는 것에 대해 걱정할 필요 없이 애플리케이션의 일부를 점진적으로 포팅할 수 있으므로 코드를 GPU로 포팅하는 것이 상당히 쉬워질 수 있다. 그러나 통합 메모리는 GPU 메모리에서 페이지 인 혹은 페이지 아웃 을 해야 하며 비동기적으로 할당할 수 없으므로 이에 대한 비용이 발생할 수 있다. 이 비용을 줄이기 위해 CUDA.jl 은 커널에 전달할 때 통합 메모리를 prefetch 한다.\n최신 시스템(오픈소스 NVIDIA 드라이버가 있는 CUDA 12.2)에서는 CuArray 생성자나 cu 함수를 사용하여 통합 메모리를 명시적으로 할당하지 않고도 GPU에서 CPU 메모리에 액세스하여 그 반대의 작업도 가능하다.\njulia&gt; cpu = [1,2];\n\njulia&gt; gpu = unsafe_wrap(CuArray, cpu)\n2-element CuArray{Int64, 1, CUDA.UnifiedMemory}:\n 1\n 2\n\njulia&gt; gpu .+= 1;\n\njulia&gt; cpu\n2-element Vector{Int64}:\n 2\n 3\n현재 CUDA.jl은 여전히 ​​장치 메모리를 할당하는 것을 기본으로 하지만, 이는 향후 변경될 수 있습니다. 기본 동작을 변경하려면 default_memory 기본 설정을 device 대신 unified 또는 host로 설정할 수 있습니다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/ML/ml.html",
    "href": "src/ML/ml.html",
    "title": "AI & ML",
    "section": "",
    "text": "고려대 오승상 교수 강의 자료 및 강의 영상 링크\n서울대 심형보 교수 강의 링크\n포항공대 이승철 교수 강의 링크\n\n\n\n\n\n\nStanford Andrew Ng 교수 강의노트, 2023 version",
    "crumbs": [
      "AI & ML"
    ]
  },
  {
    "objectID": "src/ML/ml.html#자료",
    "href": "src/ML/ml.html#자료",
    "title": "AI & ML",
    "section": "",
    "text": "고려대 오승상 교수 강의 자료 및 강의 영상 링크\n서울대 심형보 교수 강의 링크\n포항공대 이승철 교수 강의 링크\n\n\n\n\n\n\nStanford Andrew Ng 교수 강의노트, 2023 version",
    "crumbs": [
      "AI & ML"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Science & Programming",
    "section": "",
    "text": "Julia 프로그래밍 언어 : Julia 언어\nJulia 언어를 이용한 수치해석 : 수치해석과 이미지 처리에 대해 다룹니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\ndfkl"
  },
  {
    "objectID": "latexmacros.html",
    "href": "latexmacros.html",
    "title": "Science & Programming",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "src/cpp_qt/cpp_qt.html#c",
    "href": "src/cpp_qt/cpp_qt.html#c",
    "title": "C++ and Qt",
    "section": "1 C++",
    "text": "1 C++\n\nvcpkg : C++ 패키지 관리 툴\neigen3 : C++ 선형 대수 패키지"
  },
  {
    "objectID": "src/cpp_qt/cpp_qt.html#qt-and-tools",
    "href": "src/cpp_qt/cpp_qt.html#qt-and-tools",
    "title": "C++ and Qt",
    "section": "2 Qt and Tools",
    "text": "2 Qt and Tools\n\nQt : 멀티플랫폼 프레임웍\nQt Creator : Qt 개발도구\nqmake : Qt 전용 빌드 관리 시스템\nQCustomPlot : Qt 전용 Plotting tool"
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "",
    "text": "CPU 에서 호출하고 GPU 에서 실행되는 함수를 커널(kernel) 이라고 한다. 커널은 보통의 julia 함수처럼 정의한다.\nfunction my_kernel()\n    return\nend\n커널을 실행하기 위해서는 @cuda 매크로를 사용한다.\n@cuda my_kernel\n위의 명령을 실행하면 my_kernel 함수가 컴파일되며 현재의 GPU 에서 실행된다.\n\n@cuda 매크로에 launch=false 인자를 전달하면 컴파일만 되고 실행하지 않으며 호출 가능한 객체를 리턴한다.\njulia&gt; k = @cuda launch=false my_kernel()\nCUDA.HostKernel for my_kernel()\n\njulia&gt; CUDA.registers(k)\n4\n\njulia&gt; k()",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "",
    "text": "CPU 에서 호출하고 GPU 에서 실행되는 함수를 커널(kernel) 이라고 한다. 커널은 보통의 julia 함수처럼 정의한다.\nfunction my_kernel()\n    return\nend\n커널을 실행하기 위해서는 @cuda 매크로를 사용한다.\n@cuda my_kernel\n위의 명령을 실행하면 my_kernel 함수가 컴파일되며 현재의 GPU 에서 실행된다.\n\n@cuda 매크로에 launch=false 인자를 전달하면 컴파일만 되고 실행하지 않으며 호출 가능한 객체를 리턴한다.\njulia&gt; k = @cuda launch=false my_kernel()\nCUDA.HostKernel for my_kernel()\n\njulia&gt; CUDA.registers(k)\n4\n\njulia&gt; k()",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudalj_kernel_io",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudalj_kernel_io",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "2 커널 입력과 출력",
    "text": "2 커널 입력과 출력\nGPU 커널은 반환값을 가질 수 없다. 즉 항상 return 이거나 return nothing 이어야 한다. 커널과 통신하는 유일한 방법은 CuArray 를 쓰는 것 뿐이다.\nfunction my_kernel(a)\n    a[1] = 42\n    return\nend\n\na = CuArray{Int}(undef, 1);\n@cuda my_kernel(a);\na\n42",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel_configuration_and_indexing",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel_configuration_and_indexing",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "3 커널 구동 설정과 인덱싱",
    "text": "3 커널 구동 설정과 인덱싱\n@cuda 를 통해 커널을 구동하면 단일 스레드만 시작되므로 그다지 유용하지 않다. @cuda 에 대한 threads 및 blocks 키워드 인수를 사용하면 다수의 스레드를 구동할 수 있으며, 커널 내에서는 인덱싱 내장 함수를 사용하여 각 스레드의 계산을 차별화 할 수 있다.\nfunction my_kernel(a)\n    i = threadIdx().x\n    a[i] = 42\n    return\nend\n\na = CuArray{Int}(undef, 5);\n@cuda threads=length(a) my_kernel(a);\na\n5-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 42\n 42\n 42\n 42\n 42\n위에 표시된 대로, CUDA C 의 threadIdx 등의 값은 x, y, z 필드가 있는 NamedTuple 을 반환하는 함수로 사용할 수 있다. 이런 내장 함수는 1 부터 시작하는 인덱스를 반환한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel_compilation_requirements",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_kernel_compilation_requirements",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "4 커널 컴파일 요건",
    "text": "4 커널 컴파일 요건\n사용자 정의 커널이 작동하기 위해서는 어떤 요건을 충족해야 한다.\n\n메모리는 GPU에서 접근 가능해야 한다. 이는 CuArray 등을 사용하여 강제할 수 있다. 사용자 지정 구조체는 해당 튜토리얼에서 설명한 대로 이식할 수 있다.\n런타임 디스패치는 불가하며 모든 함수 호출은 컴파일 타임에 결정되어야 합니다. 여기서 런타임 디스패치는 완전히 특정되지 않은 함수에 의해 도입될 수도 있다는 점에 유의해야 한다. Julia 매뉴얼 을 참고하고 다음 예를 보자.\n\nfunction my_inner_kernel!(f, t) # does not specialize\n    t .= f.(t)\nend\n\nfunction my_outer_kernel(f, a)\n    i = threadIdx().x\n    my_inner_kernel!(f, @view a[i, :])\n    return nothing\nend\n\na = CUDA.rand(Int, (2,2))\nid(x) = x\n\n@cuda threads=size(a, 1) my_outer_kernel(id, a)\n마지막 줄 실행에서 에러가 발생하는데 이는 아래와 같이 회피 할 수 있다.\nfunction my_inner_kernel!(f::F, t::T) where {F,T}\n    t .= f.(t)\nend\n\nfunction my_outer_kernel(f, a)\n    i = threadIdx().x\n    my_inner_kernel!(f, @view a[i, :])\n    return nothing\nend\n\na = CUDA.rand(Int, (2,2))\n\nid(x) = x\n\n@cuda threads=size(a, 1) my_outer_kernel(id, a)\n단지 첫번째 함수 my_inner_kernel! 의 인자의 함수가 파라미터로 특정되었을 뿐이다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_synchronization",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_synchronization",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "5 동기화 (Synchronization)",
    "text": "5 동기화 (Synchronization)\n블록에서 스레드를 동기화하려면 sync_threads() 함수를 사용한다. predicate 를 취하는 보다 고급 변형도 사용 가능하다.\n\nsync_threads_count(pred) : pred 가 true 인 스레드의 갯수를 반환한다.\nsync_threads_and(pred) : 모든 스레드에서 pred 가 참이면 true 를 반환한다.\nsync_threads_or(pred) : 어떤 스레드에서 pred 가 참이면 true 를 반환한다.\n\n여러 스레드 동기화 장벽을 유지하려면 장벽을 식별하는 정수 인수를 취하는 barrier_sync 함수를 사용한다.\n워프에서 레인을 동기화하려면 sync_warp() 함수를 사용합니다. 이 함수는 참여할 레인을 선택하는 마스크를 취합니다(기본값은 FULL_MASK).\n실행 장벽이 아닌 메모리 장벽만 필요한 경우 펜스 내장 함수를 사용합니다.\n\nthreadfence_block : 블럭 내의 모든 쓰레드에서 메모리 정렬을 보장한다.\nthreadfence : 디바이스 내의 모든 쓰레드에서 메모리 정렬을 보장한다.\nthreadfence_system : 호스트 스레드와 peer 디바이스를 포함한 모든 스레드에서 메모리 정렬을 보장한다.\n\n\n\n공유 메모리 (Shared memory)\n스레드 간 통신을 위해 공유 메모리로 백업된 디바이스 배열은 CuStaticSharedArray 함수를 통해 할당될 수 있다. 다음은 배열의 순서를 바꾸는 커널이다. 커널 내의 b 가 스레드간 통신을 위해 공유 메모리로 백업된 배열이다.\nfunction reverse_kernel(a::CuDeviceArray{T}) where T\n    i = threadIdx().x\n    b = CuStaticSharedArray(T, 2)\n    b[2-i+1] = a[i]\n    sync_threads()\n    a[i] = b[i]\n    return\nend\n\na = cu([1,2])\n@cuda threads = 2 reverse_kernel(a)\n결과를 출력해보면 a 의 순서가 바뀌었음을 알 수 있다.\n\n공유 메모리의 크기를 미리 알 수 없고 각 크기에 대해 커널을 다시 컴파일하고 싶지 않은 경우 대신 CuDynamicSharedArray 타입을 사용할 수 있다. 이를 위해서는 공유 메모리의 크기(바이트)를 커널에 인수로 전달해야 한다.\nfunction reverse_kernel(a::CuDeviceArray{T}) where T\n    i = threadIdx().x\n    b = CuDynamicSharedArray(T, length(a))\n    b[length(a)-i+1] = a[i]\n    sync_threads()\n    a[i] = b[i]\n    return\nend\n\na = cu([1,2,3])\n@cuda threads=length(a) shmem=sizeof(a) reverse_kernel(a)\n동적 공유 메모리를 사용하는 다수의 배열이 필요한 경우 후속 CuDynamicSharedArray 생성자에 공유 메모리 시작부터 바이트 단위의 오프셋을 나타내는 오프셋 매개변수를 전달한다. @cuda 에 대한 shmem 키워드는 모든 배열에서 사용하는 총 공유 메모리 양이어야 합니다.\n\n\n\n경계 확인\n기본적으로 CuDeviceArray 를 인덱싱하면 경계 검사를 수행하고 인덱스가 경계를 벗어나면 오류를 발생시키는데 이는 비용이 많이 드는 작업이므로 인덱스가 경계 내에 있다는 것이 확실하다면 일반적인 배열과 마찬가지로 @inbounds 를 사용 할 수 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudalj_standard_output",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudalj_standard_output",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "6 표준 출력",
    "text": "6 표준 출력\nCUDA.jl 커널은 아직 Julia의 표준 입출력과 통합되지 않았지만 커널에서 표준 출력으로 인쇄하기 위한 몇 가지 기본 기능을 제공한다.\n\n@cuprintf: 표준 출력으로 형식화된 출력을 내보낸다.\n@cuprint 와 @cuprintln : 문자를 포함한 값을 표준 출력으로 내보낸다.\n@cushow : 객체의 이름과 값을 출력한다.\n\n@cuprintf 매크로는 모든 형식 옵션을 지원하지 않는다. 자세한 내용은 printf 에 대한 NVIDIA 설명서를 참조하라. @cuprintln 과 CUDA.jl 을 통해 모든 값을 적절한 문자열 표현으로 변환하는 것이 더 편리한 경우가 많다.\njulia&gt; @cuda threads=2 (()-&gt;(@cuprintln(\"Hello, I'm thread $(threadIdx().x)!\"); return))()\nHello, I'm thread 1!\nHello, I'm thread 2!\n 단순히 값만 출력하길 원한다면, which can be useful during debugging, @cushow 를 사용하라.\njulia&gt; @cuda threads=2 (()-&gt;(@cushow threadIdx().x; return))()\n(threadIdx()).x = 1\n(threadIdx()).x = 2\n이것들은 매우 제한된 수의 유형만 지원한다는 점에 유의하라. 따라서 디버깅 목적으로만 사용하는 것이 좋다. P",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_random_number",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_random_number",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "7 난수 발생",
    "text": "7 난수 발생\n커널에서 rand 혹은 randn 함수를 통해 난수 샘플을 얻을 수 있으며 이 때 GPU-호환 난수 발생기(GPU-compatible random number generator) 가 사용된다. API는 CPU에서 사용되는 난수 생성기와 매우 유사하지만 병렬 RNG의 설계에서 비롯된 몇 가지 차이점과 고려해야할 항이 있다.\n\n기본 RNG는 글로벌 상태를 사용한다. 여러개의 인스턴스를 사용하는 것은 정의되지 않은 동작이다.\n커널은 host 에서 전달된 고유한 seed 로 RNG를 자동으로 시드하여 동일한 커널을 여러 번 호출해도 다른 결과가 생성되도록 한다.\nRandom.seed! 를 호출하여 seed 를 수동으로 지정하는 것이 가능하지만 RNG는 워프 공유 상태를 사용하므로 워프당 최소 하나의 스레드가 seed 되어야 하며 워프 내의 모든 seed 는 동일해야 한다.\n후속 커널 호출이 난수를 계속 발생시켜야 하는 경우 seed 뿐만 아니라 카운터 값도 Random.seed! 를 사용하여 수동으로 구성해야 한다. 여기에 대한 예는 CUDA.jl 의 host 측 RNG를 참조하라.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_atomics",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_atomics",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "8 Atomics",
    "text": "8 Atomics\nCUDA.jl 은 두 레벨의 추상화를 통해 아토믹 연산을 제공한다\n\n저수준 : atomic_ 함수는 직접 hardware instruction 에 매핑한다.\n고수준 : CUDA.@atomic 은 편리한 성분별 연산(element-wise operation) 을 제공한다.\n\n저수준 추상화는 안정적이고 이후의 동작을 변경하지 않으므로 아토믹 연산을 사용하는 가장 안전한 방법이다. 그러나 이 경우 인터페이스는 제한적이며, 하드웨어가 제공하는 것만 지원하고, 입력 타입이 일차할것을 요구한다. CUDA.@atomic API는 훨씬 더 사용자 친화적이지만 Julia Base 의 @atomic 매크로와 통합되면 어느 시점에서 사라질 것이다.\n\n\n저수준\n저수준 아토믹 내재 함수는 포인터 입력을 사용하며 이는 CuArray 에서 포인터 함수를 호출하여 얻을 수 있다.\njulia&gt; function atomic_kernel(a)\n           CUDA.atomic_add!(pointer(a), Int32(1))\n           return\n       end\n\njulia&gt; a = cu(Int32[1])\n1-element CuArray{Int32, 1, CUDA.DeviceMemory}:\n 1\n\njulia&gt; @cuda atomic_kernel(a)\n\njulia&gt; a\n1-element CuArray{Int32, 1, CUDA.DeviceMemory}:\n 2\n지원되는 아토믹 연산은 다음과 같다.\n\n이항연산 : add, sub, and, or, xor, min, max, xchg\nNVIDIA-특정적인 이항연산 : inc, dec\n비교와 교환 : cas\n\n자세한 유형 지원 및 하드웨어 요구 사항은 해당 내장 함수의 설명서를 참조하라.\n\n\n\n고수준\n배열에 대한 아토믹 연산을 위해 CUDA.@atomic 매크로를 사용한다.\nfunction atomic_kernel(a)\n    CUDA.@atomic a[1] += 1\n    return\nend\n\na = CUDA.zeros(3)\n\n@cuda atomic_kernel(a)\n이 결과로 a 는 다음과 같다.\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n 0.0\n 0.0\n 0.0\n이 매크로는 훨씬 더 관대한 특성을 가지며, 입력값을 적절한 타입으로 자동 변환하고, 지원되지 않는 연산에 대해서는 아토믹 비교-교환 루프로 대체된다. 하지만, 이는 CUDA.jl 이 Julia Base의 @atomic 매크로와 통합되면 사라질 가능성이 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_warp_intrinsics",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_warp_intrinsics",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "9 워프 내장 함수(Warp intrinsics)",
    "text": "9 워프 내장 함수(Warp intrinsics)\nCUDA의 대부분의 워프 내장 함수는 CUDA.jl 에서 유사한 이름으로 사용할 수 있다. 이들의 동작은 대부분 동일하지만 몇 가지 차이점이 있다. 이들은 1 부터 시작하는 인덱스를 사용하며, 입력을 자동으로 변환하고 분할(일정 부분)하여 더 많은 타입을 지원한다.\n\n인덱싱 : laneid, lanemask, active_mask, warpsize\n셔플 : shfl_sync, shfl_up_sync, shfl_down_sync, shfl_xor_sync\n투표 : vote_all_sync, vote_any_sync, vote_unisync, vote_ballot_sync\n\n이 내장 함수들 중 많은 수가 마스크 인수를 필요로 하며, 이는 어떤 레인이 연산에 참여해야 하는지를 나타내는 비트 마스크이다. 모든 레인을 기본값으로 설정하려면 FULL_MASK 상수를 사용하면 된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_dynamic_parallelism",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_dynamic_parallelism",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "10 동적 병렬성 (Dynamic parallelism)",
    "text": "10 동적 병렬성 (Dynamic parallelism)\n커널은 일반적으로 호스트에서 실행되지만, 동적 병렬성을 사용하면 커널 내부에서 다른 커널을 실행할 수도 있다. 이는 재귀 알고리즘이나 동적으로 새로운 작업을 생성해야 하는 알고리즘에 유용하다. 동적 병렬성을 활용하면 GPU 내에서 작업 요구 사항에 맞게 추가적인 병렬 작업을 생성하고 실행할 수 있어 유연성과 효율성을 높일 수 있다.\nfunction outer()\n    @cuprint(\"Hello \")\n    @cuda dynamic=true inner()\n    return\nend\n\nfunction inner()\n    @cuprintln(\"World!\")\n    return\nend\n\n@cuda outer();\nHello World!\n\n커널 내에서는 제한된 하위 집합의 CUDA API만 사용할 수 있다. 여기에는 주로 다음과 같은 기능들이 포함된다.\n\n동기화 : device_synchronize 함수를 통해 동기화를 수행한다.\n스트림 : CuDeviceStream 생성자와 unsafe_destroy! 소멸자를 사용할 수 있다. 이러한 스트림은 @cuda 매크로를 사용할 때 stream 키워드 인수를 통해 전달할 수 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_cooperative_groups",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#sec-cudajl_cooperative_groups",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "11 협력 그룹",
    "text": "11 협력 그룹\n협력 그룹(cooperative groups)을 사용하면 특정 스레드 구성에 얽매이지 않고, 스레드를 더 동적으로 분할하고 스레드 그룹 간에 통신할 수 있는 병렬 커널을 작성할 수 있습니다. 이 기능은 CUDA.jl에서 비교적 새롭게 도입된 것이며, 아직 협력 그룹 프로그래밍 모델의 모든 측면을 지원하지는 않습니다.\n본질적으로, 스레드 인덱스를 수동으로 계산하고 이를 사용하여 계산을 구분하는 대신, 이제 커널 기능은 자신이 속한 그룹을 조회하고, 해당 그룹의 크기, rank 등을 조회할 수 있습니다:\njulia&gt; function reverse_kernel(d::CuDeviceArray{T}) where {T}\n           block = CG.this_thread_block()\n\n           n = length(d)\n           t = CG.thread_rank(block)\n           tr = n-t+1\n\n           s = @inbounds CuDynamicSharedArray(T, n)\n           @inbounds s[t] = d[t]\n           CG.sync(block)\n           @inbounds d[t] = s[tr]\n\n           return\n       end\n\njulia&gt; a = cu([1,2,3])\n3-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 1\n 2\n 3\n\njulia&gt; @cuda threads=length(a) shmem=sizeof(a) reverse_kernel(a)\n\njulia&gt; a\n3-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 3\n 2\n 1\n\n아래와 같은 암묵적 그룹이 지원됩니다.\n\n스레드 블럭 : CG.this_thread_block()\n그리드 그룹 : CG.this_grid()\n워프 : CG.coalesced_threads()\n\n이런 지원은 클러스터나 multi-grid implicit groups, 그리고 모든 명시적인 (tiled, patitioned) 그룹에는 해당하지 않습니다.\n스레드 블록은 모든 장치와 모든 커널에서 지원됩니다. Grid 그룹(CG.this_grid()) 은 전체 그리드를 동기화하는 데 사용할 수 있는데, 이는 일반적으로 불가능하지만 추가적인 주의가 필요합니다. 커널은 @cuda cooperative=true 를 사용하여 협력 모드로 실행되어야 하며, 이는 compute capability 6.0 이상인 장치에서만 지원됩니다. 또한 협력 커널은 장치의 SM(Streaming Multiprocessor) 수만큼의 블록만 실행할 수 있습니다.\n\n\n인덱싱\n모든 종류의 스레드 그룹은 다음의 인덱스 연산을 지원합니다.\n\nthread_rank : 그룹 내에서 현재 스레드의 랭크를 반환합니다.\nnum_thread : 그룹의 스레드 개수를 반환합니다.\n\n추가로 일부 그룹 유형은 추가적인 인덱스 연산을 지원합니다.\n\n스레드 블럭 : group_index, thread_index, dim_threads\n그리드 그룹 : block_rank, num_blocks, block_index\ncoalesced group : meta_group_rank, meta_group_size\n\n자세한 것은 각 함수의 docstring 을 참조하세요.\n\n\n\n동기화\n그룹 객체는 그룹 내 스레드를 동기화하기 위해 CG.sync 연산을 지원합니다.\n추가적으로, 스레드 및 그리드 그룹은 더 세분화된 동기화를 위해 CG.barrier_arrive 와 CG.barrier_wait 을 사용하는 배리어(barrier)를 지원합니다. barrier_arrive 를 호출하면 토큰이 반환되며, 이 토큰을 barrier_wait 에 전달하여 동기화를 수행해야 합니다.\n\n\n\n집합 연산\n특정 집합 연산(즉, 여러 스레드가 수행해야 하는 연산)은 협력 그룹을 사용할 때 더 편리한 API를 제공합니다. 예를 들어, 셔플 셰플링(shuffle) 내재 함수는 일반적으로 스레드 마스크가 필요하지만, 이를 그룹 객체로 대체할 수 있습니다:\nfunction reverse_kernel(d)\n    cta = CG.this_thread_block()\n    I = CG.thread_rank(cta)\n\n    warp = CG.coalesced_threads()\n    i = CG.thread_rank(warp)\n    j = CG.num_threads(warp) - i + 1\n\n    d[I] = CG.shfl(warp, d[I], j)\n\n    return\nend\n\n아래와 같은 집합 연산이 지원됩니다.\n\nshuffle: shfl, shfl_down, shfl_up\nvoting: vote_any, vote_all, vote_ballot\n\n\n\n\n데이터 전송\n스레드 블록과 병합된(coalesced) 그룹에서는 비동기 메모리 복사를 수행하기 위해 CG.memcpy_async 함수가 제공됩니다. 현재는 장치에서 공유 메모리로의 복사만 가속화되며, compute capability 8.0 이상인 장치에서만 지원됩니다. 그러나 구현은 우아하게 강등(degrade)되어 동기화 복사로 대체됩니다:\njulia&gt; function memcpy_kernel(input::AbstractArray{T}, output::AbstractArray{T},\n                              elements_per_copy) where {T}\n           tb = CG.this_thread_block()\n\n           local_smem = CuDynamicSharedArray(T, elements_per_copy)\n           bytes_per_copy = sizeof(local_smem)\n\n           i = 1\n           while i &lt;= length(input)\n               # this copy can sometimes be accelerated\n               CG.memcpy_async(tb, pointer(local_smem), pointer(input, i), bytes_per_copy)\n               CG.wait(tb)\n\n               # do something with the data here\n\n               # this copy is always a simple element-wise operation\n               CG.memcpy_async(tb, pointer(output, i), pointer(local_smem), bytes_per_copy)\n               CG.wait(tb)\n\n               i += elements_per_copy\n           end\n       end\n\njulia&gt; a = cu([1, 2, 3, 4]);\njulia&gt; b = similar(a);\njulia&gt; nb = 2;\n\njulia&gt; @cuda shmem=sizeof(eltype(a))*nb memcpy_kernel(a, b, nb)\n\njulia&gt; b\n4-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 1\n 2\n 3\n 4\n위의 예제는 복사가 완료될 때까지 기다린 후 계속 진행하지만, CG.wait_prior 함수를 사용하여 여러 복사를 동시에 진행할 수도 있습니다. 이 함수는 마지막 N개의 복사를 제외한 모든 복사가 완료될 때까지 기다립니다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#gpu-및-병렬-처리",
    "href": "src/gpu/cuda/01_introduction.html#gpu-및-병렬-처리",
    "title": "Introduction",
    "section": "1 GPU 및 병렬 처리",
    "text": "1 GPU 및 병렬 처리\n병렬 처리 시스템은 그 아키텍쳐와 메모리 공유 상테에 따라 분류 할 수 있다.\n\n병렬 처리 하드웨어 아키텍쳐\n플린(M. J. Flynn) 은 병렬 처리 하드웨어를 두가지 기준으로 네가지로 분류하였다.\n (\\(1\\)) 한 셋의 데이터 흐름에 대해 수행하는 명령어(instruction) 의 개수 : 1 혹은 1 이상 (&gt;1)\n (\\(2\\)) 하나의 명령에 의해 수행되는 데이터의 흐름의 수 : 1 혹은 1 이상 (&gt;1)\n\n\n\n아키텍쳐\n설명\n(\\(1\\)) 기준\n(\\(2\\)) 기준\n\n\n\n\nSISD\nSingle instruction stream,single data stream\n1\n1\n\n\nSIMD\nSingle instruction stream, multiple data stream\n1\n&gt;1\n\n\nMISD\nMultiple instruction stream, single data stream\n&gt;1\n1\n\n\nMIMD\nMultiple instruction stream multiple data stream\n&gt;1\n&gt;1\n\n\n\nMISD 구조는 아직 실현되지 않았으며 병렬컴퓨팅에서 사용되는 구조는 SIMD 와 MIMD(멀티코어 CPU) 이다.\n\n\nMIMD\n명령어와 데이터가 1-1 로 매칭되는 SISD 가 하나의 칩 안에 들어있는 구조가 널리 사용된다. 각각의 코어가 자신만의 제어 유닛과 문맥을 가지고 독립적으로 프로세스를 수해 여러개의 코어가 하나의 프로세서 칩 안에 존재한다. 또한 하나의 컴퓨터에 여러개의 프로세서 칩이 있을 경우 멀티코어 멀티 프로세서라고 한다.\n\n\n\nSIMD\n동일한 명령어를 여러 데이터에 대해 수행한다. 데이터 배열의 각 성분에 동일한 연산을 수항한다는 의미에서 벡터 프로세서, 혹은 배열 프로세서라고도 혼다. 대표적으로 GPU 가 있다. 일부 CPU 는 내부적으로 SIMD 유닛을 가지고 있는데 CPU 사양에서 이야기하는 MMX, SSE, AVX, Neon 등이 이러한 SIMD 유닛을 의미한다.\n\nALU(arithmetic logic unit, 산술 논리 유닛) 는 하나의 연산 유닛 혹은 코어를 의미한다. 하나의 제어 유닛이 여러개의 ALU를 제어하는 경우가 SIMD 의 대표적인 경우이고, ALU 와 제어 유닛이 결합된 세트가 여러개일 때가 MIMD 의 대표적인 경우이다.\n\n\n\n\n공유 메모리와 분산 메모리\n\n여러 ALU 가 메모리 공간을 공유하는 시스템을 공유 메모리 시스템이라고 하며, 각 ALU 가 각각의 메모리 공간을 갖고 ALU 간 통신이 필요한 경우 명시적인 통신을 하는 시스템을 분산 메모리 시스템이라고 한다.\n공유 메모리 시스템의 경우 여러 ALU 가 하나의 메모리 공간에 접근하기 때문에 각각의 작업이 간섭을 일으 킬 수 있다. 이 경우 각각의 ALU 의 작업 순서를 맞추는 것을 동기화(synchoronization) 이라고 한다.\nGPU 도 공유 메모리 시스템을 사용한다.\n\n\n\n\nSIMT\nGPU 는 SISD 아키텍쳐와 공유 메모리 시스템을 사용하지만 일반적으로 SIMT(Single instruction multiple threads) 구조로 정의된다.\n\n한 스레드 그룹 내의 스레드들은 하나의 제어 유닛으로 제어된다.\n각 스레드는 자신만의 제어 문맥을 가진다.\n스레드 그룹 내 스레드 들 사이의 분기가 허용된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#gpu-사용의-이점",
    "href": "src/gpu/cuda/01_introduction.html#gpu-사용의-이점",
    "title": "Introduction",
    "section": "2 GPU 사용의 이점",
    "text": "2 GPU 사용의 이점\n\nCPU 와 GPU 의 목적\n\n\n\n표 1: CPU 와 GPU 의 비교\n\n\n\n\n\n\nCPU\nGPU\n\n\n\n\n지향\n각 코어의 성능 향상\n병렬 처리 성능 향상\n\n\n코어 수\n1 ~ 수십개\n수백 ~ 수천개\n\n\n개별 코어의 성능\n높다\n낮다\n\n\n구조\nSISD, MIMD\nSIMT\n\n\n공간 분배\n캐시 및 제어 유닛에 많이\n연산 유닛에 많이\n\n\n메모리 크기\n수 GB 이상\n수 GB 이상\n\n\n메모리 접근\n접근 지연 시간 최적화\n메모리 대역폭 최대화\n\n\n\n\n\n\n\n이것을 정리하면 다음과 같다.\n\n\n\n\n\n\n\n장치\n기본 목적\n\n\n\n\nCPU\n스레드를 최대한 빠르게 처리. 최대 수십개의 스레드를 병렬적으로 처리 할 수 있음.\n\n\nGPU\n수천개의 스레드를 를 병렬적으로 최대한 빠르게. 단일 스레드는 느리더라도 throughput 을 최대한으로\n\n\n\n\n\nCPU 에 비해 상대적으로 캐싱과 흐름 제어보다 데이터 처리에 더 많은 트랜지스터를 사용한다.\n아래 그림은 CPU 와 GPU 칩의 resource 분배를 보여준다. (source : CUDA C Programming guide )\n\n\n\n\n\n\n\n\n그림 1: The GPU Devotes More Transistors to Data Processing\n\n\n\n\n데이터 처리에 더 많은 트랜지스터를 할당하면 예를 들어 부동 소수점 계산의 병렬 계산처리에 유용하다. GPU는 긴 메모리 액세스 대기 시간을 피하기 위해 대용량 데이터 캐시와 복잡한 흐름 제어에 의존하는 대신 계산을 통해 메모리 액세스 대기 시간을 숨길 수 있다. 대용량 데이터 케시와 복잡한 흐름제어 둘 다 트랜지스터 측면에서 비용이 많이 든다.\n\n일반적으로 애플리케이션은 병렬적인 부분과 순차적인 부분이 혼합되어 있으므로 시스템은 전반적인 성능을 극대화하기 위해 GPU와 CPU를 혼합하여 설계된다. 높은 수준의 병렬성을 갖춘 애플리케이션은 GPU의 이러한 대규모 병렬적 특성을 활용하여 CPU보다 더 높은 성능을 달성할 수 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#cuda-a-general-purpose-parallel-computing-platform-and-programming-model",
    "href": "src/gpu/cuda/01_introduction.html#cuda-a-general-purpose-parallel-computing-platform-and-programming-model",
    "title": "Introduction",
    "section": "3 CUDA®: A General-Purpose Parallel Computing Platform and Programming Model",
    "text": "3 CUDA®: A General-Purpose Parallel Computing Platform and Programming Model\n\nCUDA : 범용 병렬 컴퓨팅 플랫폼이자 NVIDIA GPU의 병렬 컴퓨팅 엔진을 활용하는 프로그래밍 모델\n\n\n\n\n\n\n\n\n그림 2: GPU Computing Applications. CUDA is designed to support various languages and application programming interfaces.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#확장가능한-프로그래밍-모델",
    "href": "src/gpu/cuda/01_introduction.html#확장가능한-프로그래밍-모델",
    "title": "Introduction",
    "section": "4 확장가능한 프로그래밍 모델",
    "text": "4 확장가능한 프로그래밍 모델\n\nMulticore(CPU) 와 Manycore(GPU) 의 등장으로 처리 장치의 주류가 병렬 시스템이 되었으며, 3D 그래픽 응용 프로그램이 병렬성을 투명하게 확장하여 코어 수가 매우 다양한 많은 코어 GPU로 확장되는 것처럼 이런 병렬성을 투명하게 확장하여 증가하는 프로세서 코어 수를 활용하는 애플리케이션 소프트웨어를 개발하는 것이 목표가 되었다.\nCUDA 병렬 프로그래밍 모델은 C 와 같은 표준 프로그래밍 언어에 익숙한 프로그래머에게 낮은 학습 곡선을 유지하면서 이러한 과제를 극복하도록 설계되었다.\n핵심에는 스레드 그룹, 공유 메모리, 배리어 동기화의 계층 구조라는 세 가지 핵심 추상화가 있으며, 이는 단순히 최소한의 언어 확장 세트로 프로그래머에게 주어진다.\n이러한 추상화는 거친 데이터 병렬성과 작업 병렬성 내에 포개어진 미세한 데이터 병렬성과 스레드 병렬성을 제공한다. 이는 프로그래머가 문제를 스레드 블록으로 독립적으로 병렬로 해결할 수 있는 거친 하위 문제로 분할하고, 각 하위 문제를 블록 내의 모든 스레드가 협력하여 병렬로 해결할 수 있는 더 미세한 부분으로 분할하도록 안내한다.\n이 분해는 각 하위 문제를 해결할 때 스레드가 협력할 수 있도록 하여 언어 표현력을 보존하고 동시에 자동 확장성을 가능하게 한다. 실제로 각 스레드 블록은 GPU 내의 사용 가능한 모든 멀티프로세서에서 어떤 순서로든 동시에 또는 순차적으로 예약될(scheduled) 수 있으므로 컴파일된 CUDA 프로그램은 그림 3 에서 설명한 대로 멀티프로세서의 갯수가 몇개든 실행할 수 있으며 런타임 시스템만 물리적인 멀티프로세서 수를 알면 된다.\n이 확장 가능한 프로그래밍 모델을 사용하면 GPU 아키텍처가 멀티프로세서 수와 메모리 파티션을 간단히 확장하여 광범위한 시장 범위에 걸쳐 확장할 수 있다. 고성능 매니아용 GeForce GPU와 전문가용 Quadro 및 Tesla 컴퓨팅 제품부터 다양한 저렴한 주류 GeForce GPU까지 다양합니다(모든 CUDA 지원 GPU 목록은 CUDA 지원 GPU 참조).\n\n\n\n\n\n\n\n\n노트\n\n\n\nGPU는 스트리밍 멀티프로세서(streaming multiprocessor, SM) 배열을 중심으로 구축됩니다(자세한 내용은 하드웨어 구현 참조). 멀티스레드 프로그램은 서로 독립적으로 실행되는 스레드 블록으로 분할되므로 멀티프로세서가 더 많은 GPU가 멀티프로세서가 적은 GPU보다 자동으로 프로그램을 더 짧은 시간 내에 실행합니다.\n\n\n\n\n\n\n\n\n\n그림 3: Automatic Scalability",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html",
    "href": "src/gpu/cuda/03_programming_interface.html",
    "title": "Programming Interface",
    "section": "",
    "text": "CUDA C++는 C++ 프로그래밍 언어에 익숙한 사용자에게 장치에서 실행할 프로그램을 쉽게 작성할 수 있는 단순한 경로를 제공한다. C++ 언어에 대한 최소한의 확장 세트와 런타임 라이브러리로 구성되며 핵심 언어 확장은 Programming Model에 소개되었다. 프로그래머는 커널을 C++ 함수로 정의하고 새로운 구문을 사용하여 함수가 호출될 때마다 그리드와 블록의 차원을 지정할 수 있다. 모든 확장에 대한 전체 설명은 C++ 언어 확장에서 찾을 수 있다. 이러한 확장 중 일부를 포함하는 모든 소스 파일은 NVCC 컴파일에 설명된 대로 nvcc 로 컴파일해야 한다.\n런타임에 대해서는 CUDA 런타임 을 참고하라. 호스트에서 실행되어 디바이스 메모리를 할당하거나 할당 해제하고, 호스트 메모리와 디바이스 메모리 간에 데이터를 전송하고, 여러 장치가 있는 시스템을 관리하는 등의 작업을 수행하는 C 및 C++ 함수를 제공한다. 런타임에 대한 전체 설명은 CUDA Reference Manumal 에서 찾을 수 있다.\n런타임은 하위 수준 C API인 CUDA 드라이버 API 위에 구축되며 응용프로그램에서도 API 에 접근 할 수 있다. 드라이버 API는 CUDA 컨텍스트(장치의 호스트 프로세스와 유사) 및 CUDA 모듈(장치의 동적으로 로드된 라이브러리와 유사)과 같은 하위 수준 개념을 노출하여 추가적인 제어 수준을 제공한다. 대부분의 애플리케이션은 이 추가적인 제어 수준이 필요하지 않기 때문에 드라이버 API를 사용하지 않으며 런타임을 사용할 때 컨텍스트 및 모듈 관리가 암묵적이기 때문에 코드가 더 간결해진다. 런타임은 드라이버 API와 호환되기 때문에 일부 드라이버 API 기능이 필요한 대부분의 애플리케이션은 기본적으로 런타임 API를 사용하고 필요한 경우에만 드라이버 API를 사용할 수 있다. 드라이버 API는 드라이버 API에 소개되어 있으며 Reference manual 에 자세히 설명되어 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html#sec-cuda_nvcc_compile",
    "href": "src/gpu/cuda/03_programming_interface.html#sec-cuda_nvcc_compile",
    "title": "Programming Interface",
    "section": "1 NVCC 컴파일",
    "text": "1 NVCC 컴파일\n커널은 PTX 라고 하는 CUDA 명령어 집합 아키텍처를 사용하여 작성할 수 있으며, 이는 PTX Reference manual 에 설명되어 있다. 그러나 일반적으로 C++와 같은 고급 프로그래밍 언어를 사용하는 것이 더 효과적이다. 두 경우 모두 커널은 nvcc 에서 바이너리 코드로 컴파일하여 장치에서 실행해야 한다.\nnvcc 는 C++ 또는 PTX 코드를 컴파일하는 과정을 간소화하는 컴파일러 드라이버이다. 간단하고 익숙학 명령줄 옵션을 제공하고 다양한 컴파일 단계를 구현하는 도구 모음을 호출하여 실행한다. 이 섹션에서는 nvcc 워크플로 및 명령 옵션에 대해 간단히 소개한다. 전체 설명은 nvcc user manual 에서 찾을 수 있다.\n\n\n1.1 컴파일 workflow\n\n오프라인 컴파일\nnvcc 로 컴파일된 소스 파일에는 호스트 코드(즉, 호스트에서 실행되는 코드)와 디바이스 코드(즉, 디바이스에서 실행되는 코드)가 혼합되어 포함될 수 있다. nvcc 의 기본 워크플로는 디바이스 코드를 호스트 코드에서 분리하는 것으로 시작하여 다음을 수행한다.\n\n디바이스 코드를 어셈블리 형태(PTX 코드) 및/또는 바이너리 형태(cubin 객체)로 컴파일하고,\nKernels에서 도입된 &lt;&lt;&lt;...&gt;&gt;&gt; 구문을 필요한 CUDA 런타임 함수 호출로 대체하여 호스트 코드를 수정하여 PTX 코드 및/또는 cubin 객체에서 각 컴파일된 커널을 로드하고 구동시킨다. 이 과정은 커널 실행 구성을 더 자세히 설명하는 Execution Configuration에서 다룬다.\n\n수정된 호스트 코드는 다른 도구를 사용하여 컴파일할 수 있는 C++ 코드로 출력되거나, 마지막 컴파일 단계에서 nvcc 가 호스트 컴파일러를 호출하여 직접 object code 로 출력된다. 이제 애플리케이션은 다음을 수행할 수 있다.\n\n컴파일된 호스트 코드에 링크(가장 일반적인 경우)하거나\n수정된 호스트 코드(있는 경우)를 무시하고 CUDA 드라이버 API(드라이버 API 참조)를 사용하여 PTX 코드 또는 cubin 객체를 로드하고 실행한다.\n\n\n\n\nJIT 컴파일\n런타임에 애플리케이션에서 로드한 모든 PTX 코드는 장치 드라이버에 의해 바이너리 코드로 추가로 컴파일되는데 이를 JIT(just-in-time) 컴파일이라고 한다. JIT 컴파일은 애플리케이션 로드 시간을 늘리지만 애플리케이션이 각 새 장치 드라이버와 함께 제공되는 모든 새 컴파일러 개선 사항을 활용할 수 있게 한다. 또한 애플리케이션이 컴파일된 당시 존재하지 않았던 장치에서 애플리케이션을 실행할 수 있는 유일한 방법이기도 하다(애플리케이션 호환성에서 자세히 설명).\n디바이스 드라이버가 일부 애플리케이션에 대한 일부 PTX 코드를 JIT 컴파일할 때, 이후 애플리케이션 호출에서 컴파일을 반복하지 않기 위해 생성된 바이너리 코드의 사본을 자동으로 캐시한다. 캐시(컴퓨트 캐시라고 함)는 장치 드라이버가 업그레이드될 때 자동으로 무효화되므로 애플리케이션은 장치 드라이버에 내장된 새로운 JIT 컴파일러의 개선 사항을 활용 할 수 있다.\nJIT 컴파일을 제어하는 환경 변수는 CUDA 환경 변수에 설명된 대로 ​​사용할 수 있다.\nnvcc 를 사용하여 CUDA C++ 장치 코드를 컴파일하는 대신 NVRTC를 사용하여 런타임에 CUDA C++ 장치 코드를 PTX로 컴파일할 수 있다. NVRTC는 CUDA C++용 런타임 컴파일 라이브러리이다. 자세한 내용은 NVRTC 사용자 가이드에서 확인할 수 있다.\n\n\n\n\n1.2 바이너리 호환성\n바이너리 코드는 아키텍처에 따라 다르다. cubin 객체는 -code 컴파일러 옵션을 사용하여 대상 아키텍처를 지정하여 생성된다. 예를 들어, -code=sm_80 으로 컴파일하면 compute capability 8.0의 장치에 대한 바이너리 코드가 생성된다. 바이너리 호환성은 한 마이너 리비전에서 다음 리비전으로 보장되지만, 한 마이너 리비전에서 이전 리비전으로 또는 주요 리비전 간에는 보장되지 않는다. 즉, compute capability X.y에 대해 생성된 cubin 객체는 z≥y인 컴퓨팅 기능 X.z의 장치에서만 실행된다.\n\n\n\n\n\n\n\n경고\n\n\n\n바이너리 호환성은 데스크톱에서만 지원되며 Tegra에서는 지원되지 않는다. 또한 데스크톱과 Tegra 간의 바이너리 호환성도 지원되지 않는다.\n\n\n\n\n\n1.3 PTX 호환성\n일부 PTX 명령어는 고성능 장치에서만 지원된다. 예를 들어, Warp Shuffle(https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions) 함수는 compute capability 5.0 이상의 장치에서만 지원된다. -arch 컴파일러 옵션은 C++를 PTX 코드로 컴파일할 때 가정하는 compute capability 를 지정한다. 따라서 예를 들어 Warp Shuffle이 포함된 코드는 -arch=compute_50 (또는 그 이상)으로 컴파일해야 한다.\n특정 compute capability 를 위해 생성된 PTX 코드는 항상 더 크거나 같은 컴퓨팅 기능의 바이너리 코드로 컴파일할 수 있다. 이전 PTX 버전에서 컴파일된 바이너리는 일부 하드웨어 기능을 사용하지 못할 수 있다. 예를 들어, compute capability 6.0(Pascal)을 위해 생성된 PTX에서 컴파일된 compute capability 7.0(Volta) 장치를 대상으로 하는 바이너리는 Pascal에서 사용할 수 없는 Tensor Core 명령어를 사용하지 않는다. 결과적으로 최종 바이너리는 최신 버전의 PTX를 사용하여 바이너리를 생성한 경우보다 성능이 떨어질 수 있다.\n아키텍처 조건부 기능을 대상으로 컴파일된 PTX 코드는 정확히 동일한 물리적 아키텍처에서만 실행되고 다른 곳에서는 실행되지 않는다. 아치텍쳐 조건부 PTX 코드는 이전 혹은 이후와 호환되지 않는다. sm_90a 또는 compute_90a 로 컴파일된 예제 코드는 compute capability 9.0 이 있는 장치에서만 실행되며 이전 혹은 이후와 호환되지 않는다.\n\n\n\n1.4 응용 프로그램 호환성\n특정 compute capability 의 장치에서 코드를 실행하려면 애플리케이션은 바이너리 호환성 및 PTX 호환성에서 설명한 대로 이 compute capability 과 호환되는 바이너리 또는 PTX 코드를 로드해야 한다. 특히, 더 높은 컴퓨팅 기능을 갖춘 미래 아키텍처에서 코드를 실행하려면(아직 바이너리 코드를 생성할 수 없는 경우) 애플리케이션은 이러한 장치에 대해 JIT 컴파일되는 PTX 코드를 로드해야 한다.(JIT 컴파일 참조).\nCUDA C++ 애플리케이션에 어떤 PTX와 바이너리 코드가 포함되는지는 nvcc 사용자 매뉴얼에 자세히 설명된 대로 -arch 및 -code 컴파일러 옵션 또는 -gencode 컴파일러 옵션에 의해 제어된다. 예를 들어보자.\nnvcc x.cu\n        -gencode arch=compute_50,code=sm_50\n        -gencode arch=compute_60,code=sm_60\n        -gencode arch=compute_70,code=\\\"compute_70,sm_70\\\"\n위의 컴파일은 compute capability 5.0 및 6.0(첫 번째 및 두 번째 -gencode 옵션)과 호환되는 바이너리 코드와 compute capability 7.0(세 번째 -gencode 옵션)과 호환되는 PTX 및 바이너리 코드를 embeds 한다.\n호스트 코드는 런타임에 로드하고 실행할 가장 적합한 코드를 자동으로 선택하도록 생성된다. 위의 예에서 이는 다음과 같다.\n\ncompute capability 5.0 및 5.2가 있는 장치의 경우 5.0 바이너리 코드,\ncompute capability 6.0 및 6.1이 있는 장치의 경우 6.0 바이너리 코드,\ncompute capability 7.0 및 7.5가 있는 장치의 경우 7.0 바이너리 코드,\ncompute capability 8.0 및 8.6이 있는 장치의 경우 런타임에 바이너리 코드로 컴파일되는 PTX 코드.\n\n\nx.cu 는 예를 들어, 워프 감소 연산을 사용하는 최적화된 코드 경로를 가질 수 있으며, 이는 compute capability 8.0 이상의 장치에서만 지원된다. __CUDA_ARCH__ 매크로는 compute capability 에 따라 다양한 코드 경로를 구분하는 데 사용할 수 있다. 이는 장치 코드에 대해서만 정의된다. 예를 들어 -arch=compute_80 으로 컴파일할 때 __CUDA_ARCH__ 는 800 과 같다.\nx.cu 가 sm_90a 또는 compute_90a 를 사용하여 아키텍처 조건부 기능 예제에 대해 컴파일된 경우, 코드는 compute capability 9.0이 있는 장치에서만 실행할 수 있다.\n드라이버 API를 사용하는 애플리케이션은 코드를 컴파일하여 파일을 분리하고 런타임에 가장 적합한 파일을 명시적으로 로드하여 실행해야 한다.\nVolta 아키텍처는 GPU에서 스레드가 예약되는 방식을 변경하는 독립 스레드 스케줄링을 도입합니다. 이전 아키텍처에서 SIMT 스케줄링 의 특정 동작에 의존하는 코드의 경우 독립 스레드 스케줄링은 참여 스레드 세트를 변경하여 잘못된 결과를 초래할 수 있다. Independent Thread Scheduling 에서 자세히 설명한 시정 조치를 구현하는 동안 마이그레이션을 지원하기 위해 Volta 개발자는 컴파일러 옵션 조합 -arch=compute_60  -code=sm_70을 사용하여 Pascal의 스레드 스케줄링을 선택할 수 있습니다.\nnvcc 사용자 설명서에는 -arch, -code 및 -gencode 컴파일러 옵션에 대한 다양한 약어가 나와 있다. 예를 들어, -arch=sm_70 은 -arch=compute_70 -code=compute_70,sm_70 (-gencode arch=compute_70,code=\\\"compute_70,sm_70\\\" 과 동일)의 약어입니다.\n\n\n\n1.5 C++ 호환성\n컴파일러의 프런트 엔드는 C++ 구문 규칙에 따라 CUDA 소스 파일을 처리합니다. 호스트 코드에는 전체 C++가 지원된다. 그러나 C++ 언어 지원 에 설명된 대로 C++의 하위 집합만 장치 코드에 대해 완전히 지원된다.\n\n\n\n1.6 64 비트 호환성\nnvcc의 64비트 버전은 64비트 모드에서 디바이스 코드를 컴파일한다(즉, 포인터는 64비트이다). 64비트 모드에서 컴파일된 디바이스 코드는 64비트 모드에서 컴파일된 호스트 코드에서만 지원된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html#cuda-runtime",
    "href": "src/gpu/cuda/03_programming_interface.html#cuda-runtime",
    "title": "Programming Interface",
    "section": "2 CUDA Runtime",
    "text": "2 CUDA Runtime\n런타임은 cudart 라이브러리에서 구현되며, cudart.lib 또는 libcudart.a 를 통해 정적으로 또는 cudart.dll 또는 libcudart.so 를 통해 동적으로 응용프로그램에 링크된다. 동적 링크에 cudart.dll 및/또는 cudart.so 가 필요한 애플리케이션은 일반적으로 이를 애플리케이션 설치 패키지의 일부로 포함한다. CUDA 런타임 심볼의 주소는 CUDA 런타임의 동일한 인스턴스에 링크하는 구성 요소 사이에서만 안전하게 전달할 수 있다.\n모든 진입점에는 cuda 라는 접두사가 붙습니다.\n이기종 프로그래밍(Heterogeneous Programming) 에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리가 있는 호스트와 디바이스로 구성된 시스템을 가정한다. 장치 메모리 에서 장치 메모리를 관리하는 데 사용되는 런타임 함수에 대해 소개한다.\n공유 메모리 에서 스레드 계층에서 도입된 공유 메모리를 사용하여 성능을 극대화하는 방법을 보인다.\n페이지 잠금 호스트 메모리 에서는 호스트와 장치 메모리 간의 데이터 전송과 커널 실행을 겹치게 하는 데 필요한 페이지 잠금 호스트 메모리를 소개한다.\n비동기 동시 실행 에서는 시스템의 다양한 수준에서 비동기 동시 실행을 가능하게 하는 데 사용되는 개념과 API를 설명한다.\n다중 디바이스 시스템 에서는 프로그래밍 모델이 동일한 호스트에 연결된 여러 장치가 있는 시스템으로 확장되는 방식을 보여준다.\n오류 검사 에서는 런타임에서 생성된 오류를 올바르게 검사하는 방법을 설명한다.\n호출 스택 은 CUDA C++ 호출 스택을 관리하는 데 사용되는 런타임 함수를 소개한다.\n텍스처 및 표면 메모리 에서는 장치 메모리에 액세스하는 또 다른 방법을 제공하는 텍스처 및 표면 메모리 공간을 설명한다. 또한 GPU 텍스처링 하드웨어의 하위 집합을 보여준다.\n그래픽 상호 운용성 은 런타임이 두 가지 주요 그래픽 API인 OpenGL 및 Direct3D 와 상호 운용하기 위해 제공하는 다양한 함수를 소개한다\n\n\n2.1 초기화\nCUDA 12.0부터 cudaInitDevice() 및 cudaSetDevice() 를 호출하면 지정된 디바이스와 연관된 런타임 및 기본 컨텍스트를 초기화합니다. 이러한 호출이 없으면 런타임은 암묵적으로 Device 0 을 사용하고 필요에 따라 자체로 초기화하여 다른 런타임 API 요청을 처리합니다. 런타임 함수 호출의 타이밍을 지정하고 첫 번째 호출의 오류 코드를 런타임으로 해석할 때 이 점을 염두에 두어야 합니다. 12.0 이전에는 cudaSetDevice() 가 런타임을 초기화하지 않았고 애플리케이션은 종종 no-op 런타임 호출 cudaFree(0) 를 사용하여 런타임 초기화를 다른 API 활동에서 분리했습니다(타이밍과 오류 처리를 위해).\n런타임은 시스템의 각 장치에 대한 CUDA 컨텍스트를 만듭니다(CUDA 컨텍스트에 대한 자세한 내용은 컨텍스트 참조). 이 컨텍스트는 이 디바이스의 기본 컨텍스트이며 이 디바이스 에서 활성 컨텍스트가 필요한 첫 번째 런타임 함수에서 초기화됩니다. 이 컨텍스트는 애플리케이션의 모든 호스트 스레드에서 공유됩니다. 이 컨텍스트 생성의 일부로, 필요한 경우 디바이스 코드가 JIT 컴파일 되고 디바이스 메모리에 로드됩니다. 이 모든 것이 투명하게 이루어집니다. 예를 들어 드라이버 API 상호 운용성을 위해 필요한 경우 디바이스의 기본 컨텍스트는 런타임 및 드라이버 API 간 상호 운용성에 설명된 대로 드라이버 API에서 액세스할 수 있습니다.\n호스트 스레드가 cudaDeviceReset() 을 호출하면 호스트 스레드가 현재 작동하는 디바이스의 기본 컨텍스트(즉, 디바이스 선택에서 정의된 현재 디바이스)가 파괴됩니다. 이 디바이스를 현재 디바이스로 갖는 호스트 스레드가 다음에 호출하는 런타임 함수는 이 장치에 대한 새 기본 컨텍스트를 생성합니다.\n\n\n\n\n\n\n\n노트\n\n\n\nCUDA 인터페이스는 호스트 프로그램 시작 중에 초기화되고 호스트 프로그램 종료 중에 파괴되는 전역 상태를 사용합니다. CUDA 런타임과 드라이버는 이 상태가 유효하지 않은지 감지할 수 없으므로 main) 이후 프로그램 시작 또는 종료 중에 이러한 인터페이스 중 하나를 명시적으로든 암묵적으로든 사용하면 정의되지 않은 동작이 발생합니다.\nCUDA 12.0부터 cudaSetDevice() 는 이제 호스트 스레드에 대한 현재 장치를 변경한 후 런타임을 명시적으로 초기화합니다. 이전 버전의 CUDA는 cudaSetDevice() 이후 첫 번째 런타임 호출이 이루어질 때까지 새 장치에서 런타임 초기화를 지연했습니다. 이 변경으로 인해 이제 초기화 오류에 대해 cudaSetDevice() 의 반환값을 확인하는 것이 매우 중요합니다.\n참조 매뉴얼의 오류 처리 및 버전 관리 섹션의 런타임 함수는 런타임을 초기화하지 않습니다.\n\n\n\n\n\n2.2 장치 메모리\n이기종 프로그래밍에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리를 가진 호스트와 디바이스로 구성된 시스템을 가정합니다. 커널은 디바이스 메모리에서 작동하므로 런타임은 디바이스 메모리를 할당, 할당 해제, 복사하고 호스트 메모리와 디바이스 메모리 간에 데이터를 전송하는 기능을 제공합니다.\n디바이스 메모리는 선형 메모리(Linear meomry - ?) 또는 CUDA 배열로 할당할 수 있습니다.\nCUDA 배열은 텍스처 페칭에 최적화된 불투명 메모리 레이아웃입니다. 텍스처 및 표면 메모리에서 설명합니다.\n선형 메모리는 단일 통합 주소 공간에 할당되므로 별도로 할당된 개체 예를 들어 이진 트리(binary tree) 또는 연결 리스트(linked list) 내에서 포인터를 통해 서로를 참조할 수 있습니다. 주소 공간의 크기는 호스트 시스템(CPU)과 사용된 GPU의 컴퓨팅 기능에 따라 달라집니다.\n\n\n\n\n표 1: Linear Memory Address Space\n\n\n\n\n\n\n\n\n\n\n\n\nx86_64 (AMD64)\nPOWER (ppc64le)\nARM64\n\n\n\n\nup to compute capability 5.3 (Maxwell)\n40bit\n40bit\n40bit\n\n\ncompute capability 6.0 (Pascal) or newer\nup to 47bit\nup to 49bit\nup to 48bit\n\n\n\n\n\n\n\n선형 메모리는 일반적으로 cudaMalloc() 를 사용하여 할당하고 cudaFree() 를 사용하여 해제하며 호스트 메모리와 장치 메모리 간의 데이터 전송은 일반적으로 cudaMemcpy() 를 사용하여 수행됩니다. 커널의 벡터 추가 코드 샘플에서 벡터는 호스트 메모리에서 장치 메모리로 복사해야 합니다.\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\n// Device code\n__global__ void VecAdd(float* A, float* B, float* C, int N)\n{\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i &lt; N)\n        C[i] = A[i] + B[i];\n}\n\n// Host code\nint main()\n{\n    int N = ...;\n    size_t size = N * sizeof(float);\n\n    // Allocate input vectors h_A and h_B in host memory\n    float* h_A = (float*)malloc(size);\n    float* h_B = (float*)malloc(size);\n    float* h_C = (float*)malloc(size);\n\n    // Initialize input vectors\n    ...\n\n    // Allocate vectors in device memory\n    float* d_A;\n    cudaMalloc(&d_A, size);\n    float* d_B;\n    cudaMalloc(&d_B, size);\n    float* d_C;\n    cudaMalloc(&d_C, size);\n\n    // Copy vectors from host memory to device memory\n    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n\n    // Invoke kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid =\n            (N + threadsPerBlock - 1) / threadsPerBlock;\n    VecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);\n\n    // Copy result from device memory to host memory\n    // h_C contains the result in host memory\n    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n    // Free host memory\n    ...\n}\n선형 메모리는 cudaMallocPitch() 및 cudaMalloc3D() 를 통해 할당할 수도 있습니다. 이러한 함수는 2D 또는 3D 배열의 할당에 권장되며, 디바이스 메모리 액세스에서 설명할 정렬 요구 사항을 충족하도록 할당이 적절하게 패딩되어 행 주소에 액세스하거나 2D 배열과 장치 메모리의 다른 영역 간에 복사를 수행할 때 최상의 성능을 보장합니다(cudaMemcpy2D() 및 cudaMemcpy3D() 함수 사용). 반환된 피치(또는 스트라이드)는 배열 요소에 접근하는 데 사용해야 합니다. 다음 코드 샘플은 부동 소수점 값의 너비 x 높이 2D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.\n// Host code\nint width = 64, height = 64;\nfloat* devPtr;\nsize_t pitch;\ncudaMallocPitch(&devPtr, &pitch,\n                width * sizeof(float), height);\nMyKernel&lt;&lt;&lt;100, 512&gt;&gt;&gt;(devPtr, pitch, width, height);\n\n// Device code\n__global__ void MyKernel(float* devPtr,\n                         size_t pitch, int width, int height)\n{\n    for (int r = 0; r &lt; height; ++r) {\n        float* row = (float*)((char*)devPtr + r * pitch);\n        for (int c = 0; c &lt; width; ++c) {\n            float element = row[c];\n        }\n    }\n}\n 다음 코드 샘플은 부동 소수점 값의 너비 x 높이 x 깊이 3D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.\n// Host code\nint width = 64, height = 64, depth = 64;\ncudaExtent extent = make_cudaExtent(width * sizeof(float),\n                                    height, depth);\ncudaPitchedPtr devPitchedPtr;\ncudaMalloc3D(&devPitchedPtr, extent);\nMyKernel&lt;&lt;&lt;100, 512&gt;&gt;&gt;(devPitchedPtr, width, height, depth);\n\n// Device code\n__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,\n                         int width, int height, int depth)\n{\n    char* devPtr = devPitchedPtr.ptr;\n    size_t pitch = devPitchedPtr.pitch;\n    size_t slicePitch = pitch * height;\n    for (int z = 0; z &lt; depth; ++z) {\n        char* slice = devPtr + z * slicePitch;\n        for (int y = 0; y &lt; height; ++y) {\n            float* row = (float*)(slice + y * pitch);\n            for (int x = 0; x &lt; width; ++x) {\n                float element = row[x];\n            }\n        }\n    }\n}\n\n\n\n\n\n\n\n노트\n\n\n\n너무 많은 메모리를 할당하여 시스템 전체 성능에 영향을 미치지 않도록 하려면 문제 크기에 따라 사용자에게 할당 매개변수를 요청하세요. 할당이 실패하면 다른 느린 메모리 유형(cudaMallocHost(), cudaHostRegister() 등)으로 폴백하거나 거부된 메모리가 얼마나 필요한지 알려주는 오류를 반환할 수 있습니다. 애플리케이션이 어떤 이유로 할당 매개변수를 요청할 수 없는 경우 이를 지원하는 플랫폼에 대해 cudaMallocManaged() 를 사용하는 것이 좋습니다.\n\n\n\n참조 설명서에는 cudaMalloc() 로 할당된 선형 메모리, cudaMallocPitch() 또는 cudaMalloc3D() 로 할당된 선형 메모리, CUDA 배열, 전역 또는 상수 메모리 공간에서 선언된 변수에 할당된 메모리 간에 메모리를 복사하는 데 사용되는 다양한 함수가 모두 나열되어 있습니다.\n\n다음 코드 샘플은 런타임 API를 통해 전역 변수에 액세스하는 다양한 방법을 보여줍니다.\n__constant__ float constData[256];\nfloat data[256];\ncudaMemcpyToSymbol(constData, data, sizeof(data));\ncudaMemcpyFromSymbol(data, constData, sizeof(data));\n\n__device__ float devData;\nfloat value = 3.14f;\ncudaMemcpyToSymbol(devData, &value, sizeof(float));\n\n__device__ float* devPointer;\nfloat* ptr;\ncudaMalloc(&ptr, 256 * sizeof(float));\ncudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));\ncudaGetSymbolAddress() 는 전역 메모리 공간에서 선언된 변수에 할당된 메모리를 가리키는 주소를 검색하는 데 사용됩니다. 할당된 메모리의 크기는 cudaGetSymbolSize() 를 통해 얻습니다.\n\n\n\n2.3 디바이스 메모리 L2 접근 관리\nCUDA 커널이 전역 메모리의 데이터 영역에 반복적으로 액세스하는 경우 이러한 데이터 액세스는 지속되는 것으로 간주될 수 있습니다. 반면, 데이터가 한 번만 액세스되는 경우 이러한 데이터 액세스는 스트리밍으로 간주될 수 있습니다.\nCUDA 11.0부터 컴퓨팅 기능 8.0 이상의 장치는 L2 캐시의 데이터 지속성에 영향을 미칠 수 있는 기능을 갖추고 있어 글로벌 메모리에 대한 더 높은 대역폭과 더 낮은 지연 시간 액세스를 제공할 수 있습니다.\n\n\nL2 Cache Set-Aside for Persisting Accesses\nL2 캐시의 일부는 전역 메모리에 대한 지속적인 데이터 액세스에 사용하도록 따로 보관할 수 있습니다. 지속적인 액세스는 L2 캐시의 이 따로 보관된 부분을 우선적으로 사용하지만, 글로벌 메모리에 대한 일반 또는 스트리밍 액세스는 지속적인 액세스에서 사용되지 않을 때만 L2의 이 부분을 활용할 수 있습니다.\n지속적인 액세스를 위한 L2 캐시 예약 크기는 다음 한도 내에서 조정될 수 있습니다.\ncudaGetDeviceProperties(&prop, device_id);\nsize_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);\ncudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/\n\nGPU가 Multi-Instance GPU(MIG) 모드로 구성된 경우 L2 캐시 예약 기능이 비활성화됩니다.\nMulti-Process Service(MPS)를 사용하는 경우 L2 캐시 예약 크기는 cudaDeviceSetLimit 으로 변경할 수 없습니다. 대신 예약 크기는 환경 변수 CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT 를 통해 MPS 서버를 시작할 때만 지정할 수 있습니다.\n\n\n\nL2 Policy for Persisting Accesses\naccess policy window 는 전역 메모리의 연속 영역과 해당 영역 내의 액세스에 대한 L2 캐시의 지속성 속성을 지정합니다. 아래 코드 예제는 CUDA 스트림을 사용하여 L2 persisting access window 를 설정하는 방법을 보여줍니다.\n#| code-overflow: scroll\ncudaStreamAttrValue stream_attribute;                                         // Stream level attributes data structure\nstream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast&lt;void*&gt;(ptr); // Global Memory data pointer\nstream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persistence access.\n                                                                              // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)\nstream_attribute.accessPolicyWindow.hitRatio  = 0.6;                          // Hint for cache hit ratio\nstream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit\nstream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.\n\n//Set the attributes to a CUDA stream of type cudaStream_t\ncudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);\n * to be done *\n\n\n\n\n2.4 공유 메모리\n가변 메모리 공간 지정자 에서 자세히 설명하겠지만 공유 메모리는 __shared__ 메모리 공간 지정자를 사용하여 할당됩니다.\n스레드 계층 구조에서 언급되고 공유 메모리에서 자세히 설명된 대로 공유 메모리는 전역 메모리보다 훨씬 빠를 것으로 예상됩니다. 다음 행렬 곱셈 예제에서 보여지는 것처럼 스크래치패드 메모리(또는 소프트웨어 관리 캐시)로 사용하여 CUDA 블록에서 글로벌 메모리 액세스를 최소화할 수 있습니다.\n다음 코드 샘플은 공유 메모리를 활용하지 않는 간단한 행렬 곱셈 구현입니다. 각 스레드는 A의 한 행과 B의 한 열을 읽고 C의 해당 요소를 계산합니다. 따라서 A는 글로벌 메모리에서 B.width 번 읽히고 B는 A.height 번 읽힙니다.\n// Matrices are stored in row-major order:\n// M(row, col) = *(M.elements + row * M.width + col)\ntypedef struct {\n    int width;\n    int height;\n    float* elements;\n} Matrix;\n\n// Thread block size\n#define BLOCK_SIZE 16\n\n// Forward declaration of the matrix multiplication kernel\n__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);\n\n// Matrix multiplication - Host code\n// Matrix dimensions are assumed to be multiples of BLOCK_SIZE\nvoid MatMul(const Matrix A, const Matrix B, Matrix C)\n{\n    // Load A and B to device memory\n    Matrix d_A;\n    d_A.width = A.width; d_A.height = A.height;\n    size_t size = A.width * A.height * sizeof(float);\n    cudaMalloc(&d_A.elements, size);\n    cudaMemcpy(d_A.elements, A.elements, size,\n               cudaMemcpyHostToDevice);\n    Matrix d_B;\n    d_B.width = B.width; d_B.height = B.height;\n    size = B.width * B.height * sizeof(float);\n    cudaMalloc(&d_B.elements, size);\n    cudaMemcpy(d_B.elements, B.elements, size,\n               cudaMemcpyHostToDevice);\n\n    // Allocate C in device memory\n    Matrix d_C;\n    d_C.width = C.width; d_C.height = C.height;\n    size = C.width * C.height * sizeof(float);\n    cudaMalloc(&d_C.elements, size);\n\n    // Invoke kernel\n    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);\n\n    // Read C from device memory\n    cudaMemcpy(C.elements, d_C.elements, size,\n               cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A.elements);\n    cudaFree(d_B.elements);\n    cudaFree(d_C.elements);\n}\n\n// Matrix multiplication kernel called by MatMul()\n__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n{\n    // Each thread computes one element of C\n    // by accumulating results into Cvalue\n    float Cvalue = 0;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int e = 0; e &lt; A.width; ++e)\n        Cvalue += A.elements[row * A.width + e]\n                * B.elements[e * B.width + col];\n    C.elements[row * C.width + col] = Cvalue;\n}\n\n\n\n\n\n\n\n그림 1: 공유메모리를 사용하지 않는 행렬곱\n\n\n\n\n다음 코드 샘플은 공유 메모리를 활용하는 행렬 곱셈의 구현입니다. 이 구현에서 각 스레드 블록은 C의 하나의 정사각 부분 행렬 Csub를 계산할 책임이 있고 블록 내의 각 스레드는 Csub의 하나의 요소를 계산할 책임이 있습니다. 그림 9에서 볼 수 있듯이 Csub는 두 개의 직사각형 행렬의 곱과 같습니다. 즉, Csub와 같은 행 인덱스를 갖는 (A.width, block_size) 차원의 A의 부분 행렬과 Csub와 같은 열 인덱스를 갖는 (block_size, A.width) 차원의 B의 부분 행렬입니다. 장치의 리소스에 맞추기 위해 이 두 직사각형 행렬은 필요한 만큼의 block_size 차원의 정사각 행렬로 나뉘고 Csub는 이러한 정사각 행렬의 곱의 합으로 계산됩니다. 이러한 각 곱은 먼저 하나의 스레드가 각 행렬의 한 요소를 로드하여 두 개의 해당 정사각 행렬을 전역 메모리에서 공유 메모리로 로드한 다음 각 스레드가 곱의 한 요소를 계산하여 수행됩니다. 각 스레드는 이들 각각의 곱의 결과를 레지스터에 누적하고, 완료되면 결과를 전역 메모리에 씁니다.\n// Matrices are stored in row-major order:\n// M(row, col) = *(M.elements + row * M.stride + col)\ntypedef struct {\n    int width;\n    int height;\n    int stride;\n    float* elements;\n} Matrix;\n// Get a matrix element\n__device__ float GetElement(const Matrix A, int row, int col)\n{\n    return A.elements[row * A.stride + col];\n}\n// Set a matrix element\n__device__ void SetElement(Matrix A, int row, int col,\n                           float value)\n{\n    A.elements[row * A.stride + col] = value;\n}\n// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is\n// located col sub-matrices to the right and row sub-matrices down\n// from the upper-left corner of A\n __device__ Matrix GetSubMatrix(Matrix A, int row, int col)\n{\n    Matrix Asub;\n    Asub.width    = BLOCK_SIZE;\n    Asub.height   = BLOCK_SIZE;\n    Asub.stride   = A.stride;\n    Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row\n                                         + BLOCK_SIZE * col];\n    return Asub;\n}\n// Thread block size\n#define BLOCK_SIZE 16\n// Forward declaration of the matrix multiplication kernel\n__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);\n// Matrix multiplication - Host code\n// Matrix dimensions are assumed to be multiples of BLOCK_SIZE\nvoid MatMul(const Matrix A, const Matrix B, Matrix C)\n{\n    // Load A and B to device memory\n    Matrix d_A;\n    d_A.width = d_A.stride = A.width; d_A.height = A.height;\n    size_t size = A.width * A.height * sizeof(float);\n    cudaMalloc(&d_A.elements, size);\n    cudaMemcpy(d_A.elements, A.elements, size,\n               cudaMemcpyHostToDevice);\n    Matrix d_B;\n    d_B.width = d_B.stride = B.width; d_B.height = B.height;\n    size = B.width * B.height * sizeof(float);\n    cudaMalloc(&d_B.elements, size);\n    cudaMemcpy(d_B.elements, B.elements, size,\n    cudaMemcpyHostToDevice);\n    // Allocate C in device memory\n    Matrix d_C;\n    d_C.width = d_C.stride = C.width; d_C.height = C.height;\n    size = C.width * C.height * sizeof(float);\n    cudaMalloc(&d_C.elements, size);\n    // Invoke kernel\n    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);\n    // Read C from device memory\n    cudaMemcpy(C.elements, d_C.elements, size,\n               cudaMemcpyDeviceToHost);\n    // Free device memory\n    cudaFree(d_A.elements);\n    cudaFree(d_B.elements);\n    cudaFree(d_C.elements);\n}\n// Matrix multiplication kernel called by MatMul()\n __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n{\n    // Block row and column\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n    // Each thread block computes one sub-matrix Csub of C\n    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);\n    // Each thread computes one element of Csub\n    // by accumulating results into Cvalue\n    float Cvalue = 0;\n    // Thread row and column within Csub\n    int row = threadIdx.y;\n    int col = threadIdx.x;\n    // Loop over all the sub-matrices of A and B that are\n    // required to compute Csub\n    // Multiply each pair of sub-matrices together\n    // and accumulate the results\n    for (int m = 0; m &lt; (A.width / BLOCK_SIZE); ++m) {\n        // Get sub-matrix Asub of A\n        Matrix Asub = GetSubMatrix(A, blockRow, m);\n        // Get sub-matrix Bsub of B\n        Matrix Bsub = GetSubMatrix(B, m, blockCol);\n        // Shared memory used to store Asub and Bsub respectively\n        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n        // Load Asub and Bsub from device memory to shared memory\n        // Each thread loads one element of each sub-matrix\n        As[row][col] = GetElement(Asub, row, col);\n        Bs[row][col] = GetElement(Bsub, row, col);\n        // Synchronize to make sure the sub-matrices are loaded\n        // before starting the computation\n        __syncthreads();\n        // Multiply Asub and Bsub together\n        for (int e = 0; e &lt; BLOCK_SIZE; ++e)\n            Cvalue += As[row][e] * Bs[e][col];\n        // Synchronize to make sure that the preceding\n        // computation is done before loading two new\n        // sub-matrices of A and B in the next iteration\n        __syncthreads();\n    }\n    // Write Csub to device memory\n    // Each thread writes one element\n    SetElement(Csub, row, col, Cvalue);\n}\n\n\n\n\n\n\n그림 2: 공유메모리를 사용않는 행렬곱\n\n\n\n\n\n\n2.5 분산 공유 메모리\ncompute capability 9.0에 도입된 스레드 블록 클러스터는 스레드 블록 클러스터의 스레드가 클러스터에 참여하는 모든 스레드 블록의 공유 메모리에 액세스할 수 있는 기능을 제공합니다. 이 분할된 공유 메모리를 분산 공유 메모리(distributed shared memory)라고 하며, 해당 주소 공간을 분산 공유 메모리 주소 공간이라고 합니다. 스레드 블록 클러스터에 속한 스레드는 주소가 로컬 스레드 블록에 속하는지 원격 스레드 블록에 속하는지에 관계없이 분산 주소 공간에서 읽거나 쓰거나 아토믹 연산을 수행할 수 있습니다. 커널이 분산 공유 메모리를 사용하든 사용하지 않든, 정적이든 동적이든 공유 메모리 크기 사양은 여전히 ​​스레드 블록당입니다. 분산 공유 메모리의 크기는 클러스터당 스레드 블록 수에 스레드 블록당 공유 메모리 크기를 곱한 값일 뿐입니다.\n분산 공유 메모리의 데이터에 액세스하려면 모든 스레드 블록이 존재해야 합니다. 사용자는 Cluster Group API의 cluster.sync() 를 사용하여 모든 스레드 블록이 실행을 시작했는지 보장할 수 있습니다. 사용자는 또한 모든 분산 공유 메모리 작업이 스레드 블록이 종료되기 전에 수행되도록 해야 합니다. 예를 들어 원격 스레드 블록이 주어진 스레드 블록의 공유 메모리를 읽으려고 하는 경우 사용자는 원격 스레드 블록이 읽은 공유 메모리가 종료되기 전에 완료되었는지 확인해야 합니다.\nCUDA는 분산 공유 메모리에 액세스하는 메커니즘을 제공하며, 애플리케이션은 이 기능을 활용하여 이점을 얻을 수 있습니다. 간단한 히스토그램 계산과 스레드 블록 클러스터를 사용하여 GPU에서 최적화하는 방법을 살펴보겠습니다. 히스토그램을 계산하는 표준적인 방법은 각 스레드 블록의 공유 메모리에서 계산을 수행한 다음 글로벌 메모리 아토믹을 수행하는 것입니다. 이 방법의 한계는 공유 메모리 용량입니다. 히스토그램 빈이 더 이상 공유 메모리에 맞지 않으면 사용자는 히스토그램을 직접 계산하고 따라서 글로벌 메모리의 아토믹을 계산해야 합니다. 분산 공유 메모리를 사용하면 CUDA는 중간 단계를 제공하며, 여기서 히스토그램 빈 크기에 따라 히스토그램을 공유 메모리, 분산 공유 메모리 또는 글로벌 메모리에서 직접 계산할 수 있습니다.\n아래의 CUDA 커널 예제는 히스토그램 빈의 수에 따라 공유 메모리 또는 분산 공유 메모리에서 히스토그램을 계산하는 방법을 보여줍니다.\n#include &lt;cooperative_groups.h&gt;\n\n// Distributed Shared memory histogram kernel\n__global__ void clusterHist_kernel(int *bins, const int nbins, const int bins_per_block, const int *__restrict__ input,\n                                   size_t array_size)\n{\n  extern __shared__ int smem[];\n  namespace cg = cooperative_groups;\n  int tid = cg::this_grid().thread_rank();\n\n  // Cluster initialization, size and calculating local bin offsets.\n  cg::cluster_group cluster = cg::this_cluster();\n  unsigned int clusterBlockRank = cluster.block_rank();\n  int cluster_size = cluster.dim_blocks().x;\n\n  for (int i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)\n  {\n    smem[i] = 0; //Initialize shared memory histogram to zeros\n  }\n\n  // cluster synchronization ensures that shared memory is initialized to zero in\n  // all thread blocks in the cluster. It also ensures that all thread blocks\n  // have started executing and they exist concurrently.\n  cluster.sync();\n\n  for (int i = tid; i &lt; array_size; i += blockDim.x * gridDim.x)\n  {\n    int ldata = input[i];\n\n    //Find the right histogram bin.\n    int binid = ldata;\n    if (ldata &lt; 0)\n      binid = 0;\n    else if (ldata &gt;= nbins)\n      binid = nbins - 1;\n\n    //Find destination block rank and offset for computing\n    //distributed shared memory histogram\n    int dst_block_rank = (int)(binid / bins_per_block);\n    int dst_offset = binid % bins_per_block;\n\n    //Pointer to target block shared memory\n    int *dst_smem = cluster.map_shared_rank(smem, dst_block_rank);\n\n    //Perform atomic update of the histogram bin\n    atomicAdd(dst_smem + dst_offset, 1);\n  }\n\n  // cluster synchronization is required to ensure all distributed shared\n  // memory operations are completed and no thread block exits while\n  // other thread blocks are still accessing distributed shared memory\n  cluster.sync();\n\n  // Perform global memory histogram, using the local distributed memory histogram\n  int *lbins = bins + cluster.block_rank() * bins_per_block;\n  for (int i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)\n  {\n    atomicAdd(&lbins[i], smem[i]);\n  }\n}\n위의 커널은 필요한 분산 공유 메모리 양에 따라 클러스터 크기로 런타임에 시작할 수 있습니다. 히스토그램이 한 블록의 공유 메모리에 맞을 만큼 작으면 사용자는 클러스터 크기 1로 커널을 시작할 수 있습니다. 아래 코드 조각은 공유 메모리 요구 사항에 따라 동적으로 클러스터 커널을 시작하는 방법을 보여줍니다.\n\n\n\n2.6 페이지 잠금 호스트 메모리\n\n\n2.7 비동기 동시 실행 (Asynchronous concurrent Execution)\n\n\n2.8 다중 디바이스 시스템\n\n디바이스 선택\n\n\n\n2.9 오류 검사\n\n\n2.10 호출 스택\n\n\n2.11 텍스쳐와 표면 메모리\n\n\n2.12 그래픽 상호 운용성",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#maximize-utilization",
    "href": "src/gpu/cuda/05_performance_guideline.html#maximize-utilization",
    "title": "5. Performance Guideline",
    "section": "2 Maximize Utilization",
    "text": "2 Maximize Utilization"
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#메모리-스루풋-극대화",
    "href": "src/gpu/cuda/05_performance_guideline.html#메모리-스루풋-극대화",
    "title": "5. Performance Guideline",
    "section": "3 메모리 스루풋 극대화",
    "text": "3 메모리 스루풋 극대화\n\n호스트와 디바이스 간의 데이터 전송\n\n\n디바이스 메모리 접근"
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#instruction-스루풋-극대화",
    "href": "src/gpu/cuda/05_performance_guideline.html#instruction-스루풋-극대화",
    "title": "5. Performance Guideline",
    "section": "4 Instruction 스루풋 극대화",
    "text": "4 Instruction 스루풋 극대화"
  },
  {
    "objectID": "src/tools/Asymptote/asymptote.html",
    "href": "src/tools/Asymptote/asymptote.html",
    "title": "Asymptote",
    "section": "",
    "text": "1 Examples\nimport graph;\nimport geometry;\nimport math;\nimport settings;\nimport fontsize;\n\n\nsettings.outformat = \"pdf\";\ndefaultpen(fontsize(17pt));\ndefaultpen(1);\n\nsize(400,300,IgnoreAspect);\n\nreal a=1, b=1.5, c=2;\nreal ya=1.0, yb=2.4, yc=3.0;\nreal yb2=2.6, yb3=2.2;\nint t=2;\n\nreal f(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\n\nreal f2(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb2;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\nreal f3(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb3;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\n\npair F(real x) {return (x,f(x));}\n\ndotfactor=7;\n\nxaxis(\"$t$\", xmin=0.5, xmax=2.5, Arrow, ticks=Ticks(DefaultFormat,\n                                        new real[] {1, 2}));\nyaxis(\"$P$\", XEquals(0.7), ymin=-0.2, ymax=4, Arrow);\n\npath g=graph(f,a,c);\npath g2=graph(f2,a,c);\npath g3=graph(f3,a,c);\n\ndraw(g,black);   \ndraw(g2,blue+dashed);  \ndraw(g3,dashed+red);  \n\nint n=2;\n\n\n\ndot(Label(\"$P_1$\",align=W), F(a));\ndot(Label(\"$P_2$\",align=E), F(c));\n\n\n\nEuler Lagrange",
    "crumbs": [
      "Tools",
      "Plotting",
      "Asymptote"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz.html",
    "href": "src/tools/tikz/tikz.html",
    "title": "tikz in Quarto",
    "section": "",
    "text": "\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw (0,0)node(a){} -- (10,0) node (b) {} ;\n    \\foreach \\x in  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} % edit here for the vertical lines\n    \\draw[shift={(\\x,0)},color=black] (0pt,3pt) -- (0pt,-3pt);\n    \\foreach \\x in {0, 0.2, 0.4, 0.6, 0.8, 1} % edit here for the numbers\n    \\draw[shift={(\\x*10,0)},color=black] (0pt,0pt) -- (0pt,-3pt) node[below]\n    {$\\x$};\n    \\node at (8, 0.5) (eq1) {$\\textcolor{red}{\\boldsymbol{SQ}}$};\n    \\node at (4, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(0)}}$}; \n    \\node at (7, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(1)}}$}; \n    \\node at (3, 0.5) (eq3) {$\\textcolor{blue}{\\boldsymbol{P}}$};\n    \\node at (0, 0.5) (eq4) {$\\textcolor{black}{\\boldsymbol{x_i}}$};\n    \\draw[decorate, decoration={brace, amplitude=6pt, mirror},] ([yshift=0.5cm]4,0.5)-- node[above=0.25cm]\n    {\\shortstack{Text}}([yshift=0.5cm]3,0.5);\n    \\draw[decorate, decoration={brace, amplitude=6pt},] ([yshift=-1cm]7,0)-- node[below=0.25cm]\n    {\\shortstack{Text}}([yshift=-1cm]3,0);\n    \\end{tikzpicture}\n\\end{document}",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz.html#기본-예제",
    "href": "src/tools/tikz/tikz.html#기본-예제",
    "title": "tikz in Quarto",
    "section": "1 기본 예제",
    "text": "1 기본 예제\n\n직선\n다음은 교차하는 직선과 교점을 그린 것이다.\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[scale=1.0]\n    \\draw[gray] (-1,2) -- (2,-4);\n    \\draw[red, thick] (-1,-1) -- (2,2);\n    \\filldraw[black] (0,0) circle (2pt) node[anchor=west]{Intersection point};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 2: Tikz figure-2\n\n\n\n\n\\draw[gray](-1, 2) -- (2, -4); 는 \\((-1, 2)\\) 에서 \\((2, -4)\\) 까지 회색(gray) 의 직선을 그으라는 명령어이다.\n\\draw[red, thick] (-1,-1) -- (2,2); 는 주어진 좌표간의 빨갛고(red), 두꺼운(thick) 직선을 그으라는 의미이다.\n\\filldraw[black] (0,0) circle (2pt) node[anchor=west]{Intersection point}; 는 \\((0, 0)\\) 에 반지름 2 인 속이 꽉 찬 원을 그리며, \\((0, 0)\\) 을 서쪽으로 두는 문자열 Intersection point 를 출력하라는 의미이다.\n\n\n\n\n그리드와 직선의 두께\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw[gray, very thick] (0,0) grid (4,3);\n    \\draw[blue, dashed] (0,0) -- (1,2) -- (3,3) -- (4,2);\n    \\draw[red, -&gt;, thick] (2,0) -- (3.5,2.5);\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 3: Tikz figure-3\n\n\n\n\n\\draw[gray, very thick] (0,0) grid (4,3); 는 \\((0, 0)\\) 부터 \\((4, 3)\\) 까지 가로, 세로 1 간격의 그리드를 회색(gray) 의 두꺼운 선(very thick) 으로 그린다.\ndraw[blue, dashed] (0,0) -- (1,2) -- (3,3) -- (4,2); 는 이어지는 파선(dashed) 을 파란 색으로 그린다.\n\\draw[red, -&gt;, thick] (2,0) -- (3.5,2.5); 는 끝점에 화살표가 있는 빨간 선을 두껍게(thick) 그린다.\n\n\n\n\n닫힌 선분\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw[green, thick] (0,0) -- (1,2) -- (3,3) -- (3,-1)--cycle;\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 4: Tikz figure-4\n\n\n\n\n\\draw[green, thick] (0,0) -- (1,2) -- (3,3) -- (3,-1)--cycle; 에서 --cycle 은 이 앞의 (3, -1) 을 맨 앞의 (0, 0) 과 연결시키라는 의미이다.\n\n\n\n\n곡선\n다음은 곡선을 그린 것이다.\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[scale=1.0]\n    \\draw (-2,0) -- (2,0);\n    \\filldraw [blue] (0,0) circle (2pt);\n    \\draw[cyan] (-2,-2) .. controls (0,0) .. (2,-2);\n    \\draw[magenta] (-2,2) .. controls (-1,0) and (1,0) .. (2,2);\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 5: Tikz figure-5\n\n\n\n\n\\draw[magenta] (-2,2) .. controls (-1,0) and (1,0) .. (2,2); 는 \\((-2, 2)\\) 와 \\((2, 2)\\) 를 끝점으로 하고 \\((-1, 0)\\) 과 \\((1, 0)\\) 을 제어점으로 하는 사차 베지어 곡선을 magenta 색으로 그린다.\n\\draw[cyan] (-2,-2) .. controls (0,0) .. (2,-2); 는 (\\(-2, 2)\\) 와 \\((2, 2)\\) 를 끝점으로 하고 제어점이 \\((0, 0)\\) 인 사차 베지어 곡선을 cyan 색으로 그린다. 즉 \\draw[magenta] (-2,2) .. controls (0,0) and (0,0) .. (2,2); 과 같다.\n\n\n\n\n함수\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[domain=0:4]\n    \\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);\n  \n    \\draw[-&gt;] (-0.2,0) -- (4.2,0) node[right] {$x$};\n    \\draw[-&gt;] (0,-1.2) -- (0,4.2) node[above] {$f(x)$};\n  \n    \\draw[color=red]    plot (\\x,\\x)             node[right] {$f(x) =x$};\n    % \\x r means to convert '\\x' from degrees to _r_adians:\n    \\draw[color=blue]   plot (\\x,{sin(\\x r)})    node[right] {$f(x) = \\sin x$};\n    \\draw[color=orange] plot (\\x,{0.05*exp(\\x)}) node[right] {$f(x) = \\frac{1}{20} e^x$};\n  \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 6: Tikz figure-6\n\n\n\n\n\\begin{tikzpicture}[domain=0:4] 에서 [domain=0:4] 은 함수를 사용할 때 \\([0, 4]\\) 구간을 사용한다는 의미이다.\n\\draw[color=red] plot (\\x,\\x) node[right] {$f(x) =x$}; 팔간색 선으로 \\(y=x\\) 그래프를 그리며 끝점의 오른쪽에 \\(f(x)=x\\) 라는 문자열을 넣으라는 의미.\n\n\n\n\n도형\n\nShade\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[scale = 3, rounded corners,ultra thick]\n    \\shade[top color=yellow,bottom color=black] (0,0) rectangle +(2,1);\n    \\shade[left color=yellow,right color=black] (3,0) rectangle +(2,1);\n    \\shadedraw[inner color=yellow,outer color=black,draw=yellow] (6,0) rectangle +(2,1);\n    \\shade[ball color=green] (9,.5) circle (.5cm);\n  \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 7: tikz 예시 7\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}[scale=3]\n    \\clip (-0.1,-0.2) rectangle (1.1,0.75);\n    \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);\n    \\draw (-1.5,0) -- (1.5,0);\n    \\draw (0,-1.5) -- (0,1.5);\n    \\draw (0,0) circle (1cm);\n    \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc\n    (0:30:3mm) -- cycle;\n    \\draw[red,very thick]  (30:1cm) -- +(0,-0.5);\n    \\draw[blue,very thick] (30:1cm) ++(0,-0.5) -- (0,0);\n  \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 8: tikz 예시 8\n\n\n\n\n\n\n\n좌표계\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw [red,-&gt;](0,0) -- (xyz cs:x=2) node[right, black] {$x$};\n    \\draw [green,-&gt;](0,0) -- (xyz cs:y=2) node[right, black] {$y$};\n    \\draw [blue,-&gt;](0,0) -- (xyz cs:z=2) node[right, black] {$z$};\n  \n    \\draw [red, thick, -&gt;] (0, 0) -- (xyz cs:x=2, y=2, z=1);\n    \\draw [red, dashed] (xyz cs:x=2, y=2, z=1) -- (xyz cs:x=0, y=2, z=1);\n  \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 9: tikz 예시 9\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\n\\usetikzlibrary {3d} \n\\usetikzlibrary {arrows}\n\\usetikzlibrary{shapes.geometric}\n\\begin{document}\n\\begin{tikzpicture} %[=&gt;stealth]\n    \\tikzset{\n        partial ellipse/.style args={#1:#2:#3}{\n            insert path={+ (#1:#3) arc (#1:#2:#3)}\n        }\n    }\n      \\draw [-&gt;] (0,0) -- (xyz cs:x=3);\n      \\draw [-&gt;] (0,0) -- (xyz cs:y=4.5);\n      \\draw [-&gt;] (0,0) -- (xyz cs:z=3); \n      \\draw [dashed] (0.0,3.0) ellipse (1.5 and 0.5);\n      \\draw [thick, -{stealth}] (0, 0) -- (xyz cs:x=0.7,y=2.55);\n      \\node[below, scale=.6] at (0.3, 2.3) {$\\boldsymbol{r}_i (q_j)$};\n      \\draw [thick, -{stealth}] (0, 0) --(xyz cs:x=1.4,y=2.8);\n      \\node[scale=.6] at (1.5, 1.7) {$\\boldsymbol{r}_i (q_j+dq_j)$};\n      \\draw [red, thick, -{stealth}] (0.7, 2.55) -- (1.4, 2.8);\n      \\node[red, scale=.6] at (1.0, 2.5) {$d\\boldsymbol{r}_i$};\n      \\draw [thick, -{stealth}] (0, 3) -- (0, 3.7) node[right, scale=0.6] {$\\;\\boldsymbol{n}$};\n    \n      \\draw [teal] (0, 3) -- (0.7, 2.55);\n      \\draw [teal] (0, 3) -- (1.4, 2.8);\n      \\draw[teal, -&gt;] (0, 3.0) [partial ellipse=297:340:0.6 and 0.2] node[above, scale=0.6] {$dq_j$} ;\n      \\draw[purple, -&gt;] (0, 0) [partial ellipse=90:75:1 and 1] node[above left, scale=0.6] {$\\theta$} ;\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 10: tikz 예시 10\n\n\n\n\n\n\n신호처리 함수\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}[ &gt;=stealth]\n\n\n    \\draw[-&gt;] (-2, 0) -- (2, 0) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (0, -0.5) -- (0, 1.5);\n    \\draw[thick] (-2, 0) -- (0, 0) node[below left, black] {$0$} -- (0, 1) node[left, black] {$1$}-- (2, 1) node[above left, black] {$\\text{step}(x)$};\n    \n    \\draw[-&gt;] (4, 0) -- (8, 0) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (6, -0.5) -- (6, 1.5);\n    \\draw[thick] (4, 0) -- (5.5, 0) node[below, black] {$-\\frac{1}{2}$} -- (5.5, 1) -- (6, 1)  -- (6.5, 1)-- (6.5, 0)  node[below, black] {$\\frac{1}{2}$} -- (8, 0) node[above left, black] {$\\text{rect}(x)$};\n    \n    \\draw[-&gt;] (-2, -3) -- (2, -3) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (0, -3.5) -- (0, -1.5);\n    \\draw[thick] (-2, -3) -- (-0.5, -3) node[below, black] {$-\\frac{1}{2}$} -- (0, -2) -- (0.5, -3) node[below, black] {$-\\frac{1}{2}$} -- (2, -3) node[above left, black] {$\\text{tri}(x)$};\n    \n    \n    \\draw[-&gt;] (4, -3) node[above right, black] {$\\text{sinc}(x)$} -- (8, -3) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (6, -3.5) -- (6, -1.5);\n    \\draw[domain=4.0:8.0, smooth, variable=\\x, thick] plot ({\\x}, {0.116*sin(500* (\\x-6))/((\\x-6)) -3 });\n    \\draw (6.36, -2.9) -- (6.36, -3.1) node[below, black] {$\\pi$} ; \n    \n    \n    \\draw[-&gt;] (-2, -6) -- (2, -6) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (0, -6.5) -- (0, -4);\n    \\draw[domain=-1.8:1.8, smooth, variable=\\x, thick] plot ({\\x}, {exp(-(\\x)^2) - 6 });\n    \\node[black, right] at (0, -4.7) {$\\text{gauss}(x)$};\n    %\\draw (6.36, -2.9) -- (6.36, -3.1) node[below, black] {$\\pi$} ; \n    \n    \n    \\draw[-&gt;] (4, -6)  -- (8, -6) node[below, black] {$x$} ;\n    \\draw[ -&gt;] (6, -6.5) -- (6, -4);\n    \\draw[very thick] (4.2, -6) -- (6, -6) -- (6, -4) node[left, black] {$\\infty$}-- (6, -6)  -- (7.8, -6) node[above left, black] {$\\delta (x)$};\n    \n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 11: tikz 예시 11 - 신호 함수",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz.html#이미지",
    "href": "src/tools/tikz/tikz.html#이미지",
    "title": "tikz in Quarto",
    "section": "2 이미지",
    "text": "2 이미지\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw [black, very thick, -&gt;](0, 0) -- (2, 2)  node[right] {$\\boldsymbol{v}$};\n    \\draw [red, very thick, -&gt;] (0, 0) -- (-2.646, 1 ) node[left] {$\\boldsymbol{H_v x}$};\n    \\draw [blue, very thick, -&gt;] (0, 0) -- (-0.646, 3 ) node[left] {$\\boldsymbol{x}$};\n    \\draw [green, dashed, thick] (1, -1) -- (-2, 2 ) node[left, black] {Plane normal to $\\boldsymbol{v}$};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 12: tikz 예시 12-Householder plane\n\n\n\n\n\\documentclass[border=3pt,tikz]{standalone}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\\begin{document}\n\\begin{tikzpicture}\n    \\draw[step=1.0,black,thick] (-3,-3) grid (3,3);\n    \\node at (-2.5, 2.5) {$x_{1}$};\n    \\node at (-2.5, 1.5) {$x_{2}$};\n    \\node at (-2.5, 0.5) {$x_{3}$};\n    \\node at (-2.5, -0.5) {$\\vdots$};\n    \\node at (-1.5, 2.5) {$\\cdots$};\n    \\node at (2.5, -2.5) {$x_N$};\n    \\node at (5.5, 2.5) {Detector};\n    \\draw[red, thick, -{stealth}] (-4, -2) -- (5.5, 1.5);\n    \n    \\begin{scope}[shift={(5.7,1.55)},rotate=20.4]\n    \\draw[blue, very thick] (-0.5,-0.5) -- (0.5,-0.5) -- (0.5,0.5) -- (-0.5,0.5); \n    \\end{scope}\n    \n    \\begin{scope}[shift={(0.3, 0.2)},rotate=20.4]\n    \\draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=4ex}]\n      (-0.5,0) -- (0.5,0) node[midway,yshift=-3em, red]{$w_{ij}$};\n    \\end{scope}\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 13: tikz 예시 13-검출기",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_pde.html",
    "href": "src/tools/tikz/tikz_pde.html",
    "title": "tikz in PDE",
    "section": "",
    "text": "그리드와 직선의 두께\n\\documentclass[border=3pt,tikz]{standalone}\n\\usepackage{amsmath}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\begin{document}\n\\begin{tikzpicture}\n\n\n    \\draw[-{stealth}] (-1., 0) -- (8,0) node[right] {$x$};\n    \\draw[-{stealth}] (0,-1) -- (0,6) node[above] {$y$};\n    \n    \\foreach \\x in {2,3,4,5,7} {\n      \\draw[] (\\x, 1) -- (\\x, 5); \n      \\draw[] (\\x, 0.1) -- (\\x, -0.1);\n    }\n    \\foreach \\y in {1,2,3,5} {\n      \\draw[] (2, \\y) -- (7, \\y);\n      \\draw[] (0.1, \\y) -- (-0.1, \\y);\n    }\n    \n    \\node[below, scale=0.8] at (2, -0.1) {$x_0=a$};\n    \\node[below, scale=0.8] at (3, -0.1) {$x_1$};\n    \\node[below, scale=0.8] at (4, -0.1) {$x_2$};\n    \\node[below, scale=0.8] at (5, -0.1) {$x_3$};\n    \\node[below, scale=0.8] at (7, -0.1) {$x_N=b$};\n    \n    \\node[left, scale=0.8] at (-0.1, 1) {$y_0=c$};\n    \\node[left, scale=0.8] at (-0.1, 2) {$y_1$};\n    \\node[left, scale=0.8] at (-0.1, 3) {$y_2$};\n    \\node[left, scale=0.8] at (-0.1, 5) {$y_M=d$};\n    \n    \\node[] at (6, 1.5) {$\\cdots$};\n    \\node[] at (6, 2.5) {$\\cdots$};\n    \\node[] at (2.5, 4) {$\\vdots$};\n    \\node[] at (3.5, 4) {$\\vdots$};\n    \\node[] at (4.5, 4) {$\\vdots$};\n    \\node[] at (5.5, 4) {$\\vdots$};\n    \\node[] at (6.5, 4) {$\\vdots$};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n그림 1: PDE grid",
    "crumbs": [
      "Tools",
      "Plotting",
      "tikz in PDE"
    ]
  },
  {
    "objectID": "src/topics/endians.html#sec-topics_endian",
    "href": "src/topics/endians.html#sec-topics_endian",
    "title": "엔디언과 바이트",
    "section": "1 엔디언",
    "text": "1 엔디언\n컴퓨터의 모든 데이터와 명령어들은 내부적으로 모두 2진수로 처리되며 각 2진 처리 단위를 비트(bit) 라고 하며, 명령어와 데이터의 단위는 8개의 비트가 모인 바이트(Byte) 이다. 즉 1 Byte = 8 bits 이다. 1 Byte 는 28=256 개의 서로 다른 정보를 표현 할 수 있다.\nC/C++ 의 char 나 unsigned char Julia 의 UInt8, Int8, 그리고 Python 의 numpy 의 dtype 인 np.uint8 이나 np.int8 같은 경우는 8 비트 그러니까 1 바이트 단위로 데이터를 저장한다. 그러나 이 타입은 많아야 256 개의 서로 다른 것 이상을 표현 할 수 없기 때문에 여러 바이트의 자료형이 필요하다. 예를 들어 C/C++ 의 부동소수 타입인 float 같은 경우는 CPU 에 따라 다르지만 4 바이트 혹은 8 바이트 의 자료형이며 마찬가지로 Julia 의 Float32 나 numpy 의 np.float32 같은 경우는 4 바이트, 그러니까 32 비트 데이터 타입이다.\n엔디언은 여러 바이트로 이루어진 데이터를 어떤 순서로 나열하느냐에 대한 것이다. 컴퓨터의 역사에서 두가지 방법이 있었다. 예를 들어 2 바이트 부호 없는 정수 타입의 24577 을 이진수로 16자리까지 표현하면 01100000 00000001 이다. 바이트를 구분하기 위해 중간에 공백을 두었다. 컴퓨터 메모리상에 이렇게 저장하는 방식을 리틀 엔디언(little endian) 이라고 한다. 바이트 내의 비트 순서는 그대로 두면서 바이트 순서만 바뀌는 것, 즉 2바이트 부호 없는 정수 타입의 24577 을 00000001 01100000 로 표현하는 방식을 빅 엔디언(big endian) 이라고 한다. 이것은 2바이트 보다 큰 바이트에도 그대로 적용되는데 리틀 엔디언이 우리가 일반적으로 쓰는 2진수 표현과 동등하다면 빅 엔디언은 역시 바이트 순서가 역전된 방식 (바이트 내의 비트 순서는 리틀 엔디언과 동일하다) 이다.\n이게 단순히 역사적 문제이면 좋겠는데, 어떤 컴퓨터는 내부적으로 빅엔디언을, 어떤 컴퓨터는 리틀엔디언을 사용하며 이 두 컴퓨터가 통신할 때 각자의 엔디언에 맞추어 변환하지 않는다면 당연히 오류가 발생한다. 네트워크 전송에는 보통 빅 엔디언을 사용하여 데이터를 전송하지만 항상 그런것도 아니다.\n\n예를 들어 필자의 컴퓨터는 리틀 엔디언을 사용하지만 데이터를 네트워크 전송할 때는 빅 엔디언으로 전송하는 것이 좋다. 특별한 약속이 없다면 받는 쪽도 당연히 빅 엔디언으로 받는 것으로 알 것이기 때문이다.\n한 데이터 내의 바이트 배치만 거꾸로 되는 것이지 데이터 자체의 순서가 바뀌는 것은 아니다. 예를 들어 1바이트 문자열 ABCD 는 ABCD 로 보내면 된다.\n\n\n워낙 오래되고, 중요한 사항이기 때문에 각 컴파일러, 인터프리터 들은 엔디언 변환 함수를 제공한다.",
    "crumbs": [
      "주제별",
      "엔디언과 바이트"
    ]
  },
  {
    "objectID": "src/topics/endians.html#sec-topics_byte_data",
    "href": "src/topics/endians.html#sec-topics_byte_data",
    "title": "엔디언과 바이트",
    "section": "2 바이트 데이터",
    "text": "2 바이트 데이터\nC/C++, Julia, Python(아마도 numpy) 는 1 바이트, 2바이트, 4바이트, 8바이트, 16 바이트의 정수형 데이터 타입을 제공한다. 그런데 3 바이트나 6바이트 데이터를 다뤄야 할 경우가 존재한다. 보통 대량의 데이터를 고속으로 전송하고 처리해야 하는데, 데이터가 표현해야 하는 값의 개수가 216 = 65,536 보다는 크고 232=4,294,967,296 보다는 작을 경우 2바이트는 부족하고 4바이트는 남는다.",
    "crumbs": [
      "주제별",
      "엔디언과 바이트"
    ]
  },
  {
    "objectID": "src/ML/pytorch/tensor.html",
    "href": "src/ML/pytorch/tensor.html",
    "title": "텐서",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]\n여기서는 pytorch 의 tensor 에 대해 좀 더 깊이 알아보도록 하자.",
    "crumbs": [
      "AI & ML",
      "PyTorch",
      "텐서"
    ]
  },
  {
    "objectID": "src/ML/pytorch/tensor.html#텐서에-대해-알아야-할-것들",
    "href": "src/ML/pytorch/tensor.html#텐서에-대해-알아야-할-것들",
    "title": "텐서",
    "section": "1 텐서에 대해 알아야 할 것들",
    "text": "1 텐서에 대해 알아야 할 것들\n\n1.1 View 와 tensor.contiguous()\n\n참고자료 : tensor view\n\n예를 들어 텐서의 축 순서를 바꾸는 tensor.transpose() 는 원래의 텐서의 데이터로부터 새로운 데이터를 생성하지 않고 원본 텐서에 데이터를 참조하면서 그것이 보여지거나 연산되는 방식을 바꾼다. 이렇게 원본 텐서를 참조하면서 모양, 기능, 출력 등만 바뀌는 것을 View 라고 한다. 한 텐서가 다른 텐서의 View 라면 둘 중 하나의 성분을 바꾸면 나머지 하나도 당연히 같이 바뀌게 된다.\n이렇게 텐서의 내용을 복사 변경하지 않고 View 를 지원하는 메서드는 tensor.transpose(), tensor.expand(), tensor.diagonal() 등 다수가 있다. 기본적으로 tensor[1:-1:2, 2:] 와 같은 슬라이싱도 View 이다.\ntensor.is_contiguous() 메서드는 텐서가 View 인지 아닌지를 반환하며 tensor.contiguous() 메서드는 원본 텐서의 데이터로부터 원하는 모양과 기능을 가진 새로운 데이터셋을 가진 텐서를 만든다.\n\n\nIn\n\nt0 = torch.tensor([1,2,3,4,5.0])\nt1 = t0[1:-1:2]\nprint(t1.is_contiguous())\nt1=t1.contiguous() \nprint(t1.is_contiguous())\n\n\n\nOut\n\nFalse\nTrue",
    "crumbs": [
      "AI & ML",
      "PyTorch",
      "텐서"
    ]
  },
  {
    "objectID": "src/ML/theory/classification.html",
    "href": "src/ML/theory/classification.html",
    "title": "퍼셉트론과 분류",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "퍼셉트론과 분류"
    ]
  },
  {
    "objectID": "src/ML/theory/classification.html#분류-문제",
    "href": "src/ML/theory/classification.html#분류-문제",
    "title": "퍼셉트론과 분류",
    "section": "1 분류 문제",
    "text": "1 분류 문제\n\n1.1 개요\n분류(classification) 문제는 주어진 데이터에 대한 output 유한개인 경우이다. 여기서 가능한 ouput 을 클래스라고 하자. 대부분의 문제에서 각 클래스는 서로 겹치지 않으며, 여기서도 겹치지 않는다고 간주한다. 따라서 입력 벡터의 공간은 각각의 클래스에 따라 분리되며, 각 클래스를 분리하는 경계를 결정 경계(decision boundary) 혹은 결정 표면(decision surface) 이라고 하며 결정 경계를 바탕으로 분리된 부분집합을 결정 구역(decision region) 이라고 한다.\n입력 벡터가 \\(D\\) 차원 공간이라고 하자. 결정 표면을 \\(D\\) 차원 공간에 대한 \\(D-1\\) 차원 초평면 으로 분리하는 모델을 선형 모델(linear model) 이라고 하며, 데이터들이 다수의 초평면으로 정확하게 각각의 클래스로 분류될 수 있을 때, 이 데이터의 집합을 선형 분리 가능 집합(linearly seperable set) 이라고 한다.\n가능한 output 이 \\(K\\) 개의 클래스라고 하자. 이 \\(K\\) 개의 클래스를 \\(\\mathcal{M}_n(\\mathbb{R})\\) 의 표준 기저 벡터로 표현하는 것을 원 핫 인코딩(one hot encoding) 이라고 한다. 예를 들어 다수의 과일 이미지를 사과, 배, 딸기로 분류한다다면 이 이미지들은 3개의 클래스로 분류된다는 의미이다. 사과 클래스는 \\(\\boldsymbol{e}_1=\\begin{bmatrix} 1 & 0 & 0\\end{bmatrix}^T\\) 로 표현하고, 배, 딸기는 각각 \\(\\boldsymbol{e}_2,\\,\\boldsymbol{e}_3\\) 로 표현될 수 있다.\n\n\n\n1.2 일반화된 선형 모델\n입력값 \\(\\boldsymbol{x}\\) 에 대한 모델을 구성할 때 모댈 내부의 매개변수 \\(\\boldsymbol{w}\\) 에 대한 가장 간단한 함수로서\n\\[\ny(\\boldsymbol{x}; \\boldsymbol{w},\\,w_0) = f(\\boldsymbol{w}^T \\boldsymbol{x}+w_0)\n\\tag{1}\\]\n를 생각 할 수 있다. 이 때 보통 \\(f(s)\\) 는 비선형 함수이며 활성화 함수(activation fucntion) 이라고 불린다. 또한 식 1 로 기술되는 모델을 일반화된 선형 모델(generalized linear model) 이라고 한다.\n일반화된 선형 모델의 경우 결정 표면은 어떤 상수 \\(c\\) 에 대해 \\(\\boldsymbol{w}^T\\boldsymbol{x} + w_0 = c\\) 인 초평면이 된다. 즉 선형 분리 가능 집합의 경우 일반화된 선형 모델로 잘 설명이 된다.\n\n\n\n1.3 선형 판별\n입력벡터를 어느 클래스로 분류할지 판단하는 함수를 판별함수라고 하고 판별함수에 의한 결정표면이 초평면 일 경우 선형 판별(linear determination) 이라고 한다,.\n\n2 개의 클래스 의 경우\n두개의 클라스 \\(C_1,\\,C_2\\) 로 분류하는 문제를 살펴 보자. 식 1 에서의 활성화 함수 \\(f\\) 를 \\(\\text{sign}(a)\\) 함수 즉,\n\\[\n\\text{sign}(a) = \\left\\{\\begin{array}{ll} 1, \\qquad & a\\ge 0 \\\\ -1 & a&lt;0 \\end{array} \\right.\n\\]\n로 정한다. 즉 \\(y(\\boldsymbol{x};\\boldsymbol{w},\\,w_0)\\) 값이 \\(1\\) 이면 \\(C_1\\), \\(-1\\) 이면 \\(C_2\\) 클래스에 포함되도 하는 매개변수를 찾는 문제가 된다.\n\n다중 클래스 의 경우\n2 개 이상 \\(K\\) 개의 클래스 \\(C_1,\\ldots,\\,C_K\\) 로 분류하는 문제의 경우는 매우 복잡해진다. 예를 들어 각 클래스 \\(C_1,\\ldots,\\,C_N\\) 에 대한 활성화 함수 \\(f_1,\\,\\ldots,\\,f_N\\) 을 정하더라도 겹치거나, 어디에도 포함되지 않는 모호한 영역이 생길 수 있다. 이런 경우를 처리할 수 있는 한가지 방법으로 \\(K\\) 개의 선형 판별 함수 \\(y_1,\\ldots,\\,y_K\\) 가 아래와 같이 정의된다고 하자.\n\\[\ny_k(\\boldsymbol{x}; \\boldsymbol{w}_k,\\, w_{k0}) = \\boldsymbol{w}_k^T \\boldsymbol{x} + w_{k0},\\qquad k=1,\\ldots,\\, K.\n\\]\n이 때 \\(y_{k}(\\boldsymbol{x}; \\boldsymbol{w}_k,\\, w_{k0}) \\ge y_{j}(\\boldsymbol{x}; \\boldsymbol{w}_j,\\, w_{j0})\\) 이면 \\(C_k\\) 클래스에 포함되도록 하면 된다.\n\n\n\n1.4 퍼셉트론\n수학적으로 \\(f:\\mathbb{R}^n \\to \\{0,\\,1\\}\\) 인 함수로 입력 벡터 \\(\\boldsymbol{x}\\in \\mathcal{M}_n(\\mathbb{R})\\) 에 대해\n\\[\nf(\\boldsymbol{w}^T\\boldsymbol{x} + b) = \\left\\{\\begin{array}{ll} 1, \\qquad & \\boldsymbol{w}^T\\boldsymbol{x} + b  \\ge 0, \\\\ -1, & \\text{otherwise}.\\end{array} \\right.\n\\]\n인 함수이다. 여기서 \\(\\boldsymbol{w}\\) 는 매개변수(parameter), \\(b\\) 를 편향(bias) 이라고 한다. 즉 퍼셉트론은 입력 벡터 \\(\\boldsymbol{x}\\) 에 대한 이진 분류 함수이다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "퍼셉트론과 분류"
    ]
  },
  {
    "objectID": "src/ML/theory/classification.html#support-vector-machine",
    "href": "src/ML/theory/classification.html#support-vector-machine",
    "title": "퍼셉트론과 분류",
    "section": "2 Support Vector Machine",
    "text": "2 Support Vector Machine",
    "crumbs": [
      "AI & ML",
      "이론",
      "퍼셉트론과 분류"
    ]
  },
  {
    "objectID": "src/ML/theory/regression.html",
    "href": "src/ML/theory/regression.html",
    "title": "선형 회귀",
    "section": "",
    "text": "% %\n%\n\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]",
    "crumbs": [
      "AI & ML",
      "이론",
      "선형 회귀"
    ]
  },
  {
    "objectID": "src/ML/theory/regression.html#선형회귀",
    "href": "src/ML/theory/regression.html#선형회귀",
    "title": "선형 회귀",
    "section": "1 선형회귀",
    "text": "1 선형회귀\n\n1.1 선형회귀와 설계행렬\n\\(n\\) 차원 벡터 입력변수 \\(\\boldsymbol{x}_1,\\ldots,\\,\\boldsymbol{x}_m \\in \\mathbb{R}^n\\) 와 입력 변수 각각에 대한 레이블 \\(y_1,\\,\\ldots,\\,y_m\\) 을 가장 잘 기술하는 함수\n\\[\nf(\\boldsymbol{x};\\boldsymbol{\\theta})= \\begin{bmatrix} \\theta_1 & \\cdots & \\theta_n\\end{bmatrix}\\boldsymbol{x} + \\theta_0 = \\begin{bmatrix}\\theta_0 & \\theta_1 & \\cdots & \\theta_m\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\boldsymbol{x}\\end{bmatrix}\n\\]\n를 찾는 문제를 선형 회귀 (linear regression) 문제라고 한다.\n\n여기서 주의할 것은 \\(\\{\\boldsymbol{x}_i\\}\\) 에 대해 선형이기 때문에 선형 회귀가 아니라 \\(\\boldsymbol{\\theta}=  \\begin{bmatrix}\\theta_0 & \\theta_1 & \\cdots & \\theta_m\\end{bmatrix}\\) 에 대한 선형 함수이므로 선형 회귀이다.\n\n이제 \\(\\boldsymbol{x}_i \\in \\mathbb{R}^n\\) 의 \\(j\\) 번째 성분을 \\((\\boldsymbol{x}_i)_j\\) 라고 표기하기로 하고 아래와 같이 행렬과 벡터를 정의한다.\n\\[\n\\boldsymbol{\\Phi}:= \\begin{bmatrix} 1 & (\\boldsymbol{x}_1)_1 & \\cdots & (\\boldsymbol{x}_1)_n \\\\ 1 & (\\boldsymbol{x}_2)_1 & \\cdots & (\\boldsymbol{x}_2)_n \\\\ \\vdots & & & \\vdots \\\\ 1 & (\\boldsymbol{x}_m)_1 & \\cdots & (\\boldsymbol{x}_m)_n  \\end{bmatrix}, \\qquad \\boldsymbol{\\theta} := \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix},\\qquad \\boldsymbol{y} := \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n\\]\n이 때 \\(\\boldsymbol{\\Phi}\\) 를 설계 행렬(design matrix) 이라고 한다. 그리고\n\\[\n\\boldsymbol{e}:=\\boldsymbol{y}-\\boldsymbol{\\Phi \\theta}\n\\]\n라고 하면 \\(\\boldsymbol{e}\\in \\mathbb{R}^m\\) 의 \\(i\\) 번째 성분 \\(e_i = y_i - f(\\boldsymbol{x}_i;\\boldsymbol{\\theta})\\) 이다. 결국 선형 회귀 문제는 벡터 \\(\\boldsymbol{e}\\) 의 크기, 즉 노름을 최소화 하는 \\(\\boldsymbol{\\theta}^\\ast\\) 를 찾는 문제로 바뀌며 다음과 같이 기술 될 수 있다.\n\\[\n\\boldsymbol{\\theta}^\\ast = \\argmin_\\boldsymbol{\\theta} \\left\\|\\boldsymbol{y}- \\boldsymbol{\\Phi \\theta}\\right\\|\n\\tag{1}\\]\n노름 가운데 유클리드 노름, 즉 \\(L_2\\) 노름 혹은 \\(L_1\\) 노름을 사용한다. \\(L_p\\) 노름을 사용했을 경우 식 1 는 다음과 같이 표기된다.\n\\[\n\\boldsymbol{\\theta}^\\ast = \\argmin_\\boldsymbol{\\theta} \\left\\|\\boldsymbol{y}- \\boldsymbol{\\Phi \\theta}\\right\\|_p\n\\tag{2}\\]\n벡터 \\(\\boldsymbol{v}\\in \\mathbb{R}^k\\) 의 \\(L_p\\) 노름 \\(\\|\\boldsymbol{v}\\|_p\\) 는 다음과 같이 정의된다.\n\\[\n\\|\\boldsymbol{v}\\|_p := \\left(\\sum_{i=1}^k |v_i|^p\\right)^{1/p}\n\\]\n즉 \\(\\|\\boldsymbol{v}\\|_1 = \\sum |v_i|\\) 이며 \\(\\|\\boldsymbol{v}\\|_2 = \\displaystyle \\sqrt{\\sum_{i=1}^k |v_i|^2}\\) 는 유클리드 노름이다. \\(L_\\infty\\) 는 \\(p\\to \\infty\\) 극한으로 정의되며 \\(\\|\\boldsymbol{v}\\|_\\infty =\\displaystyle \\max_{i=1,\\ldots,\\,k} |v_k|\\) 이다. 만약 데이터 가운데 소위 튀는 데이터가 있다면 \\(L_\\infty\\) 노름이 가장 큰 영향을 받으며 \\(\\|\\boldsymbol{v}\\|_1\\) 노름이 가장 영향을 적게 받는다. 따라서 \\(L_\\infty\\) 노름은 거의 사용되지 않는다. \\(L_2\\) 노름의 경우\n\\[\n\\boldsymbol{\\theta}^\\ast = \\argmin_\\boldsymbol{\\theta} \\|\\boldsymbol{y}-\\boldsymbol{\\Phi\\theta}\\|_2^2\n\\tag{3}\\]\n이며 \\(\\argmin_\\boldsymbol{\\theta}\\|\\boldsymbol{y}-\\boldsymbol{\\Phi\\theta}\\|_2^2\\) 가 \\(\\sqrt{\\cdots}\\) 가 없어 미분 계산이 훨씬 간단해 지기 때문에 노름의 제곱을 많이 사용한다.\n\n\n\n1.2 학습/훈련과 예측\n우리가 이미 획득한 입력 벡터의 집합 \\(\\{\\boldsymbol{x}_i\\}\\) 와 레이블(label) \\(\\{y_i = y_i(\\boldsymbol{x}_i)\\}\\) 로 이루어진 데이터 \\(\\{(\\boldsymbol{x}_i,\\, y_i):i=1,\\ldots,\\,m\\}\\), 그리고 데이터를 잘 설명할것이라고 기대되는 함수 \\(f(\\boldsymbol{x},\\,\\boldsymbol{\\theta})\\) 에 대해 식 1 의 \\(\\boldsymbol{\\theta}^\\ast\\) 를 얻는 것을 훈련 혹은 학습 이라고 한다. 이 \\(\\boldsymbol{\\theta}^\\ast\\) 에 대해 \\(\\boldsymbol{x}\\) 의 상황에서 \\(f(\\boldsymbol{x};\\boldsymbol{\\theta}^\\ast)\\) 의 값을 얻을것이라고 기대한다. 즉 우리는 주어진 데이터를 학습시켜 미지의 상황을 예측 할 수 있다. 이것은 비단 선형 회귀 뿐만 아니라 모든 통계학적 처리 및 기계학습의 과정에 공통된다.\n여기서 우리가 고려해야 할 상황이 있다. 이것은 항상 명심해야 한다.\n  Q1. 데이터는 편항되거나 오염되거나 부실하지 않은가?\n  Q2. \\(f(\\boldsymbol{x};\\boldsymbol{\\theta})\\) 의 선택은 합리적인가?\n  Q3. \\(\\boldsymbol{\\theta}^\\ast\\) 를 얻는 과정은 정확한가?\n\n\n\n1.3 최소제곱합\n식 3 은 보통 데이터와 모델값의 오차에 대한 제곱합 오차 함수 \\(E(\\boldsymbol{\\theta})\\) 이다.\n\\[\nE(\\boldsymbol{\\theta})=\\sum_{i=1}^m (y_i - f(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))^2 = \\|\\boldsymbol{y}-\\boldsymbol{\\Phi \\theta}\\|_2^2.\n\\]\n이 때 \\(E(\\boldsymbol{\\theta})\\) 가 미분가능하다면 \\(E(\\boldsymbol{\\theta})\\) 를 최소로 하는 \\(\\boldsymbol{\\theta}^\\ast\\) 에 대해 \\(\\nabla_\\boldsymbol{\\theta} E(\\boldsymbol{\\theta}^\\ast)=\\boldsymbol{0}\\) 이어야 한다.\n\\[\n\\begin{aligned}\n\\nabla_\\boldsymbol{\\theta}E(\\boldsymbol{\\theta}) &= -2\\boldsymbol{\\Phi}^T \\boldsymbol{y}-2\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\boldsymbol{\\theta}^\\ast = \\boldsymbol{0}\n\\end{aligned}\n\\]\n이로부터\n\\[\n\\boldsymbol{\\theta}^\\ast = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T \\boldsymbol{y}\n\\tag{4}\\]\n를 얻는다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "선형 회귀"
    ]
  },
  {
    "objectID": "src/ML/theory/regression.html#다양한-선형-회귀",
    "href": "src/ML/theory/regression.html#다양한-선형-회귀",
    "title": "선형 회귀",
    "section": "2 다양한 선형 회귀",
    "text": "2 다양한 선형 회귀\n\n2.1 1차원 선형 회귀\n1차원 선형회귀에서 설계행렬 \\(\\boldsymbol{\\Phi}\\) 는 다음과 같다.\n\\[\n\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\\\ 1 & x_n\\end{bmatrix}.\n\\]\n그리고\n\\[\n\\begin{aligned}\n\\boldsymbol{\\theta}=\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix},\n\\qquad \\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}, \\qquad\n\\boldsymbol{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n\\end{bmatrix}\n\\end{aligned}\n\\]\n에 대해\n\\[\n\\theta^\\ast = \\begin{bmatrix} \\theta_1^\\ast \\\\ \\theta_2^\\ast\\end{bmatrix} = \\argmin_{\\boldsymbol{\\theta}} \\|\\boldsymbol{y} - \\boldsymbol{\\Phi \\theta}\\|^2\n\\]\n이다. \\(L_2\\) 노름의 경우 \\(\\boldsymbol{\\theta}^\\ast\\) 는 식 4 로 부터 다음과 같다는 것을 안다.\n\\[\n\\boldsymbol{\\theta}^\\ast = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T \\boldsymbol{y}.\n\\]\n\n\n\n2.2 다항 회귀\n1 차원 변수 \\(x\\) 에 대한 다항식 \\(f(x;\\boldsymbol{\\theta}) = \\theta_0 + \\theta_1 x + \\cdots + \\theta_m x^m\\) 로 데이터를 설명한다고 하자. 우리는 \\(\\mathbb{R}\\) 에서 정의된 \\(\\{1,\\,x,\\,x^2,\\ldots,\\,x^m\\}\\) 이 선형독립임을 안다. 즉 다항회귀를 다차원 선형회귀로 간주 할 수 있다. 그렇다면 설계행렬 \\(\\boldsymbol{\\Phi}\\) 는 다음과 같다.\n\\[\n\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^m \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^m \\\\ \\vdots & & & &\\vdots \\\\ 1 & x_n & x_n^2 & \\cdots & x_n^m\\end{bmatrix}.\n\\]\n\\(L_2\\) 노름에 대해서라면 역시 \\(\\boldsymbol{\\theta}^\\ast = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T \\boldsymbol{y}\\) 를 구하면 된다.\n\n아래 코드와 그림은 데이터를 3차 다항식으로 회귀시킨다.\nusing LinearAlgebra, CairoMakie,\n\nxs = 1:0.5:10\nf(x, p) = p[1]  + p[2] * x^1 +p[3] * x^2 + p[4] * x^3\np0 = [-9.0, 11.8, -3.0, 0.2]\nf(x) = f(x, p0)\nys = f.(xs) .+ 1.0*(rand(length(xs)) .- 0.5);\nΦ = [x^k for x in xs, k in 0:3]\nθ = inv(Φ'*Φ) * Φ' * ys\nf1(x) = f(x, θ)\n\n\n\n\n\n\n그림 1: 다항 회귀\n\n\n\n\n\n\n2.3 비선형 회귀\n다항회귀에서 예를 보였지만 우리가 맞추기를 원하는 함수가 굳이 선형함수가 아니어도 선형회귀를 쓸 수 있다. 실계수 다항식의 집합 \\(\\mathbb{R}[x]\\) 은 벡터공간이며 여기에서 \\(\\{1,\\,x,\\,x^2,\\ldots,\\,x^m\\}\\) 이 선형독립인것, 따라서 기저가 될 수 있다는 것을 알고 있다. 마찬가지로 선형 독립인 함수의 집합을 기저로 사용 할 수 있다.\n선형 독립인 기저함수를 \\(\\{\\phi_1,\\ldots,\\,\\phi_m\\}\\) 라고 하고 데이터 \\(\\{\\boldsymbol{x}_i\\},\\, \\{y_i\\}\\) 를 아래의 \\(f(\\boldsymbol{x};\\boldsymbol{\\theta})\\) 로 회귀시키고자 한다고 하자.\n\\[\nf(\\boldsymbol{x};\\boldsymbol{\\theta}) = \\theta_1\\phi_1(\\boldsymbol{x}) + \\cdots + \\theta_m \\phi_m (\\boldsymbol{x})\n\\]\n그렇다면 설계 행렬 \\(\\boldsymbol{\\Phi}\\) 는\n\\[\n\\boldsymbol{\\Phi} = \\begin{bmatrix}  \\phi_1(\\boldsymbol{x}_1) & \\cdots & \\phi_m(\\boldsymbol{x}_1) \\\\ \\phi_1(\\boldsymbol{x}_2) & \\cdots & \\phi_2(\\boldsymbol{x}_2) \\\\ \\vdots & & \\vdots \\\\  \\phi_m(\\boldsymbol{x}_1) & \\cdots & \\phi_m(\\boldsymbol{x}_1) \\end{bmatrix}\n\\]\n와 같고 역시 \\(L_2\\) 노름에 대해서라면 \\(\\boldsymbol{\\theta}^\\ast = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T \\boldsymbol{y}\\) 를 구한다.\n비선형 회귀에 사용되는 기저함수로는\n\n\n\n표 1: 비선형 회귀에 사용되는 기저 함수\n\n\n\n\n\n\n\n\n\n\n이름\n정의\n특징\n\n\n\n\n다항 함수\n\\(\\phi_j (x) = x^{j}\\)\n\n\n\n가우시안 기저 함수\n\\(\\phi_j (\\boldsymbol{x}) = \\exp \\left[- \\dfrac{\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_j\\|^2}{2\\sigma^2}\\right]\\)\n소위 radial basis function(RBF)\n\n\n시그모이드 함수\n\\(\\phi_j(x) = \\dfrac{1}{1+\\exp (-(x-\\mu_j)/\\sigma)}\\)",
    "crumbs": [
      "AI & ML",
      "이론",
      "선형 회귀"
    ]
  },
  {
    "objectID": "src/ML/theory/regression.html#reguralization",
    "href": "src/ML/theory/regression.html#reguralization",
    "title": "선형 회귀",
    "section": "3 Reguralization",
    "text": "3 Reguralization\n\n\n\n\n\n\nReguralization 의 한글 번역\n\n\n\nReguralization 에 대한 한글 번역으로 정규화, 정칙화 등이 사용되지만 보통 정규화로 번역되는 normalization 와는 달리 reguralization 은 특정 값이 너무 커지지 않게 묶어 두는 역할을 하게 되며, 정규화나 정칙화의 글자 그대로의 의미와는 거리가 있다. 게다가 값 자체를 바꾸는 것이 아니고 값 자체의 크기를 제한하기 때문에 xx화 라는 이름을 붙이는 것도 어울리지 않는것처럼 보인다. (억제기가 어떨까?)\n\n\n\n\n3.1 Overfitting\n주어진 데이터 에 대한 2차, 4차, 5차, 9차 다항식에 대한 다항회귀의 결과는 아래와 같다.\n\n\n\n\n\n\n그림 2: 다항회귀\n\n\n\n데이터 자체가 그다지 규칙적이지 않다. 총 9개의 데이터가 있으며 우리는 주어진 데이터의 결과와 일치하는 다항식이 8차 이상에 서 존재한다는 것을 안다. 그러나 이렇게 얻은 8차 이상의 다항식이 주어진 변수 \\(\\{x_i\\}\\) 이외의 점에서 제대로 된 예측값을 내는것은 다른 문제이다. 예를 들어 위에서 얻은 8차 다항식에 의하면 \\(x=2\\) 에서의 예측값이 71.8 정도인데 실제 데이터가 -20 에서 20 사이에 존재한다는 것을 고려해보면 매우 튀는 결과이다.\n물론 이 데이터가 어떤 데이터냐에 따라 맞는 결과, 혹은 정답에 가까운 결과일 수도 있다. 문제는 실제로 이 데이터가 실제로 어떤 시스템에 대한 데이터냐이며, 만약 우리가 다루는 시스템에서 허용하지 않거나 아주 희박한 확률로 가능한 결과라면 여기서 얻은 8차 다항식은 시스템을 제대로 설명하는 모델이라고 할 수 없다.\n이렇게 주어진 데이터에는 잘 맞지만 이 데이터가 기술하는 시스템과는 동떨어진 모델을 만들어 내는 것을 과적합(overfitting) 이라고 한다. 과적합은 모델에서 결정해야 하는 매개변수의 갯수가 필요 이상으로 많아서, 즉 모델의 자유도가 지나치게 커서 발생한다.\n이것을 해결하기 위해 우선 생각할 수 있는 것은 매개변수의 갯수가 적은 모델을 선택하는 것이다. 또 하나의 방법은 매개변수의 절대값이 지나치게 커지는 것을 막는 것이다. 이 방법중 가장 유용한 방법이 다음에 소개할 정규화(reguralization) 이다.\n\n\n\n3.2 Reguralization\n우리는 매개변수의 최적값 \\(\\boldsymbol{\\theta}^\\ast\\) 를\n\\[\n\\boldsymbol{\\theta}^\\ast = \\argmin_{\\boldsymbol{\\theta}} \\|\\boldsymbol{y}-\\boldsymbol{\\Phi \\theta}\\|_2^2\n\\]\n로 정했다. 만약 적당햔 양수 \\(\\lambda&gt;0\\) 에 대해\n\\[\n\\boldsymbol{\\theta}^\\ast =   \\argmin_\\boldsymbol{\\theta} \\left(\\|\\boldsymbol{y}-\\boldsymbol{\\Phi \\theta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\theta}\\| \\right)\n\\tag{5}\\]\n로 정하는 것이다. 식 5 의 \\(\\lambda \\|\\boldsymbol{\\theta}\\|\\) 는 \\(\\|\\boldsymbol{\\theta}\\|\\) 값이 큰 경우에서 \\(\\boldsymbol{\\theta}^\\ast\\) 가 결정되는 것을 방해한다. 뒤의 \\(\\lambda \\|\\boldsymbol{\\theta}\\|\\) 의 노름은 \\(L_1\\) 혹은 \\(L_2\\) 가운데 선택하며 앞의 \\(\\|\\boldsymbol{y}-\\boldsymbol{\\Phi \\theta}\\|\\) 의 노름과 같을 필요는 없다. \\(L_2\\) 노름의 경우를 Ridge regression 이라고 하고 \\(L_1\\) 노름의 경우는 LASSO (least absolute shrinkage and selection operator) 라고 한다.",
    "crumbs": [
      "AI & ML",
      "이론",
      "선형 회귀"
    ]
  }
]