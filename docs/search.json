[
  {
    "objectID": "src/tools/tikz/tikz_test.html",
    "href": "src/tools/tikz/tikz_test.html",
    "title": "tikz test",
    "section": "",
    "text": "코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_resolution_2\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"DWT resolution\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (0, 0) -- (8.5,0) node[right] {$t$};\n\\draw[-{stealth}] (0,0) -- (0,4) node[above] {$\\omega$};\n\n\\draw [thick] (0, 0.5) -- (8, 0.5);\n\\draw [thick] (0, 1.5) -- (8, 1.5);\n\\draw [thick] (0, 3.5) -- (8, 3.5);\n\n\n\\filldraw[black] (0, 0) circle (1pt) node [below, scale=0.7] {$n=0$};\n\\filldraw[black] (4, 0) circle (1pt) node [below, scale=0.7] {$n=1$};\n\\filldraw[black] (8, 0) circle (1pt) node [below, scale=0.7] {$n=2$};\n\\filldraw[black] (0, 0) circle (1pt) node [left, scale=0.7] {$m=0$};\n\\filldraw[black] (0, 0.5) circle (1pt) node [left, scale=0.7] {$m=-1$};\n\\filldraw[black] (0, 1.5) circle (1pt) node [left, scale=0.7] {$m=-2$};\n\\filldraw[black] (0, 3.5) circle (1pt) node [left, scale=0.7] {$m=-3$};\n\n\\foreach \\x in {0,...,4}\n{\n\\draw [thick] (2*\\x, 0) -- (2*\\x, 3.5);\n\n}\n\n\n\\foreach \\x in {0,...,8}\n{\n\\draw [thick] (\\x, 0.5) -- (\\x, 3.5);\n}\n\n\\foreach \\x in {0,...,15}\n{\n\\draw [thick] (\\x/2, 1.5) -- (\\x/2, 3.5);\n}\n\n\\draw[{stealth}-{stealth}] (7.5, 3.7) -- (8.0,3.7);\n\\node[above] at (7.75, 3.7) {$\\sigma_t$};\n\n\\draw[{stealth}-{stealth}] (8.2, 1.5) -- (8.2,3.5);\n\\node[right] at (8.2, 2.5) {$\\sigma_\\omega$};\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 1: DWT resolution",
    "crumbs": [
      "Tools",
      "tikz test"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_image.html",
    "href": "src/tools/tikz/tikz_image.html",
    "title": "Tikz code for image in Quarto",
    "section": "",
    "text": "코드\n\\begin{tikzpicture}\n    \\draw [black, very thick, -&gt;](0, 0) -- (2, 2)  node[right] {$\\boldsymbol{v}$};\n    \\draw [red, very thick, -&gt;] (0, 0) -- (-2.646, 1 ) node[left] {$\\boldsymbol{H_v x}$};\n    \\draw [blue, very thick, -&gt;] (0, 0) -- (-0.646, 3 ) node[left] {$\\boldsymbol{x}$};\n    \\draw [green, dashed, thick] (1, -1) -- (-2, 2 ) node[left, black] {Plane normal to $\\boldsymbol{v}$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 1: Householder 반사\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_first_exam\n#| code-fold: true\n#| output: asis\n#| fig-align: center\n#| fig-cap: \"tikz in Quarto 예시\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\n\\begin{tikzpicture}\n\\draw[step=1.0,black,thick] (-3,-3) grid (3,3);\n\\node at (-2.5, 2.5) {$x_{1}$};\n\\node at (-2.5, 1.5) {$x_{2}$};\n\\node at (-2.5, 0.5) {$x_{3}$};\n\\node at (-2.5, -0.5) {$\\vdots$};\n\\node at (-1.5, 2.5) {$\\cdots$};\n\\node at (2.5, -2.5) {$x_N$};\n\\node at (5.5, 2.5) {Detector};\n\\draw[red, thick, -{stealth}] (-4, -2) -- (5.5, 1.5);\n\n\\begin{scope}[shift={(5.7,1.55)},rotate=20.4]\n\\draw[blue, very thick] (-0.5,-0.5) -- (0.5,-0.5) -- (0.5,0.5) -- (-0.5,0.5); \n\\end{scope}\n\n\\begin{scope}[shift={(0.3, 0.2)},rotate=20.4]\n\\draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=4ex}]\n  (-0.5,0) -- (0.5,0) node[midway,yshift=-3em, red]{$w_{ij}$};\n\\end{scope}\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 2: tikz in Quarto 예시",
    "crumbs": [
      "Tools",
      "Tikz code for image in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz.html",
    "href": "src/tools/tikz/tikz.html",
    "title": "tikz in Quarto",
    "section": "",
    "text": "코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_first_exam\n#| code-fold: true\n#| output: asis\n#| fig-align: center\n#| fig-cap: \"tikz in Quarto 예시\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows.meta}\n\\usetikzlibrary{decorations.pathreplacing}\n\n\\begin{tikzpicture}\n\\draw (0,0)node(a){} -- (10,0) node (b) {} ;\n\\foreach \\x in  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} % edit here for the vertical lines\n\\draw[shift={(\\x,0)},color=black] (0pt,3pt) -- (0pt,-3pt);\n\\foreach \\x in {0, 0.2, 0.4, 0.6, 0.8, 1} % edit here for the numbers\n\\draw[shift={(\\x*10,0)},color=black] (0pt,0pt) -- (0pt,-3pt) node[below]\n{$\\x$};\n\\node at (8, 0.5) (eq1) {$\\textcolor{red}{\\boldsymbol{SQ}}$};\n\\node at (4, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(0)}}$}; \n\\node at (7, 0.5) (eq2) {$\\textcolor{purple}{\\boldsymbol{G_i(1)}}$}; \n\\node at (3, 0.5) (eq3) {$\\textcolor{blue}{\\boldsymbol{P}}$};\n\\node at (0, 0.5) (eq4) {$\\textcolor{black}{\\boldsymbol{x_i}}$};\n\\draw[decorate, decoration={brace, amplitude=6pt, mirror},] ([yshift=0.5cm]4,0.5)-- node[above=0.25cm]\n{\\shortstack{Text}}([yshift=0.5cm]3,0.5);\n\\draw[decorate, decoration={brace, amplitude=6pt},] ([yshift=-1cm]7,0)-- node[below=0.25cm]\n{\\shortstack{Text}}([yshift=-1cm]3,0);\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 1: tikz in Quarto 예시",
    "crumbs": [
      "Tools",
      "tikz in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz.html#기본-예제",
    "href": "src/tools/tikz/tikz.html#기본-예제",
    "title": "tikz in Quarto",
    "section": "1 기본 예제",
    "text": "1 기본 예제\n\n직선\n다음은 교차하는 직선과 교점을 그린 것이다.\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_2\n#| code-fold: true\n#| output: asis\n#| fig-width: 2\n#| fig-align: center\n#| fig-cap: \"tikz 2\"\n\n\\begin{tikzpicture}[scale=1.0]\n\\draw[gray] (-1,2) -- (2,-4);\n\\draw[red, thick] (-1,-1) -- (2,2);\n\\filldraw[black] (0,0) circle (2pt) node[anchor=west]{Intersection point};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 2: tikz 2\n\n\n\n\n\n\\draw[gray](-1, 2) -- (2, -4); 는 \\((-1, 2)\\) 에서 \\((2, -4)\\) 까지 회색(gray) 의 직선을 그으라는 명령어이다.\n\\draw[red, thick] (-1,-1) -- (2,2); 는 주어진 좌표간의 빨갛고(red), 두꺼운(thick) 직선을 그으라는 의미이다.\n\\filldraw[black] (0,0) circle (2pt) node[anchor=west]{Intersection point}; 는 \\((0, 0)\\) 에 반지름 2 인 속이 꽉 찬 원을 그리며, \\((0, 0)\\) 을 서쪽으로 두는 문자열 Intersection point 를 출력하라는 의미이다.\n\n\n\n\n그리드와 직선의 두께\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_2_2\n#| code-fold: true\n#| output: asis\n#| fig-width: 2\n#| fig-align: center\n#| fig-cap: \"그리드와 직선의 두께\"\n\n\\begin{tikzpicture}\n    \\draw[gray, very thick] (0,0) grid (4,3);\n    \\draw[blue, dashed] (0,0) -- (1,2) -- (3,3) -- (4,2);\n    \\draw[red, -&gt;, thick] (2,0) -- (3.5,2.5);\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 3: 그리드와 직선의 두께\n\n\n\n\n\n\\draw[gray, very thick] (0,0) grid (4,3); 는 \\((0, 0)\\) 부터 \\((4, 3)\\) 까지 가로, 세로 1 간격의 그리드를 회색(gray) 의 두꺼운 선(very thick) 으로 그린다.\ndraw[blue, dashed] (0,0) -- (1,2) -- (3,3) -- (4,2); 는 이어지는 파선(dashed) 을 파란 색으로 그린다.\n\\draw[red, -&gt;, thick] (2,0) -- (3.5,2.5); 는 끝점에 화살표가 있는 빨간 선을 두껍게(thick) 그린다.\n\n\n\n\n닫힌 선분\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_2_3\n#| code-fold: true\n#| output: asis\n#| fig-width: 2\n#| fig-align: center\n#| fig-cap: \"닫힌 선분\"\n\n\\begin{tikzpicture}\n    \\draw[green, thick] (0,0) -- (1,2) -- (3,3) -- (3,-1)--cycle;\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 4: 닫힌 선분\n\n\n\n\n\n\\draw[green, thick] (0,0) -- (1,2) -- (3,3) -- (3,-1)--cycle; 에서 --cycle 은 이 앞의 (3, -1) 을 맨 앞의 (0, 0) 과 연결시키라는 의미이다.\n\n\n\n\n곡선\n다음은 곡선을 그린 것이다.\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_3\n#| code-fold: true\n#| output: asis\n#| fig-width: 2\n#| fig-align: center\n#| fig-cap: \"tikz 3\"\n\n\\begin{tikzpicture}[scale=1.0]\n\\draw (-2,0) -- (2,0);\n\\filldraw [blue] (0,0) circle (2pt);\n\\draw[cyan] (-2,-2) .. controls (0,0) .. (2,-2);\n\\draw[magenta] (-2,2) .. controls (-1,0) and (1,0) .. (2,2);\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 5: tikz 3\n\n\n\n\n\n\\draw[magenta] (-2,2) .. controls (-1,0) and (1,0) .. (2,2); 는 \\((-2, 2)\\) 와 \\((2, 2)\\) 를 끝점으로 하고 \\((-1, 0)\\) 과 \\((1, 0)\\) 을 제어점으로 하는 사차 베지어 곡선을 magenta 색으로 그린다.\n\\draw[cyan] (-2,-2) .. controls (0,0) .. (2,-2); 는 (\\(-2, 2)\\) 와 \\((2, 2)\\) 를 끝점으로 하고 제어점이 \\((0, 0)\\) 인 사차 베지어 곡선을 cyan 색으로 그린다. 즉 \\draw[magenta] (-2,2) .. controls (0,0) and (0,0) .. (2,2); 과 같다.\n\n\n\n\n함수\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_4\n#| code-fold: true\n#| output: asis\n#| fig-width: 4\n#| fig-align: center\n#| fig-cap: \"tikz 3\"\n\\begin{tikzpicture}[domain=0:4]\n  \\draw[very thin,color=gray] (-0.1,-1.1) grid (3.9,3.9);\n\n  \\draw[-&gt;] (-0.2,0) -- (4.2,0) node[right] {$x$};\n  \\draw[-&gt;] (0,-1.2) -- (0,4.2) node[above] {$f(x)$};\n\n  \\draw[color=red]    plot (\\x,\\x)             node[right] {$f(x) =x$};\n  % \\x r means to convert '\\x' from degrees to _r_adians:\n  \\draw[color=blue]   plot (\\x,{sin(\\x r)})    node[right] {$f(x) = \\sin x$};\n  \\draw[color=orange] plot (\\x,{0.05*exp(\\x)}) node[right] {$f(x) = \\frac{1}{20} e^x$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 6: tikz 3\n\n\n\n\n\n\\begin{tikzpicture}[domain=0:4] 에서 [domain=0:4] 은 함수를 사용할 때 \\([0, 4]\\) 구간을 사용한다는 의미이다.\n\\draw[color=red] plot (\\x,\\x) node[right] {$f(x) =x$}; 팔간색 선으로 \\(y=x\\) 그래프를 그리며 끝점의 오른쪽에 \\(f(x)=x\\) 라는 문자열을 넣으라는 의미.\n\n\n\n\n도형\n\nShade\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_shade\n#| code-fold: true\n#| output: asis\n#| fig-width: 6\n#| fig-align: center\n#| fig-cap: \"tikz shade\"\n\n\n\\begin{tikzpicture}[scale = 3, rounded corners,ultra thick]\n  \\shade[top color=yellow,bottom color=black] (0,0) rectangle +(2,1);\n  \\shade[left color=yellow,right color=black] (3,0) rectangle +(2,1);\n  \\shadedraw[inner color=yellow,outer color=black,draw=yellow] (6,0) rectangle +(2,1);\n  \\shade[ball color=green] (9,.5) circle (.5cm);\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 7: tikz shade\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_shade2\n#| code-fold: true\n#| output: asis\n#| fig-width: 3\n#| fig-align: center\n#| fig-cap: \"tikz shade\"\n\n\n\\begin{tikzpicture}[scale=3]\n  \\clip (-0.1,-0.2) rectangle (1.1,0.75);\n  \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);\n  \\draw (-1.5,0) -- (1.5,0);\n  \\draw (0,-1.5) -- (0,1.5);\n  \\draw (0,0) circle (1cm);\n  \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc\n  (0:30:3mm) -- cycle;\n  \\draw[red,very thick]  (30:1cm) -- +(0,-0.5);\n  \\draw[blue,very thick] (30:1cm) ++(0,-0.5) -- (0,0);\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 8: tikz shade\n\n\n\n\n\n\n\n\n좌표계\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-tikz_co_1\n#| code-fold: true\n#| output: asis\n#| fig-width: 2\n#| fig-align: center\n#| fig-cap: \"좌표계\"\n\\begin{tikzpicture}\n  \\draw [red,-&gt;](0,0) -- (xyz cs:x=2) node[right, black] {$x$};\n  \\draw [green,-&gt;](0,0) -- (xyz cs:y=2) node[right, black] {$y$};\n  \\draw [blue,-&gt;](0,0) -- (xyz cs:z=2) node[right, black] {$z$};\n\n  \\draw [red, thick, -&gt;] (0, 0) -- (xyz cs:x=2, y=2, z=1);\n  \\draw [red, dashed] (xyz cs:x=2, y=2, z=1) -- (xyz cs:x=0, y=2, z=1);\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 9: 좌표계\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-rotation_of_vector\n#| code-fold: true\n#| fig-width: 6\n#| output: asis\n#| fig-align: center\n#| fig-cap: \"두 좌표계\"\n\n\\usetikzlibrary {3d} \n\\usetikzlibrary {arrows}\n\\usetikzlibrary{shapes.geometric}\n\\begin{tikzpicture} %[=&gt;stealth]\n\\tikzset{\n    partial ellipse/.style args={#1:#2:#3}{\n        insert path={+ (#1:#3) arc (#1:#2:#3)}\n    }\n}\n  \\draw [-&gt;] (0,0) -- (xyz cs:x=3);\n  \\draw [-&gt;] (0,0) -- (xyz cs:y=4.5);\n  \\draw [-&gt;] (0,0) -- (xyz cs:z=3); \n  \\draw [dashed] (0.0,3.0) ellipse (1.5 and 0.5);\n  \\draw [thick, -{stealth}] (0, 0) -- (xyz cs:x=0.7,y=2.55);\n  \\node[below, scale=.6] at (0.3, 2.3) {$\\boldsymbol{r}_i (q_j)$};\n  \\draw [thick, -{stealth}] (0, 0) --(xyz cs:x=1.4,y=2.8);\n  \\node[scale=.6] at (1.5, 1.7) {$\\boldsymbol{r}_i (q_j+dq_j)$};\n  \\draw [red, thick, -{stealth}] (0.7, 2.55) -- (1.4, 2.8);\n  \\node[red, scale=.6] at (1.0, 2.5) {$d\\boldsymbol{r}_i$};\n  \\draw [thick, -{stealth}] (0, 3) -- (0, 3.7) node[right, scale=0.6] {$\\;\\boldsymbol{n}$};\n\n  \\draw [teal] (0, 3) -- (0.7, 2.55);\n  \\draw [teal] (0, 3) -- (1.4, 2.8);\n  \\draw[teal, -&gt;] (0, 3.0) [partial ellipse=297:340:0.6 and 0.2] node[above, scale=0.6] {$dq_j$} ;\n  \\draw[purple, -&gt;] (0, 0) [partial ellipse=90:75:1 and 1] node[above left, scale=0.6] {$\\theta$} ;\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 10: 두 좌표계\n\n\n\n\n\n\n\n신호처리 함수\n\n\n코드\n\\usetikzlibrary{arrows}\n\\begin{tikzpicture}[ &gt;=stealth]\n\n\n\\draw[-&gt;] (-2, 0) -- (2, 0) node[below, black] {$x$} ;\n\\draw[ -&gt;] (0, -0.5) -- (0, 1.5);\n\\draw[thick] (-2, 0) -- (0, 0) node[below left, black] {$0$} -- (0, 1) node[left, black] {$1$}-- (2, 1) node[above left, black] {$\\text{step}(x)$};\n\n\\draw[-&gt;] (4, 0) -- (8, 0) node[below, black] {$x$} ;\n\\draw[ -&gt;] (6, -0.5) -- (6, 1.5);\n\\draw[thick] (4, 0) -- (5.5, 0) node[below, black] {$-\\frac{1}{2}$} -- (5.5, 1) -- (6, 1)  -- (6.5, 1)-- (6.5, 0)  node[below, black] {$\\frac{1}{2}$} -- (8, 0) node[above left, black] {$\\text{rect}(x)$};\n\n\\draw[-&gt;] (-2, -3) -- (2, -3) node[below, black] {$x$} ;\n\\draw[ -&gt;] (0, -3.5) -- (0, -1.5);\n\\draw[thick] (-2, -3) -- (-0.5, -3) node[below, black] {$-\\frac{1}{2}$} -- (0, -2) -- (0.5, -3) node[below, black] {$-\\frac{1}{2}$} -- (2, -3) node[above left, black] {$\\text{tri}(x)$};\n\n\n\\draw[-&gt;] (4, -3) node[above right, black] {$\\text{sinc}(x)$} -- (8, -3) node[below, black] {$x$} ;\n\\draw[ -&gt;] (6, -3.5) -- (6, -1.5);\n\\draw[domain=4.0:8.0, smooth, variable=\\x, thick] plot ({\\x}, {0.116*sin(500* (\\x-6))/((\\x-6)) -3 });\n\\draw (6.36, -2.9) -- (6.36, -3.1) node[below, black] {$\\pi$} ; \n\n\n\\draw[-&gt;] (-2, -6) -- (2, -6) node[below, black] {$x$} ;\n\\draw[ -&gt;] (0, -6.5) -- (0, -4);\n\\draw[domain=-1.8:1.8, smooth, variable=\\x, thick] plot ({\\x}, {exp(-(\\x)^2) - 6 });\n\\node[black, right] at (0, -4.7) {$\\text{gauss}(x)$};\n%\\draw (6.36, -2.9) -- (6.36, -3.1) node[below, black] {$\\pi$} ; \n\n\n\\draw[-&gt;] (4, -6)  -- (8, -6) node[below, black] {$x$} ;\n\\draw[ -&gt;] (6, -6.5) -- (6, -4);\n\\draw[very thick] (4.2, -6) -- (6, -6) -- (6, -4) node[left, black] {$\\infty$}-- (6, -6)  -- (7.8, -6) node[above left, black] {$\\delta (x)$};\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\n그림 11: 기본적인 신호들",
    "crumbs": [
      "Tools",
      "tikz in Quarto"
    ]
  },
  {
    "objectID": "src/posts/index.html",
    "href": "src/posts/index.html",
    "title": "The Notebook",
    "section": "",
    "text": "첫번째 글\n\n\n\n\n\n\n\n\n\n\n\nJulia_KAERI\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html",
    "href": "src/gpu/cuda/02_programming_model.html",
    "title": "Programming Model",
    "section": "",
    "text": "CUDA C++는 프로그래머가 커널이라는 C++ 함수를 정의할 수 있도록 하여 C++를 확장한 것.\n이 함수는 호출되면 N개의 서로 다른 CUDA 스레드에 의해 병렬로 N번 실행된다. 반면 일반 C++ 함수는 한 번만 실행된다.\n\n\n간단한 커널의 예는 아래와 같다.\n// Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation with N threads\n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ...\n}",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#커널",
    "href": "src/gpu/cuda/02_programming_model.html#커널",
    "title": "Programming Model",
    "section": "",
    "text": "CUDA C++는 프로그래머가 커널이라는 C++ 함수를 정의할 수 있도록 하여 C++를 확장한 것.\n이 함수는 호출되면 N개의 서로 다른 CUDA 스레드에 의해 병렬로 N번 실행된다. 반면 일반 C++ 함수는 한 번만 실행된다.\n\n\n간단한 커널의 예는 아래와 같다.\n// Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation with N threads\n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ...\n}",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_thread_hierarchy",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_thread_hierarchy",
    "title": "Programming Model",
    "section": "2 스레드 계층 구조 : 스레드 \\(\\rightarrow\\) 블럭 \\(\\rightarrow\\) 그리드",
    "text": "2 스레드 계층 구조 : 스레드 \\(\\rightarrow\\) 블럭 \\(\\rightarrow\\) 그리드\n\nthreadIdx 는 3차원 벡터로 1, 2, 3 차원 스레드 인덱스를 이용하여 스레드를 특정할 수 있다.\n스레드 블럭(Thread block) : 1, 2 혹은 3차원의 스레드 인덱스의 집합\n\n\n\n스레드 인덱스와 스레드 ID\n\n1차원 블럭 : ID = 인덱스\n2차원 블럭 : 블럭 사이즈가 \\((D_x, D_y)\\) 일 경우 스레드 인덱스 \\((x,\\,y)\\) 에 대한 \\(\\text{ID} = x+yD_x\\).\n3차원 블럭 : 블걱 사이즈가 \\((D_x, D_y, D_z)\\) 일 경우 스레드 인덱스 \\((x,\\,y,\\,z)\\) 에 대한 \\(\\text{ID} = x+yD_x + zD_x D_y\\).\n\n\n\n\n블럭당 스레드 수 제한 : 블럭의 모든 스레드는 같은 SM(streaming multiprocess) 코어 안에 있어야 하며 그 코어의 제한된 메모리 자원을 공유한다. 현재의 GPU 에서 하나의 블럭은 최대 1024 개의 스레드를 포함 할 수 있다.\n커널은 같은 형상의 다수의 스레드 블럭에서 실행되며, 따라서 총 스레드 수는 블럭수 와 블럭당 스레드 수의 곱이다.\n그리드 (Grid) 는 블럭의 집합이며, 1, 2 혹은 3차원 인덱스를 갖는다. 한 그리드의 스레드 블럭의 갯수는 처리하고자 하는 데이터의 크기에 의해 지정되며, 보통 시스템의 프로세서 수 보다 크다.\n\n\n\n\n\n\n\n\n그림 1: Grid of Thread Blocks\n\n\n\n\n행렬곱에 대한 코드와 실행에 대한 아래의 코드를 보자.\n// Kernel definition\n__global__ void MatAdd(float A[N][N], float B[N][N],\nfloat C[N][N])\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i &lt; N && j &lt; N)\n        C[i][j] = A[i][j] + B[i][j];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);\n    ...\n}\n\n&lt;&lt;&lt;...&gt;&gt;&gt; 구문에서 지정된 블록당 스레드 수와 그리드당 블록 수는 int 또는 dim3 타입이다. 2차원 블록 또는 그리드는 위의 예와 같이 지정할 수 있다.\n그리드 내의 각 블록은 커널 내에서 내장된 blockIdx 변수를 통해 액세스할 수 있는 1차원, 2차원 또는 3차원 고유 인덱스로 식별할 수 있다. 스레드 블록의 차원은 커널 내에서 내장된 blockDim 변수를 통해 액세스할 수 있다.\n위 코드의 경우 이 경우 임의적이기는 하지만 16x16(256개 스레드)의 스레드 블록 크기가 일반적인 선택이다. 그리드는 행렬 요소당 하나의 스레드를 가질 수 있을 만큼 충분한 블록으로 생성된다. 단순화를 위해 이 예에서는 각 차원의 그리드당 스레드 수가 해당 차원의 블록당 스레드 수로 균등하게 나누어진다고 가정하지만 반드시 그럴 필요는 없다.\n스레드 블록은 독립적으로 실행될 수 있어야 한다. 병렬 또는 직렬로 어떤 순서로든 실행할 수 있어야 한다. 이러한 독립성으로 인해 앞장의 그림 3 에서 볼 수 있듯이 스레드 블록을 모든 코어 수에 걸쳐 어떤 순서로든 예약할 수 있으므로 프로그래머는 코어 수에 따라 확장되는 코드를 작성할 수 있다.\n블록 내의 스레드는 일부 공유 메모리를 통해 데이터를 공유하고 실행을 동기화하여 메모리 액세스를 조정함으로써 협력할 수 있다. 더 정확하게 말하면 __syncthreads() 내장 함수를 호출하여 커널에서 동기화 지점을 지정할 수 있다. __syncthreads() 는 어떤 허락이 떨어지기 전까지 블록의 모든 스레드가 진행을 멈추고 기다려야 하는 장벽 역할을 한다. 공유 메모리 에는 공유 메모리를 사용하는 예를 제시한다. __syncthreads() 외에도 Cooperative Groups API는 풍부한 스레드 동기화 기본 요소를 제공한다.\n효율적인 협력을 위해 공유 메모리는 각 프로세서 코어 근처의 저지연 메모리(L1 캐시와 매우 유사)가 될 것으로 예상되고 __syncthreads() 는 가벼울 것으로 예상된다.\n\n\n\nThread Block Cluseter\nNVIDIA Compute Capability 9.0 부터 CUDA 프로그래밍 모델은 스레드 블록으로 구성된 스레드 블록 클러스터라는 선택적 계층 구조를 도입한다. 스레드 블록의 스레드가 스트리밍 멀티프로세서에서 공동 스케줄링되는 것과 유사하게 클러스터의 스레드 블록도 GPU의 GPU 처리 클러스터(GPC)에서 공동 스케줄링된다.\n스레드 블록과 유사하게 클러스터도 스레드 블록 클러스터 그리드에서 설명한 대로 1차원, 2차원 또는 3차원으로 구성된다. 클러스터의 스레드 블록 수는 사용자가 정의할 수 있으며 클러스터의 최대 8개 스레드 블록이 CUDA에서 이식 가능한 클러스터 크기로 지원된다. 8개의 멀티프로세서를 지원하기에는 너무 작은 GPU 하드웨어 또는 MIG 구성에서는 최대 클러스터 크기가 그에 따라 줄어들게된다. 8개 이상의 스레드 블록 클러스터 크기를 지원하는 더 큰 구성뿐만 아니라 이러한 더 작은 구성을 식별하는 것은 아키텍처에 따라 다르며 cudaOccupancyMaxPotentialClusterSize API를 사용하여 쿼리할 수 있다.\n\n\n\n\n\n\n그림 2: Grid of Thread Block Clusters\n\n\n\n\n\n\n\n\n\n노트\n\n\n\nIn a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the Cluster Group API.\n\n\n\n스레드 블록 클러스터는 __cluster_dims__(X,Y,Z) 를 사용하는 컴파일러 시간 커널 속성을 사용하거나 CUDA 커널 실행 API cudaLaunchKernelEx 를 사용하여 커널에서 활성화할 수 있다. 아래 예는 컴파일러 시간 커널 속성을 사용하여 클러스터를 실행하는 방법을 보여준다. 커널 속성을 사용하는 클러스터 크기는 컴파일 시간에 고정되고 그런 다음 고전적인 &lt;&lt;&lt; , &gt;&gt;&gt; 를 사용하여 커널을 실행할 수 있다. 커널이 이런 컴파일 시간의 클러스터 크기를 사용하는 경우 커널을 실행할 때 클러스터 크기를 수정할 수 없다.\n// Kernel definition\n// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension\n__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)\n{\n\n}\n\nint main()\n{\n    float *input, *output;\n    // Kernel invocation with compile time cluster size\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n\n    // The grid dimension is not affected by cluster launch, and is still enumerated\n    // using number of blocks.\n    // The grid dimension must be a multiple of cluster size.\n    cluster_kernel&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(input, output);\n}\n\n스레드 블록 클러스터 크기는 런타임에 설정할 수도 있고, 커널은 CUDA 커널 실행 API cudaLaunchKernelEx 를 사용하여 실행할 수 있다. 아래 코드 예제는 확장 가능한 API를 사용하여 클러스터 커널을 실행하는 방법을 보여준다.\n// Kernel definition\n// No compile time attribute attached to the kernel\n__global__ void cluster_kernel(float *input, float* output)\n{\n\n}\n\nint main()\n{\n    float *input, *output;\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n\n    // Kernel invocation with runtime cluster size\n    {\n        cudaLaunchConfig_t config = {0};\n        // The grid dimension is not affected by cluster launch, and is still enumerated\n        // using number of blocks.\n        // The grid dimension should be a multiple of cluster size.\n        config.gridDim = numBlocks;\n        config.blockDim = threadsPerBlock;\n\n        cudaLaunchAttribute attribute[1];\n        attribute[0].id = cudaLaunchAttributeClusterDimension;\n        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension\n        attribute[0].val.clusterDim.y = 1;\n        attribute[0].val.clusterDim.z = 1;\n        config.attrs = attribute;\n        config.numAttrs = 1;\n\n        cudaLaunchKernelEx(&config, cluster_kernel, input, output);\n    }\n}\ncompute capability 9.0 호환 GPU에서 클러스터의 모든 스레드 블록은 단일 GPU 처리 클러스터(GPC)에서 공동 스케줄링되도록 보장되며 클러스터의 스레드 블록이 Cluster Group API cluster.sync() 를 사용하여 하드웨어가 지원하는 동기화를 수행할 수 있다. Cluster Group은 또한 num_threads() 및 num_blocks() API를 사용하여 스레드 수 또는 블록 수 측면에서 클러스터 그룹 크기를 쿼리하는 멤버 함수를 제공한다. 클러스터 그룹의 스레드 또는 블록 순위는 dim_threads() 및 dim_blocks() API를 사용하여 각각 쿼리할 수 있다.\n클러스터에 속한 스레드 블록은 분산 공유 메모리(Distributed shared memory)에 액세스할 수 있다. 클러스터의 스레드 블록은 분산 공유 메모리의 모든 주소를 읽고, 쓰고, 원자성을 수행할 수 있다. 분산 공유 메모리(Distributed shared memory)에는 분산 공유 메모리에서 히스토그램을 수행하는 예를 제시한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_memory_hierarchy",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_memory_hierarchy",
    "title": "Programming Model",
    "section": "3 메모리 계층 구조",
    "text": "3 메모리 계층 구조\n\n\n\n\n\n\n그림 3: Memory Hierarchy\n\n\n\nCUDA 스레드는 그림 3 에서 설명한 대로 실행 중에 여러 메모리 공간에서 데이터에 액세스할 수 있다. 각 스레드에는 개별적인 로컬 메모리가 있다. 각 스레드 블록에는 블록의 모든 스레드에서 볼 수 있고 블록과 동일한 수명을 가진 공유 메모리가 있다. 스레드 블록 클러스터의 스레드 블록은 서로의 공유 메모리에서 읽기, 쓰기 및 아토믹 연산을 수행할 수 있다. 모든 스레드는 동일한 전역 메모리(Global memory)에 액세스할 수 있다.\n또한 모든 스레드에서 액세스할 수 있는 읽기 전용 메모리 공간이 추가로 존재한다 - 상수 메모리 공간과 텍스처 메모리 공간. 전역, 상수 및 텍스처 메모리 공간은 다양한 메모리 사용에 맞게 최적화되어 있다(장치 메모리 액세스 참조). 텍스처 메모리는 또한 일부 특정 데이터 형식에 대해 다양한 주소 지정 모드와 데이터 필터링을 제공한다(텍스처 및 표면 메모리 참조).\n글로벌, 상수 및 텍스처 메모리 공간은 동일한 애플리케이션에서 커널을 시작할 때에도 지속된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda_heterogeneous_programming",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda_heterogeneous_programming",
    "title": "Programming Model",
    "section": "4 이기종 프로그래밍 (Heterogeneous Programming)",
    "text": "4 이기종 프로그래밍 (Heterogeneous Programming)\n\n\n\n\n\n\n그림 4: Heterogeneous Programming\n\n\n\n\n\n그림 4 에서 알 수 있듯이 CUDA 프로그래밍 모델은 C++ 프로그램을 실행하는 호스트에 대한 보조 프로세서로 작동하는 물리적으로 분리된 장치에서 CUDA 스레드가 실행된다고 가정한다. 예를 들자면 커널이 GPU에서 실행되고 나머지 C++ 프로그램이 CPU에서 실행되는 경우이다. 이후 GPU 가 실행되는 분리된 장치를 디바이스라고 부르겠다. (호스트와 디바이스라는 명칭으로 나누는 것은 일종의 관례이다)\nCUDA 프로그래밍 모델은 또한 호스트와 디바이스가 각각 호스트 메모리와 디바이스 메모리라고 하는 DRAM에서 자체적인 별도 메모리 공간을 유지한다고 가정한다. 따라서 프로그램은 CUDA 런타임(프로그래밍 인터페이스에서 설명)에 대한 호출을 통해 커널에서 볼 수 있는 전역, 상수 및 텍스처 메모리 공간을 관리한다. 여기에는 디바이스 메모리 할당 및 할당 해제와 호스트와 디바이스 메모리 간의 데이터 전송이 포함된다.\nUnified Memory 는 호스트와 디바이스 메모리 공간을 연결하는 관리되는 메모리를 제공한다. 관리되는 메모리는 시스템의 모든 CPU와 GPU에서 공통 주소 공간이 있는 단일의 일관된 메모리 이미지로 액세스할 수 있다. 이 기능은 디바이스 메모리에 대한 초과 예약(oversubcription)을 가능하게 하며 호스트와 디바이스에서 데이터를 명시적으로 미러링할 필요성을 없애 애플리케이션 포팅 작업을 크게 단순화할 수 있다. Unified Memory에 대한 소개는 Unified Memory Programming 을 참조하라.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#sec-cuda-asynchronous_simt_programming_model",
    "href": "src/gpu/cuda/02_programming_model.html#sec-cuda-asynchronous_simt_programming_model",
    "title": "Programming Model",
    "section": "5 비동기적 SIMT 프로그래밍 모델",
    "text": "5 비동기적 SIMT 프로그래밍 모델\nCUDA 프로그래밍 모델에서 스레드는 계산이나 메모리 작업을 수행하기 위한 가장 낮은 수준의 추상화이다. NVIDIA Ampere GPU 아키텍처 기반 장치부터 시작하여 CUDA 프로그래밍 모델은 비동기 프로그래밍 모델을 통해 메모리 작업에 가속을 제공한다. 비동기 프로그래밍 모델은 CUDA 스레드와 관련하여 비동기 연산의 행동을 정의한다.\n비동기 프로그래밍 모델은 CUDA 스레드 간 동기화를 위한 비동기 배리어 의 동작을 정의한다. 이 모델은 또한 cuda::memcpy_async를 사용하여 GPU에서 계산하는 동안 글로벌 메모리에서 비동기적으로 데이터를 이동하는 방법을 설명하고 정의한다.\n\n\n비동기 연산\n비동기 연산은 한 CUDA 스레드에서 시작하여 다른 스레드에서와 같이 비동기적으로 실행되는 연산으로 정의된다. 잘 구성된 프로그램에서 하나 이상의 CUDA 스레드가 비동기 연산과 동기화된다. 비동기 작업을 시작한 CUDA 스레드는 동기화 스레드에 포함될 필요가 없다.\n이러한 비동기 스레드(as-if 스레드)는 항상 비동기 연산을 시작한 CUDA 스레드와 연결됩니다. 비동기 연산은 동기화 객체를 사용하여 작업 완료를 동기화한다. 이러한 동기화 객체는 사용자가 명시적으로 관리하거나(예: cuda::memcpy_async) 라이브러리 내에서 암묵적으로 관리할 수 있습니다(예: cooperative_groups::memcpy_async).\n동기화 객체는 cuda::barrier 또는 cuda::pipeline 일 수 있다. 이러한 객체는 비동기 Barrier 및 cuda::pipeline 을 사용한비동기 데이터 복사에서 자세히 설명한다. 이러한 동기화 객체는 다른 스레드 범위에서 사용할 수 있다. scope 는 비동기 작업과 동기화하기 위해 동기화 객체를 사용할 수 있는 스레드 집합을 정의한다. 다음 표는 CUDA C++에서 사용 가능한 스레드 범위와 각각과 동기화할 수 있는 스레드를 정의합니다.\n\n\n\n\n\n\n\nThread Scope\nDescription\n\n\n\n\ncuda::thread_scope::thread_scope_thread\nOnly the CUDA thread which initiated asynchronous operations synchronizes.\n\n\ncuda::thread_scope::thread_scope_block\nAll or any CUDA threads within the same thread block as the initiating thread synchronizes.\n\n\ncuda::thread_scope::thread_scope_device\nAll or any CUDA threads in the same GPU device as the initiating thread synchronizes.\n\n\ncuda::thread_scope::thread_scope_system\nAll or any CUDA or CPU threads in the same system as the initiating thread synchronizes.\n\n\n\n\n이러한 스레드 scope 는 표준 C++ 라이브러리에 대한 확장인 CUDA Standard C++ library 로 구현되었다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda/02_programming_model.html#compute-capability",
    "href": "src/gpu/cuda/02_programming_model.html#compute-capability",
    "title": "Programming Model",
    "section": "6 Compute Capability",
    "text": "6 Compute Capability\n장치의 컴퓨팅 기능은 버전 번호로 표현되며, 때로는 “SM 버전”이라고도 한다. 이 버전 번호는 GPU 하드웨어에서 지원하는 기능을 식별하며, 런타임에 애플리케이션에서 현재 GPU에서 사용할 수 있는 하드웨어 기능 및/또는 명령어를 확인하는 데 사용됩다. 컴퓨팅 기능은 주요 개정 번호 X와 부차 개정 번호 Y로 구성되며 X.Y로 표시된다.\n동일한 주요 개정 번호가 있는 장치는 동일한 코어 아키텍처입니다. 주요 개정 번호는 NVIDIA Hopper GPU 아키텍처 기반 장치의 경우 9, NVIDIA Ampere GPU 아키텍처 기반 장치의 경우 8, Volta 아키텍처 기반 장치의 경우 7, ​​Pascal 아키텍처 기반 장치의 경우 6, Maxwell 아키텍처 기반 장치의 경우 5, Kepler 아키텍처 기반 장치의 경우 3 이다.\n부차 개정 번호는 코어 아키텍처에 대한 점진적 개선에 해당하며, 새로운 기능이 포함될 수 있다.\nTuring은 컴퓨팅 기능 7.5의 장치에 대한 아키텍처이며 Volta 아키텍처를 기반으로 하는 점진적 업데이트이다.\nCUDA 지원 GPU는 모든 CUDA 지원 장치와 해당 컴퓨팅 기능을 나열한다. 컴퓨팅 기능은 각 컴퓨팅 기능의 기술 사양을 제공한다.\n\n\n\n\n\n\n\n경고\n\n\n\n특정 GPU의 컴퓨팅 기능 버전은 CUDA 소프트웨어 플랫폼의 버전인 CUDA 버전(예: CUDA 7.5, CUDA 8, CUDA 9)과 다르다. CUDA 플랫폼은 애플리케이션 개발자가 아직 발명되지 않은 미래의 GPU 아키텍처를 포함하여 여러 세대의 GPU 아키텍처에서 실행되는 애플리케이션을 만드는 데 사용된다. CUDA 플랫폼의 새 버전은 종종 해당 아키텍처의 컴퓨팅 기능 버전을 지원하여 새 GPU 아키텍처에 대한 기본 지원을 추가하지만, CUDA 플랫폼의 새 버전은 일반적으로 하드웨어 세대와 독립적인 소프트웨어 기능도 포함한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Model"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html",
    "title": "CUDA.jl 성능 팁",
    "section": "",
    "text": "CUDA.jl 의 Performance Tips 을 요약 & 번역한 것이다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#일반적인-팁",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#일반적인-팁",
    "title": "CUDA.jl 성능 팁",
    "section": "1 일반적인 팁",
    "text": "1 일반적인 팁\n항상 코드 프로파일링부터 시작한다(자세한 내용은 profiling 페이지 참조). 먼저 CUDA.@profile 또는 NSight Systems 를 사용하여 프로그램 전체를 분석하고 핫스팟과 병목지점을 파악해야 한다. 여기에 집중하면서 다음을 수행한다.\n\nCPU와 GPU 간의 데이터 전송을 최소할 것. 불필요한 메모리 사본을 제거하고 다수의 적은 전송을 일괄적인 큰 전송으로 한다.\n문제가 있는 커널 호출을 식별한다. 단일 호출로 처리할 수 있는 수천 개의 커널을 동작시킬 수 있다.\nCPU가 GPU를 바쁘게 유지할 만큼 빠르게 작업을 제출하지 않는 스톨을 찾습니다.\n\n이것으로 충분하지 않고 느리게 실행되는 커널을 식별한 경우 NSight Compute를 사용하여 해당 커널을 자세히 분석해 볼 수 있다. 중요도 순으로 시도해야 할 몇 가지 사항은 다음과 같다.\n\n메모리 액세스를 최적한다. 예를 들어 불필요한 전역 접근(대신 공유 메모리에서 버퍼링)을 피하거나 접근을 병합한다.\n각 스트리밍 멀티프로세서(SM)에서 더 많은 스레드를 실행한다. 이는 레지스터 압력을 낮추거나 공유 메모리 사용을 줄임으로써 달성할 수 있다. 아래 팁은 레지스터 압력을 줄일 수 있는 다양한 방법을 설명한다.\nFloat64 및 Int/Int64 와 같은 64비트 유형 대신 Float32 및 Int32 와 같은 32비트 타입을 사용한다.\n같은 워프의 스레드가 갈라지는 원인이 되는 제어 흐름 사용을 피하세요. 즉, while 또는 for 루프가 전체 워프에서 동일하게 동작하도록 하고, 워프 내에서 갈라지는 if 를 if else 로 바꾼다.\nGPU가 메모리 액세스의 지연 시간을 숨길 수 있도록 계산 강도를 높인다.\n\n\n\nInlining\nInlining 은 레지스터 사용을 줄여 커널 속도를 높일 수 있다. 모든 함수의 인라인을 강제로 실행하려면 @cuda always_inline=true 를 사용하면 된다.\n\n\n\n쓰레드 당 최대 레지스터 개수를 제한한다.\n시작할 수 있는 스레드 수는 부분적으로 커널이 사용하는 레지스터 수에 의해 결정된다. 이는 멀티프로세서의 모든 스레드에서 레지스터가 공유되기 때문이다. 스레드당 최대 레지스터 수를 설정하면 사용되는 레지스터가 줄어들어 스레드 수가 늘어나고 레지스터를 로컬 메모리에 분산시키는 댓가로 성능이 향상될 수 있다. 최대 레지스터 수를 32로 설정하려면 @cuda maxregs=32 라고 한다.\n\n\nFastMath\n일반적인 산술 함수의 고속 버젼을 사용하는 @fastmath 매크로를 사용할 수 있다. 다음의 매크로를 사용하라. @cuda fastmath=true",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#julia-전용-팁",
    "href": "src/gpu/cuda.jl/cuda_jl_performance_tips.html#julia-전용-팁",
    "title": "CUDA.jl 성능 팁",
    "section": "2 Julia 전용 팁",
    "text": "2 Julia 전용 팁\n\n런타임 예외를 최소화한다\nJulia 에서 많은 일반적인 연산은 런타임에 Error 를 발생시킬 수 있으며, Error 는 종종 분기를 만들고 해당 분기에서 함수를 호출하는데, 이 둘 다 GPU 에서는 느리다. 배열을 인덱싱할 때 (inbounds를?) 사용하면 경계 검사로 인한 예외가 제거된다. --check-bounds=yes (Pkg.test의 기본값) 로 코드를 실행하면 항상 경계 검사가 발생한다. LLVM.jl 패키지의 assume 을 사용하여 예외를 제거할 수도 있다. 다음을 보라.\nusing LLVM.Interop\n\nfunction test(x, y)\n    assume(x &gt; 0)\n    div(y, x)\nend\nassume(x &gt; 0) 은 컴파일러에게 0 으로 나누는 에러가 발생하지 않을 것임을 말해준다.\n\n\n\n32 비트 정수\n가능하면 32비트 정수를 하용한다. 레지스터 압력의 일반적인 원인은 32비트만 필요한데 64비트 정수를 사용하는 것이다. 예를 들어, 하드웨어의 인덱스는 32비트 정수이지만 Julia의 리터럴은 Int64 로, blockIdx().x-1 과 같은 표현식은 64비트 정수로 승격된다. 32비트 정수를 사용하려면 1 을 Int32(1) 로 대체하거나 CUDA를 사용하여 실행하는 경우 더 간결하게 1i32 로 대체할 수 있다.\n이것이 얼마나 큰 차이를 만드는지 확인하기 위해 소개 튜토리얼에서 소개한 커널을 사용해 보자.\nusing CUDA, BenchmarkTools\n\nfunction gpu_add3!(y, x)\n    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n    for i = index:stride:length(y)\n        @inbounds y[i] += x[i]\n    end\n    return\nend\ngpu_add3! (generic function with 1 method)\n몇개의 레지스터가 사용되었는지 확인해보자.\nx_d = CUDA.fill(1.0f0, 2^28)\ny_d = CUDA.fill(2.0f0, 2^28)\n\nCUDA.registers(@cuda gpu_add3!(y_d, x_d))\n29\n위의 결과는 기기마다 다를 수 있다. 이제 32 비트 정수를 사용하는 커널은 아래와 같다.\nfunction gpu_add4!(y, x)\n    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n    for i = index:stride:length(y)\n        @inbounds y[i] += x[i]\n    end\n    return\nend\ngpu_add4! (generic function with 1 method)\nCUDA.registers(@cuda gpu_add4!(y_d, x_d))\n28\n따라서 32비트 정수로 전환하여 레지스터 하나를 덜 사용하게 되고, 64비트 정수를 더 많이 사용하는 커널에서는 레지스터 수가 더 크게 감소할 것으로 예상된다.\n\n\n\nStepRange 사용을 피하라\nfor 루프의 이전 커널에서 StepRange 인 index:stride:length(y) 를 순회했다. 안타깝게도 StepRange 를 구성하는 것은 오류가 발생할 수 있고, 단순히 반복하고 싶을 경우에는 불필요한 계산을 포함하기 때문에 느립니다. 대신 다음과 같이 while 루프를 사용하는 것이 더 빠르다.\nfunction gpu_add5!(y, x)\n    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x\n    stride = gridDim().x * blockDim().x\n\n    i = index\n    while i &lt;= length(y)\n        @inbounds y[i] += x[i]\n        i += stride\n    end\n    return\nend\ngpu_add5! (generic function with 1 method)\n벤치마크는\nfunction bench_gpu4!(y, x)\n    kernel = @cuda launch=false gpu_add4!(y, x)\n    config = launch_configuration(kernel.fun)\n    threads = min(length(y), config.threads)\n    blocks = cld(length(y), threads)\n\n    CUDA.@sync kernel(y, x; threads, blocks)\nend\n\nfunction bench_gpu5!(y, x)\n    kernel = @cuda launch=false gpu_add5!(y, x)\n    config = launch_configuration(kernel.fun)\n    threads = min(length(y), config.threads)\n    blocks = cld(length(y), threads)\n\n    CUDA.@sync kernel(y, x; threads, blocks)\nend\nbench_gpu5! (generic function with 1 method)\n@btime bench_gpu4!($y_d, $x_d)\n  76.149 ms (57 allocations: 3.70 KiB)\n@btime bench_gpu5!($y_d, $x_d)\n  75.732 ms (58 allocations: 3.73 KiB)\n이 벤치마크는 이 커널에 대한 성능 이점에서는 미미하지만 StepRange 를 사용할 때 28개의 레지스터가 사용되었다는 점을 상기하면 사용된 레지스터의 양에 큰 차이가 있음을 확인 할 수 있다.\nCUDA.registers(@cuda gpu_add5!(y_d, x_d))\n12",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 성능 팁"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html",
    "title": "CUDA.jl 배열 처리",
    "section": "",
    "text": "필요한 패키지는 다음과 같다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#cuarray-타입",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#cuarray-타입",
    "title": "CUDA.jl 배열 처리",
    "section": "1 CuArray 타입",
    "text": "1 CuArray 타입\n\n기본 연산\nCUDA.jl 의 기본적인 타입은 CuArray 타입으로 Array 타입과 많은 부분에서 비슷하다. 우선 CPU 에서 만든 배열을 GPU 의 CuArray 타입으로 전환시켜 보자.\ncarr0 = CuArray(rand(Float32, 128, 128))\n결과는 다음과 같다. CPU 에서 128 x 128 랜덤 배열을 만든 후 GPU 에서의 CuArray 타입으로 변환시켰다. 아래로의 많은 줄이 생략되었다.\n128×128 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 0.614408   0.545875   0.716857   …  0.25018    0.304857   0.99422\n 0.432157   0.148661   0.060947      0.0701835  0.15738    0.26359\n\n기본적인 사용법도 눈여겨보라.\ncarr1 = CuArray{Float32}(undef,  1024)\ncarr2 = fill!(copy(carr1), 0f0)\n@test carr2 == CUDA.zeros(Float32, 1024)\n\nCuArray 에 많은 배열 연산을 수행 할 수 있다.\ncarr3 = carr1.^2 + carr2.^2\ncarr4 = map(cos, carr1)\ncarr5 = reduce(+, carr1)\n\nArray 와 같이 논리 연산을 통해 성분을 선택 할 수 있다.\nc1 = CuArray([1,2,3,4,5])\nc2 = c1[[true, false, false, true, true]]\n3-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 1\n 4\n 5\n\n이 외의 중요한 연산을 수행해보자. 모두 Array 에서와 같이 작동한다.\nfindall(isodd, c1)\nfindfirst(isodd, c1)\nfindmin(c1)\n\nreshape, view 와 같은 것도 Array 와 똑같이 동작한다.\nc2 = CuArray{Int32}(collect(1:6))\nc3 = reshape(c2, 2, 3)\nc4 = view(c2, 2:4)\n\n\n\n스칼라 인덱싱\n다음은 Array 에서와 달리 경고를 발생시킨다.\na = CuArray([1])\na[1]+=1\n┌ Warning: Performing scalar indexing on task Task (runnable, started) @0x000075014b772400.\n│ Invocation of getindex resulted in scalar indexing of a GPU array.\n│ This is typically caused by calling an iterating implementation of a method.\n│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n│ and therefore should be avoided.\n│ \n│ If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`\n│ to enable scalar iteration globally or for the operations in question.\n└ @ GPUArraysCore ~/.julia/packages/GPUArraysCore/GMsgk/src/GPUArraysCore.jl:149\n배열의 개별 원소에 인덱스를 이용하여 접근하는 것을 스칼라 인덱싱이라고 한다. Array 에서의 경우는 아무 문제가 없지만 CuArray 의 경우는 위의 경고 메시지에서 나오듯이 큰 성능 하락을 불러일으킬 수 있다. Julia REPL 이나 jupyter 와 같은 상호작용 세션에서는 한번 경고를 발생시키고 수행하며, 이후에는 경고도 없이 수행하지만 실행 프로그램 상에서라면 에러를 발생시킨다. 아래의 코드를 파일로 저장하고 실행시켜보라.\n#! /usr/bin/env julia\nusing CUDA\n\na = CuArray([1])\na[1]=3\n아래와 같은 에러메시지가 출력된다.\nERROR: LoadError: Scalar indexing is disallowed.\nInvocation of setindex! resulted in scalar indexing of a GPU array.\n...\n스칼라 인덱싱을 허용하려면 @arrowscalar 매크롤르 사용한다.\nCUDA.@allowscalar a[1] += 1",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#고급-기능",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#고급-기능",
    "title": "CUDA.jl 배열 처리",
    "section": "2 고급 기능",
    "text": "2 고급 기능\n\n난수 발생\nCUDA 에서 제공하는 난수발생기를 사용 할 수 있다.\nCUDA.randn(Float64, 2, 1)\nCUDA 는 난수발생 모듈인 CURAND 를 포함하며 CURAND 는 lognormal 분포나 푸아송 분포에 대한 난수발생을 제공한다.\nCUDA.rand_logn(Float32, 1, 5; mean=2, stddev=20)\nCUDA.rand_poisson(UInt32, 1, 10; lambda=100)\n\n\n\n선형 대수\nCUDA 에는 자체 선형 대수 모듈인 CUBLAS 가 포함되어 있고 관련 함수들이 Julia 의 표준 선형 대수 모듈인 LinearAlgebra 에 포함되어 있다. 즉 LinearAlgebra 모듈을 통해 CuArray 에 대한 선형 대수 계산을 GPU 에서 수행 할 수 있다.\nC1= CuArray{Float32}([5 -1; -1 4]);\nlu(C1)\nLU{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}, CuArray{Int32, 1, CUDA.DeviceMemory}}\nL factor:\n2×2 CuArray{Float32, 2, CUDA.DeviceMemory}:\n  1.0  0.0\n -0.2  1.0\nU factor:\n2×2 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 5.0  -1.0\n 0.0   3.8\n CUBLAS에 존재하지만 (아직) LinearAlgebra 표준 라이브러리의 고수준 constructs 에 포함되지 않은 연산은 CUBLAS 서브모듈로 직접 접근 할 수 있다. 많은 연산이(예: cublasDdot) 더 상위 수준의 wrapper (예: dot) 를 사용할 수 있으므로 C 래퍼를 직접 호출할 필요가 없다.\nc1 = CuArray{Float32}([1, 2])\nCUBLAS.dot(2, c1, c1)\n5.0f0\n\n\n\nSolver\n선형 시스템의 해를 구하는 LAPACK 유사 기능은 CUDA 에 포함된 CUSOLVER 에 포함되어 있으며 LinearAlgebra 표준 라이브러리의 메서드를 통해서 접근 할 수 있다.\nA = CUDA.rand(3, 3)\nA = A * A'\ncholesky(A)\nCholesky{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}}\nU factor:\n3×3 UpperTriangular{Float32, CuArray{Float32, 2, CUDA.DeviceMemory}}:\n 0.685254  0.386212  0.527034\n  ⋅        0.977934  0.542067\n  ⋅         ⋅        0.182822\n\nA = CUDA.rand(3, 3)\nb = CUDA.rand(3)\nx=A\\b\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n  1.9668096\n  4.2866626\n -1.7602696\nA*x-b\n3-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n  2.9802322f-8\n -2.2351742f-8\n -2.7939677f-9\n\n\n\n희소 행렬\nCUDA 에 포함된 CUSPARSE 라이브러리를 통해 희소행렬을 다룰 수 있다. CUSPARSE 에서 희소행렬은 주로 CuSparseArray 객체를 사용하며 이 객체의 기능은 SparseArrays 패키지를 통해서도 접근 할 수 있다.\nsp1=sprand(10, 0.2)\ncsp1 = CuSparseVector(sp1)\n10-element CuSparseVector{Float64, Int32} with 2 stored entries:\nsparsevec(Int32[1, 2], [0.12468759985600719, 0.5027074361089312], 10)\n\n\n\nFFT\nCUDA 에는 고속 이산 푸리에 변환(FFT) 를 수행하는 CUFFT 가 포함되어 있으며 CUFFT 서브모듈로 접근한다. 사용법은 다음 절의 벤치마크 를 참고하라.\n\n\n\n벤치마크\n2차원 푸리에 변환에 대한 수행 시간을 확인해 보자.\nd1 = rand(Float32, 2048, 2048);\nc1 = CuArray(d1);\n@benchmark fft(d1)\nBenchmarkTools.Trial: 26 samples with 1 evaluation.\n Range (min … max):  115.318 ms … 233.668 ms  ┊ GC (min … max): 0.00% … 1.47%\n Time  (median):     187.126 ms               ┊ GC (median):    1.84%\n Time  (mean ± σ):   195.700 ms ±  36.154 ms  ┊ GC (mean ± σ):  1.59% ± 0.57%\n\n@benchmark CUDA.@sync fft($c1)\nBenchmarkTools.Trial: 6485 samples with 1 evaluation.\n Range (min … max):  433.504 μs …  11.076 ms  ┊ GC (min … max): 0.00% … 18.69%\n Time  (median):     741.453 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   765.796 μs ± 767.649 μs  ┊ GC (mean ± σ):  1.96% ±  1.84%",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_array_programming.html#메모리-관리",
    "href": "src/gpu/cuda.jl/cuda_jl_array_programming.html#메모리-관리",
    "title": "CUDA.jl 배열 처리",
    "section": "3 메모리 관리",
    "text": "3 메모리 관리\n다음을 보자\na1 = [1.0, 2.0, 3.0]\nca1 = CuArray{Float32}(a1)\na1 은 메모리상의 배열로, a1 에 대한 처리는 CPU 가 담당한다. 우리는 관례를 따라 앞으로 CPU 와 메인 메모리를 합쳐 host 라고 부르기로 하자. ca1 = CuArray{Float32}(a1) 은 a1 의 배열을 Float32 로 바꾸어 GPU 의 메모리로 옮긴다. GPU 와 GPU 의 메모리 등을 합쳐 device 라고 부르자. 굳이 Float32 타입으로 바꾸는 이유는 GPU 에서는 Float32 형식의 부동소수 연산이 Float64 보다 훨씬 빠르기 때문이다. Float64 타입이 무조건 필요한 경우가 아니라면 보통 Float32 를 사용한다. 이렇게 host 에서 device 로 데이터를 옮기는 것을 upload, device 에서 host 로 옮기는 것을 download 라고 하자.\n\n\n타입 보존 upload\nharr = Diagonal([1.0,2,3])\n여기서 harr 은 host 의 Float64 타입 희소행렬이다. 이것을 GPU 로 업로드 할 때\nCuArray(harr)\n라고 하면 CuArray{Float64} 타입 배열이 된다. Float32 타입으로 바꾸기 위해\nCuArray{Float64}(harr)\n라고 해도 되고 CUDA.jl 의 cu 함수를 이용해도 된다.\ncu(harr)\n\n\n\n통합 메모리\nCuArray 생성자와 cu 함수는 기본적으로 GPU에서만 액세스할 수 있는 장치 메모리를 할당한다. CPU 와 GPU 모두 접근할 수 있는 통합 메모리(Unified memory) 가 있으며 장치 드라이버가 데이터의 이동을 관장한다. 즉 필요에 따리 CPU 와 GPU 메모리에 존재하며 전송된다. 1 차원 배열에 대한 통합메모리는 다음과 같이 사용 할 수 있다. 아래에서 carr1, carr2, carr3 는 모두 같다.\narr1 = [1, 2, 3]\ncarr1 = CuArray{Float32, 1, CUDA.UnifiedMemory}(arr1)\ncarr2 = CuVector{Float32, CUDA.UnifiedMemory}(arr1)\ncarr3 = cu(arr1; unified=true)\n2차원 베열에 대한 통합메모리는 다음과 같이 사용 할 수 있다. 역시 cmat1, cmat2, cmat3 는 모두 같다.\nmat1 = [1 2; 3 4]\ncmat1 = CuMatrix{Float32, CUDA.UnifiedMemory}(mat1)\ncmat2 = CuArray{Float32, 2, CUDA.UnifiedMemory}(mat1)\ncmat3 = cu(mat1, unified=true)\n이렇게 하면 CPU 코드를 실행하거나 AbstractArray 로의 폴백(fallback)을 트리거하는 것에 대해 걱정할 필요 없이 애플리케이션의 일부를 점진적으로 포팅할 수 있으므로 코드를 GPU로 포팅하는 것이 상당히 쉬워질 수 있다. 그러나 통합 메모리는 GPU 메모리에서 페이지 인 혹은 페이지 아웃 을 해야 하며 비동기적으로 할당할 수 없으므로 이에 대한 비용이 발생할 수 있다. 이 비용을 줄이기 위해 CUDA.jl 은 커널에 전달할 때 통합 메모리를 prefetch 한다.\n최신 시스템(오픈소스 NVIDIA 드라이버가 있는 CUDA 12.2)에서는 CuArray 생성자나 cu 함수를 사용하여 통합 메모리를 명시적으로 할당하지 않고도 GPU에서 CPU 메모리에 액세스하여 그 반대의 작업도 가능하다.\njulia&gt; cpu = [1,2];\n\njulia&gt; gpu = unsafe_wrap(CuArray, cpu)\n2-element CuArray{Int64, 1, CUDA.UnifiedMemory}:\n 1\n 2\n\njulia&gt; gpu .+= 1;\n\njulia&gt; cpu\n2-element Vector{Int64}:\n 2\n 3\n현재 CUDA.jl은 여전히 ​​장치 메모리를 할당하는 것을 기본으로 하지만, 이는 향후 변경될 수 있습니다. 기본 동작을 변경하려면 default_memory 기본 설정을 device 대신 unified 또는 host로 설정할 수 있습니다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 배열 처리"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\ndfkl"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Science & Programming",
    "section": "",
    "text": "Julia 프로그래밍 언어 : Julia 언어\nJulia 언어를 이용한 수치해석 : 수치해석과 이미지 처리에 대해 다룹니다."
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "",
    "text": "CPU 에서 호출하고 GPU 에서 실행되는 함수를 커널(kernel) 이라고 한다. 커널은 보통의 julia 함수처럼 정의한다.\nfunction my_kernel()\n    return\nend\n커널을 실행하기 위해서는 @cuda 매크로를 사용한다.\n@cuda my_kernel\n위의 명령을 실행하면 my_kernel 함수가 컴파일되며 현재의 GPU 에서 실행된다.\n\n@cuda 매크로에 launch=false 인자를 전달하면 컴파일만 되고 실행하지 않으며 호출 가능한 객체를 리턴한다.\njulia&gt; k = @cuda launch=false my_kernel()\nCUDA.HostKernel for my_kernel()\n\njulia&gt; CUDA.registers(k)\n4\n\njulia&gt; k()",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "",
    "text": "CPU 에서 호출하고 GPU 에서 실행되는 함수를 커널(kernel) 이라고 한다. 커널은 보통의 julia 함수처럼 정의한다.\nfunction my_kernel()\n    return\nend\n커널을 실행하기 위해서는 @cuda 매크로를 사용한다.\n@cuda my_kernel\n위의 명령을 실행하면 my_kernel 함수가 컴파일되며 현재의 GPU 에서 실행된다.\n\n@cuda 매크로에 launch=false 인자를 전달하면 컴파일만 되고 실행하지 않으며 호출 가능한 객체를 리턴한다.\njulia&gt; k = @cuda launch=false my_kernel()\nCUDA.HostKernel for my_kernel()\n\njulia&gt; CUDA.registers(k)\n4\n\njulia&gt; k()",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-입력과-출력",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-입력과-출력",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "2 커널 입력과 출력",
    "text": "2 커널 입력과 출력\nGPU 커널은 반환값을 가질 수 없다. 즉 항상 return 이거나 return nothing 이어야 한다. 커널과 통신하는 유일한 방법은 CuArray 를 쓰는 것 뿐이다.\nfunction my_kernel(a)\n    a[1] = 42\n    return\nend\n\na = CuArray{Int}(undef, 1);\n@cuda my_kernel(a);\na\n42",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-구동-설정과-인덱싱",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-구동-설정과-인덱싱",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "3 커널 구동 설정과 인덱싱",
    "text": "3 커널 구동 설정과 인덱싱\n@cuda 를 통해 커널을 구동하면 단일 스레드만 시작되므로 그다지 유용하지 않다. (cuda?) 에 대한 threads 및 blocks 키워드 인수를 사용하면 다수의 스레드를 구동할 수 있으며, 커널 내에서는 인덱싱 내장 함수를 사용하여 각 스레드의 계산을 차별화 할 수 있다.\nfunction my_kernel(a)\n    i = threadIdx().x\n    a[i] = 42\n    return\nend\n\na = CuArray{Int}(undef, 5);\n@cuda threads=length(a) my_kernel(a);\na\n5-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n 42\n 42\n 42\n 42\n 42\n위에 표시된 대로, CUDA C 의 threadIdx 등의 값은 x, y, z 필드가 있는 NamedTuple 을 반환하는 함수로 사용할 수 있다. 이런 내장 함수는 1 부터 시작하는 인덱스를 반환한다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-컴파일-요건",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#커널-컴파일-요건",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "4 커널 컴파일 요건",
    "text": "4 커널 컴파일 요건\n사용자 정의 커널이 작동하기 위해서는 어떤 요건을 충족해야 한다.\n\n메모리는 GPU에서 접근 가능해야 한다. 이는 CuArray 등을 사용하여 강제할 수 있다. 사용자 지정 구조체는 해당 튜토리얼에서 설명한 대로 이식할 수 있다.\n런타임 디스패치는 불가하며 모든 함수 호출은 컴파일 타임에 결정되어야 합니다. 여기서 런타임 디스패치는 완전히 특정되지 않은 함수에 의해 도입될 수도 있다는 점에 유의해야 한다. Julia 매뉴얼 을 참고하고 다음 예를 보자.\n\nfunction my_inner_kernel!(f, t) # does not specialize\n    t .= f.(t)\nend\n\nfunction my_outer_kernel(f, a)\n    i = threadIdx().x\n    my_inner_kernel!(f, @view a[i, :])\n    return nothing\nend\n\na = CUDA.rand(Int, (2,2))\nid(x) = x\n\n@cuda threads=size(a, 1) my_outer_kernel(id, a)\n마지막 줄 실행에서 에러가 발생하는데 이는 아래와 같이 회피 할 수 있다.\nfunction my_inner_kernel!(f::F, t::T) where {F,T}\n    t .= f.(t)\nend\n\nfunction my_outer_kernel(f, a)\n    i = threadIdx().x\n    my_inner_kernel!(f, @view a[i, :])\n    return nothing\nend\n\na = CUDA.rand(Int, (2,2))\n\nid(x) = x\n\n@cuda threads=size(a, 1) my_outer_kernel(id, a)\n단지 첫번째 함수 my_inner_kernel! 의 인자의 함수가 파라미터로 특정되었을 뿐이다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#동기화-synchronization",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#동기화-synchronization",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "5 동기화 (Synchronization)",
    "text": "5 동기화 (Synchronization)\n블록에서 스레드를 동기화하려면 sync_threads() 함수를 사용한다. predicate 를 취하는 보다 고급 변형도 사용 가능하다.\n\nsync_threads_count(pred) : pred 가 true 인 스레드의 갯수를 반환한다.\nsync_threads_and(pred) : 모든 스레드에서 pred 가 참이면 true 를 반환한다.\nsync_threads_or(pred) : 어떤 스레드에서 pred 가 참이면 true 를 반환한다.\n\n여러 스레드 동기화 장벽을 유지하려면 장벽을 식별하는 정수 인수를 취하는 barrier_sync 함수를 사용한다.\n워프에서 레인을 동기화하려면 sync_warp() 함수를 사용합니다. 이 함수는 참여할 레인을 선택하는 마스크를 취합니다(기본값은 FULL_MASK).\n실행 장벽이 아닌 메모리 장벽만 필요한 경우 펜스 내장 함수를 사용합니다.\n\nthreadfence_block : 블럭 내의 모든 쓰레드에서 메모리 정렬을 보장한다.\nthreadfence : 디바이스 내의 모든 쓰레드에서 메모리 정렬을 보장한다.\nthreadfence_system : 호스트 스레드와 peer 디바이스를 포함한 모든 스레드에서 메모리 정렬을 보장한다.\n\n\n\n공유 메모리 (Shared memory)\n스레드 간 통신을 위해 공유 메모리로 백업된 디바이스 배열은 CuStaticSharedArray 함수를 통해 할당될 수 있다. 다음은 배열의 순서를 바꾸는 커널이다. 커널 내의 b 가 스레드간 통신을 위해 공유 메모리로 백업된 배열이다.\nfunction reverse_kernel(a::CuDeviceArray{T}) where T\n    i = threadIdx().x\n    b = CuStaticSharedArray(T, 2)\n    b[2-i+1] = a[i]\n    sync_threads()\n    a[i] = b[i]\n    return\nend\n\na = cu([1,2])\n@cuda threads = 2 reverse_kernel(a)\n결과를 출력해보면 a 의 순서가 바뀌었음을 알 수 있다.\n\n공유 메모리의 크기를 미리 알 수 없고 각 크기에 대해 커널을 다시 컴파일하고 싶지 않은 경우 대신 CuDynamicSharedArray 타입을 사용할 수 있다. 이를 위해서는 공유 메모리의 크기(바이트)를 커널에 인수로 전달해야 한다.\nfunction reverse_kernel(a::CuDeviceArray{T}) where T\n    i = threadIdx().x\n    b = CuDynamicSharedArray(T, length(a))\n    b[length(a)-i+1] = a[i]\n    sync_threads()\n    a[i] = b[i]\n    return\nend\n\na = cu([1,2,3])\n@cuda threads=length(a) shmem=sizeof(a) reverse_kernel(a)\n동적 공유 메모리를 사용하는 다수의 배열이 필요한 경우 후속 CuDynamicSharedArray 생성자에 공유 메모리 시작부터 바이트 단위의 오프셋을 나타내는 오프셋 매개변수를 전달한다. @cuda 에 대한 shmem 키워드는 모든 배열에서 사용하는 총 공유 메모리 양이어야 합니다.\n\n\n\n경계 확인\n기본적으로 CuDeviceArray 를 인덱싱하면 경계 검사를 수행하고 인덱스가 경계를 벗어나면 오류를 발생시키는데 이는 비용이 많이 드는 작업이므로 인덱스가 경계 내에 있다는 것이 확실하다면 일반적인 배열과 마찬가지로 @inbounds 를 사용 할 수 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#표준-출력",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#표준-출력",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "6 표준 출력",
    "text": "6 표준 출력\nCUDA.jl 커널은 아직 Julia의 표준 입출력과 통합되지 않았지만 커널에서 표준 출력으로 인쇄하기 위한 몇 가지 기본 기능을 제공한다.\n\n@cuprintf: 표준 출력으로 형식화된 출력을 내보낸다.\n@cuprint 와 @cuprintln : 문자를 포함한 값을 표준 출력으로 내보낸다.\n@cushow : 객체의 이름과 값을 출력한다.\n\n@cuprintf 매크로는 모든 형식 옵션을 지원하지 않는다. 자세한 내용은 printf 에 대한 NVIDIA 설명서를 참조하라. @cuprintln 과 CUDA.jl 을 통해 모든 값을 적절한 문자열 표현으로 변환하는 것이 더 편리한 경우가 많다.\njulia&gt; @cuda threads=2 (()-&gt;(@cuprintln(\"Hello, I'm thread $(threadIdx().x)!\"); return))()\nHello, I'm thread 1!\nHello, I'm thread 2!\n 단순히 값만 출력하길 원한다면, which can be useful during debugging, @cushow 를 사용하라.\njulia&gt; @cuda threads=2 (()-&gt;(@cushow threadIdx().x; return))()\n(threadIdx()).x = 1\n(threadIdx()).x = 2\n이것들은 매우 제한된 수의 유형만 지원한다는 점에 유의하라. 따라서 디버깅 목적으로만 사용하는 것이 좋다. P",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#난수-발생",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#난수-발생",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "7 난수 발생",
    "text": "7 난수 발생\n커널에서 rand 혹은 randn 함수를 통해 난수 샘플을 얻을 수 있으며 이 때 GPU-호환 난수 발생기(GPU-compatible random number generator) 가 사용된다. API는 CPU에서 사용되는 난수 생성기와 매우 유사하지만 병렬 RNG의 설계에서 비롯된 몇 가지 차이점과 고려해야할 항이 있다.\n\n기본 RNG는 글로벌 상태를 사용한다. 여러개의 인스턴스를 사용하는 것은 정의되지 않은 동작이다.\n커널은 host 에서 전달된 고유한 seed 로 RNG를 자동으로 시드하여 동일한 커널을 여러 번 호출해도 다른 결과가 생성되도록 한다.\nRandom.seed! 를 호출하여 seed 를 수동으로 지정하는 것이 가능하지만 RNG는 워프 공유 상태를 사용하므로 워프당 최소 하나의 스레드가 seed 되어야 하며 워프 내의 모든 seed 는 동일해야 한다.\n후속 커널 호출이 난수를 계속 발생시켜야 하는 경우 seed 뿐만 아니라 카운터 값도 Random.seed! 를 사용하여 수동으로 구성해야 한다. 여기에 대한 예는 CUDA.jl 의 host 측 RNG를 참조하라.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#atomics",
    "href": "src/gpu/cuda.jl/cuda_jl_kernel_programming.html#atomics",
    "title": "CUDA.jl 커널 프로그래밍",
    "section": "8 Atomics",
    "text": "8 Atomics\nCUDA.jl 은 두 레벨의 추상화를 통해 아토믹 연산을 제공한다\n\n저수준 : atomic_ 함수는 직접 hardware instruction 에 매핑한다.\n고수준 : CUDA.@atomic 은 편리한 성분별 연산(element-wise operation) 을 제공한다.\n\n저수준 추상화는 안정적이고 이후의 동작을 변경하지 않으므로 아토믹 연산을 사용하는 가장 안전한 방법이다. 그러나 이 경우 인터페이스는 제한적이며, 하드웨어가 제공하는 것만 지원하고, 입력 타입이 일차할것을 요구한다. CUDA.@atomic API는 훨씬 더 사용자 친화적이지만 Julia Base 의 @atomic 매크로와 통합되면 어느 시점에서 사라질 것이다.\n\n\n저수준\n저수준 아토믹 내재 함수는 포인터 입력을 사용하며 이는 CuArray 에서 포인터 함수를 호출하여 얻을 수 있다.\njulia&gt; function atomic_kernel(a)\n           CUDA.atomic_add!(pointer(a), Int32(1))\n           return\n       end\n\njulia&gt; a = cu(Int32[1])\n1-element CuArray{Int32, 1, CUDA.DeviceMemory}:\n 1\n\njulia&gt; @cuda atomic_kernel(a)\n\njulia&gt; a\n1-element CuArray{Int32, 1, CUDA.DeviceMemory}:\n 2\n지원되는 아토믹 연산은 다음과 같다.\n\n이항연산 : add, sub, and, or, xor, min, max, xchg\nNVIDIA-특정적인 이항연산 : inc, dec\n비교와 교환 : cas\n\n자세한 유형 지원 및 하드웨어 요구 사항은 해당 내장 함수의 설명서를 참조하라.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA.jl",
      "CUDA.jl 커널 프로그래밍"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#gpu-사용의-이점",
    "href": "src/gpu/cuda/01_introduction.html#gpu-사용의-이점",
    "title": "Introduction",
    "section": "2 GPU 사용의 이점",
    "text": "2 GPU 사용의 이점\n\nCPU 와 GPU 의 목적\n\n\n\n표 1: CPU 와 GPU 의 비교\n\n\n\n\n\n\nCPU\nGPU\n\n\n\n\n지향\n각 코어의 성능 향상\n병렬 처리 성능 향상\n\n\n코어 수\n1 ~ 수십개\n수백 ~ 수천개\n\n\n개별 코어의 성능\n높다\n낮다\n\n\n구조\nSISD, MIMD\nSIMT\n\n\n공간 분배\n캐시 및 제어 유닛에 많이\n연산 유닛에 많이\n\n\n메모리 크기\n수 GB 이상\n수 GB 이상\n\n\n메모리 접근\n접근 지연 시간 최적화\n메모리 대역폭 최대화\n\n\n\n\n\n\n\n이것을 정리하면 다음과 같다.\n\n\n\n\n\n\n\n장치\n기본 목적\n\n\n\n\nCPU\n스레드를 최대한 빠르게 처리. 최대 수십개의 스레드를 병렬적으로 처리 할 수 있음.\n\n\nGPU\n수천개의 스레드를 를 병렬적으로 최대한 빠르게. 단일 스레드는 느리더라도 throughput 을 최대한으로\n\n\n\n\n\nCPU 에 비해 상대적으로 캐싱과 흐름 제어보다 데이터 처리에 더 많은 트랜지스터를 사용한다.\n아래 그림은 CPU 와 GPU 칩의 resource 분배를 보여준다. (source : CUDA C Programming guide )\n\n\n\n\n\n\n\n\n그림 1: The GPU Devotes More Transistors to Data Processing\n\n\n\n\n데이터 처리에 더 많은 트랜지스터를 할당하면 예를 들어 부동 소수점 계산의 병렬 계산처리에 유용하다. GPU는 긴 메모리 액세스 대기 시간을 피하기 위해 대용량 데이터 캐시와 복잡한 흐름 제어에 의존하는 대신 계산을 통해 메모리 액세스 대기 시간을 숨길 수 있다. 대용량 데이터 케시와 복잡한 흐름제어 둘 다 트랜지스터 측면에서 비용이 많이 든다.\n\n일반적으로 애플리케이션은 병렬적인 부분과 순차적인 부분이 혼합되어 있으므로 시스템은 전반적인 성능을 극대화하기 위해 GPU와 CPU를 혼합하여 설계된다. 높은 수준의 병렬성을 갖춘 애플리케이션은 GPU의 이러한 대규모 병렬적 특성을 활용하여 CPU보다 더 높은 성능을 달성할 수 있다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#cuda-a-general-purpose-parallel-computing-platform-and-programming-model",
    "href": "src/gpu/cuda/01_introduction.html#cuda-a-general-purpose-parallel-computing-platform-and-programming-model",
    "title": "Introduction",
    "section": "3 CUDA®: A General-Purpose Parallel Computing Platform and Programming Model",
    "text": "3 CUDA®: A General-Purpose Parallel Computing Platform and Programming Model\n\nCUDA : 범용 병렬 컴퓨팅 플랫폼이자 NVIDIA GPU의 병렬 컴퓨팅 엔진을 활용하는 프로그래밍 모델\n\n\n\n\n\n\n\n\n그림 2: GPU Computing Applications. CUDA is designed to support various languages and application programming interfaces.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#a-scalable-programming-model",
    "href": "src/gpu/cuda/01_introduction.html#a-scalable-programming-model",
    "title": "Introduction",
    "section": "4 A Scalable Programming Model",
    "text": "4 A Scalable Programming Model\n\nMulticore(CPU) 와 Manycore(GPU) 의 등장으로 처리 장치의 주류가 병렬 시스템이 되었으며, 3D 그래픽 응용 프로그램이 병렬성을 투명하게 확장하여 코어 수가 매우 다양한 많은 코어 GPU로 확장되는 것처럼 이런 병렬성을 투명하게 확장하여 증가하는 프로세서 코어 수를 활용하는 애플리케이션 소프트웨어를 개발하는 것이 목표가 되었다.\nCUDA 병렬 프로그래밍 모델은 C 와 같은 표준 프로그래밍 언어에 익숙한 프로그래머에게 낮은 학습 곡선을 유지하면서 이러한 과제를 극복하도록 설계되었다.\n핵심에는 스레드 그룹, 공유 메모리, 배리어 동기화의 계층 구조라는 세 가지 핵심 추상화가 있으며, 이는 단순히 최소한의 언어 확장 세트로 프로그래머에게 노출된다.\n이러한 추상화는 거친 데이터 병렬성과 작업 병렬성 내에 중첩된 미세한 데이터 병렬성과 스레드 병렬성을 제공한다. 이는 프로그래머가 문제를 스레드 블록으로 독립적으로 병렬로 해결할 수 있는 거친 하위 문제로 분할하고, 각 하위 문제를 블록 내의 모든 스레드가 협력하여 병렬로 해결할 수 있는 더 미세한 부분으로 분할하도록 안내한다.\n이 분해는 각 하위 문제를 해결할 때 스레드가 협력할 수 있도록 하여 언어 표현력을 보존하고 동시에 자동 확장성을 가능하게 합니다. 실제로 각 스레드 블록은 GPU 내의 사용 가능한 모든 멀티프로세서에서 어떤 순서로든 동시에 또는 순차적으로 예약할 수 있으므로 컴파일된 CUDA 프로그램은 그림 3 에서 설명한 대로 아무리 많은 멀티프로세서에서나 실행할 수 있으며 런타임 시스템만 물리적 멀티프로세서 수를 알면 된다.\n이 확장 가능한 프로그래밍 모델을 사용하면 GPU 아키텍처가 멀티프로세서 수와 메모리 파티션을 간단히 확장하여 광범위한 시장 범위에 걸쳐 확장할 수 있다. 고성능 매니아용 GeForce GPU와 전문가용 Quadro 및 Tesla 컴퓨팅 제품부터 다양한 저렴한 주류 GeForce GPU까지 다양합니다(모든 CUDA 지원 GPU 목록은 CUDA 지원 GPU 참조).\n\n\n\n세가지 중요한 추상화\n\n스레드 그룹의 계층 구조\n공유 메모리\n배리어 동기화\n\n이러한 추상화는 세분화된(fine-grained) 데이터 병렬 처리 및 스레드 병렬 처리를 제공하며 거친(coarse-grained) 데이터 병렬 처리 및 작업 병렬 처리 내에 nested 되어 있다. 이 추상화들은 프로그래머가 문제를 스레드 블록에 의해 병렬로 독립적으로 해결될 수 있는 거친 하위 문제로 분할하고 각 하위 문제를 블록 내의 모든 스레드가 병렬로 공동으로 해결할 수 있는 더 미세한 부분으로 분할하도록 안내한다.\n\n\n\n\n\n\n\n그림 3: Automatic Scalability",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html",
    "href": "src/gpu/cuda/03_programming_interface.html",
    "title": "Programming Interface",
    "section": "",
    "text": "CUDA C++는 C++ 프로그래밍 언어에 익숙한 사용자에게 장치에서 실행할 프로그램을 쉽게 작성할 수 있는 단순한 경로를 제공한다. C++ 언어에 대한 최소한의 확장 세트와 런타임 라이브러리로 구성되며 핵심 언어 확장은 Programming Model에 소개되었다. 프로그래머는 커널을 C++ 함수로 정의하고 새로운 구문을 사용하여 함수가 호출될 때마다 그리드와 블록 차원을 지정할 수 있다. 모든 확장에 대한 전체 설명은 C++ 언어 확장에서 찾을 수 있다. 이러한 확장 중 일부를 포함하는 모든 소스 파일은 NVCC 컴파일에 설명된 대로 nvcc 로 컴파일해야 합니다.\n런타임에 대해서는 CUDA 런타임 을 참고하라. 호스트에서 실행되어 장치 메모리를 할당 및 할당 해제하고, 호스트 메모리와 장치 메모리 간에 데이터를 전송하고, 여러 장치가 있는 시스템을 관리하는 등의 작업을 수행하는 C 및 C++ 함수를 제공한다. 런타임에 대한 전체 설명은 CUDA Reference Manumal 에서 찾을 수 있다.\n런타임은 하위 수준 C API인 CUDA 드라이버 API 위에 구축되며 애플리케이션에서도 액세스할 수 있다. 드라이버 API는 CUDA 컨텍스트(장치의 호스트 프로세스와 유사) 및 CUDA 모듈(장치의 동적으로 로드된 라이브러리와 유사)과 같은 하위 수준 개념을 노출하여 추가적인 제어 수준을 제공한다. 대부분의 애플리케이션은 드라이버 API를 사용하지 않는다. 이 추가적인 제어 수준이 필요하지 않고 런타임을 사용할 때 컨텍스트 및 모듈 관리가 암묵적이기 때문에 코드가 더 간결해진다. 런타임은 드라이버 API와 상호 운용 가능하므로 일부 드라이버 API 기능이 필요한 대부분의 애플리케이션은 기본적으로 런타임 API를 사용하고 필요한 경우에만 드라이버 API를 사용할 수 있다. 드라이버 API는 드라이버 API에 소개되어 있으며 Reference manual 에 자세히 설명되어 있습니다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html#sec-cuda_nvcc_compile",
    "href": "src/gpu/cuda/03_programming_interface.html#sec-cuda_nvcc_compile",
    "title": "Programming Interface",
    "section": "1 NVCC 컴파일",
    "text": "1 NVCC 컴파일\n커널은 PTX 라고 하는 CUDA 명령어 세트 아키텍처를 사용하여 작성할 수 있으며, 이는 PTX Reference manual 에 설명되어 있다. 그러나 일반적으로 C++와 같은 고급 프로그래밍 언어를 사용하는 것이 더 효과적이다. 두 경우 모두 커널은 nvcc 에서 바이너리 코드로 컴파일하여 장치에서 실행해야 한다.\nnvcc 는 C++ 또는 PTX 코드를 컴파일하는 프로세스를 간소화하는 컴파일러 드라이버이다. 간단하고 친숙한 명령줄 옵션을 제공하고 다양한 컴파일 단계를 구현하는 도구 모음을 호출하여 실행한다. 이 섹션에서는 nvcc 워크플로 및 명령 옵션에 대한 개요를 설명한다. 전체 설명은 nvcc user manual 에서 찾을 수 있다.\n\n\n1.1 컴파일 workflow\nnvcc 로 컴파일된 소스 파일에는 호스트 코드(즉, 호스트에서 실행되는 코드)와 디바이스 코드(즉, 디바이스에서 실행되는 코드)가 혼합되어 포함될 수 있다. nvcc 의 기본 워크플로는 디바이스 코드를 호스트 코드에서 분리한 다음 다음을 수행하는 것으로 구성됩니다.\n\n디바이스 코드를 어셈블리 형태(PTX 코드) 및/또는 바이너리 형태(cubin 객체)로 컴파일하고,\nKernels에서 도입된 &lt;&lt;&lt;...&gt;&gt;&gt; 구문을 필요한 CUDA 런타임 함수 호출로 대체하여 호스트 코드를 수정하여 PTX 코드 및/또는 cubin 객체에서 각 컴파일된 커널을 로드하고 시작한다.\n\n수정된 호스트 코드는 다른 도구를 사용하여 컴파일할 수 있는 C++ 코드로 출력되거나, 마지막 컴파일 단계에서 nvcc 가 호스트 컴파일러를 호출하여 직접 object code 로 출력된다. 이제 애플리케이션은 다음을 수행할 수 있다.\n\n컴파일된 호스트 코드에 링크(가장 일반적인 경우)하거나\n수정된 호스트 코드(있는 경우)를 무시하고 CUDA 드라이버 API(드라이버 API 참조)를 사용하여 PTX 코드 또는 cubin 객체를 로드하고 실행한다.\n\n\n\nJIT 컴파일\n런타임에 애플리케이션에서 로드한 모든 PTX 코드는 장치 드라이버에 의해 바이너리 코드로 추가로 컴파일된다. 이를 JIT(just-in-time) 컴파일이라고 한다. JIT 컴파일은 애플리케이션 로드 시간을 늘리지만 애플리케이션이 각 새 장치 드라이버와 함께 제공되는 모든 새 컴파일러 개선 사항의 이점을 누릴 수 있도록 한다. 또한 애플리케이션이 컴파일된 당시 존재하지 않았던 장치에서 애플리케이션을 실행할 수 있는 유일한 방법이기도 하다(애플리케이션 호환성에서 자세히 설명).\n장치 드라이버가 일부 애플리케이션에 대한 일부 PTX 코드를 JIT 컴파일할 때, 이후 애플리케이션 호출에서 컴파일을 반복하지 않기 위해 생성된 바이너리 코드의 사본을 자동으로 캐시한다. 캐시(컴퓨트 캐시라고 함)는 장치 드라이버가 업그레이드될 때 자동으로 무효화되므로 애플리케이션은 장치 드라이버에 내장된 새로운 JIT 컴파일러의 개선 사항의 이점을 누릴 수 있다.\n환경 변수는 CUDA 환경 변수에 설명된 대로 적시 컴파일을 제어하는 ​​데 사용할 수 있습니다.\nnvcc 를 사용하여 CUDA C++ 장치 코드를 컴파일하는 대신 NVRTC를 사용하여 런타임에 CUDA C++ 장치 코드를 PTX로 컴파일할 수 있다. NVRTC는 CUDA C++용 런타임 컴파일 라이브러리이다. 자세한 내용은 NVRTC 사용자 가이드에서 확인할 수 있다.\n\n\n\n\n1.2 바이너리 호환성\n이진 코드는 아키텍처에 따라 다르다. cubin 객체는 대상 아키텍처를 지정하는 컴파일러 옵션 -code 를 사용하여 생성된다. 예를 들어, -code=sm_80 으로 컴파일하면 Compute Capability 8.0의 장치에 대한 이진 코드가 생성된다. 이진 호환성은 한 마이너 리비전에서 다음 리비전으로 보장되지만, 한 마이너 리비전에서 이전 리비전으로 또는 주요 리비전 간에는 보장되지 않는다. 즉, 컴퓨팅 기능 X.y에 대해 생성된 cubin 객체는 z≥y인 컴퓨팅 기능 X.z의 장치에서만 실행된다.\n\n\n\n\n\n\n\n경고\n\n\n\n바이너리 호환성은 데스크톱에서만 지원되며 Tegra에서는 지원되지 않는다. 또한 데스크톱과 Tegra 간의 바이너리 호환성도 지원되지 않는다.\n\n\n\n\n\n1.3 PTX 호환성\n일부 PTX 명령어는 고성능 장치에서만 지원된다. 예를 들어, Warp Shuffle(https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions) 함수는 compute capability 5.0 이상의 장치에서만 지원된다. -arch 컴파일러 옵션은 C++를 PTX 코드로 컴파일할 때 가정하는 compute capability 를 지정합니다. 따라서 예를 들어 Warp Shuffle이 포함된 코드는 -arch=compute_50 (또는 그 이상)으로 컴파일해야 한다.\n특정 compute capability 를 위해 생성된 PTX 코드는 항상 더 크거나 같은 컴퓨팅 기능의 바이너리 코드로 컴파일할 수 있다. 이전 PTX 버전에서 컴파일된 바이너리는 일부 하드웨어 기능을 사용하지 못할 수 있다. 예를 들어, compute capability 6.0(Pascal)을 위해 생성된 PTX에서 컴파일된 compute capability 7.0(Volta) 장치를 대상으로 하는 바이너리는 Pascal에서 사용할 수 없는 Tensor Core 명령어를 사용하지 않는다. 결과적으로 최종 바이너리는 최신 버전의 PTX를 사용하여 바이너리를 생성한 경우보다 성능이 떨어질 수 있다.\n아키텍처 조건부 기능을 대상으로 컴파일된 PTX 코드는 정확히 동일한 물리적 아키텍처에서만 실행되고 다른 곳에서는 실행되지 않습니다. 아치텍쳐 조건부 PTX 코드는 이전 혹은 이후와 호환되지 않는다. sm_90a 또는 compute_90a 로 컴파일된 예제 코드는 compute capability 9.0 이 있는 장치에서만 실행되며 이전 혹은 이후와 호환되지 않는다.\n\n\n\n1.4 응용 프로그램 호환성\n특정 compute capability 의 장치에서 코드를 실행하려면 애플리케이션은 바이너리 호환성 및 PTX 호환성에서 설명한 대로 이 compute capability 과 호환되는 바이너리 또는 PTX 코드를 로드해야 한다. 특히, 더 높은 컴퓨팅 기능을 갖춘 미래 아키텍처에서 코드를 실행하려면(아직 바이너리 코드를 생성할 수 없는 경우) 애플리케이션은 이러한 장치에 대해 JIT 컴파일되는 PTX 코드를 로드해야 합니다(JIT 컴파일 참조).\nCUDA C++ 애플리케이션에 어떤 PTX와 바이너리 코드가 포함되는지는 nvcc 사용자 매뉴얼에 자세히 설명된 대로 -arch 및 -code 컴파일러 옵션 또는 -gencode 컴파일러 옵션에 의해 제어된다. 예를 들어보자.\nnvcc x.cu\n        -gencode arch=compute_50,code=sm_50\n        -gencode arch=compute_60,code=sm_60\n        -gencode arch=compute_70,code=\\\"compute_70,sm_70\\\"\n위의 컴파일은 compute capability 5.0 및 6.0(첫 번째 및 두 번째 -gencode 옵션)과 호환되는 바이너리 코드와 compute capability 7.0(세 번째 -gencode 옵션)과 호환되는 PTX 및 바이너리 코드를 embeds 한다.\n호스트 코드는 런타임에 로드하고 실행할 가장 적합한 코드를 자동으로 선택하도록 생성됩니다. 위의 예에서 이는 다음과 같다.\n\ncompute capability 5.0 및 5.2가 있는 장치의 경우 5.0 바이너리 코드,\ncompute capability 6.0 및 6.1이 있는 장치의 경우 6.0 바이너리 코드,\ncompute capability 7.0 및 7.5가 있는 장치의 경우 7.0 바이너리 코드,\ncompute capability 8.0 및 8.6이 있는 장치의 경우 런타임에 바이너리 코드로 컴파일되는 PTX 코드.\n\n\nx.cu 는 예를 들어, 워프 감소 연산을 사용하는 최적화된 코드 경로를 가질 수 있으며, 이는 compute capability 8.0 이상의 장치에서만 지원됩니다. __CUDA_ARCH__ 매크로는 compute capability 에 따라 다양한 코드 경로를 구분하는 데 사용할 수 있다. 이는 장치 코드에 대해서만 정의됩니다. 예를 들어 -arch=compute_80 으로 컴파일할 때 __CUDA_ARCH__ 는 800 과 같다.\nx.cu 가 sm_90a 또는 compute_90a 를 사용하여 아키텍처 조건부 기능 예제에 대해 컴파일된 경우, 코드는 compute capability 9.0이 있는 장치에서만 실행할 수 있다.\n드라이버 API를 사용하는 애플리케이션은 코드를 컴파일하여 파일을 분리하고 런타임에 가장 적합한 파일을 명시적으로 로드하여 실행해야 한다.\nVolta 아키텍처는 GPU에서 스레드가 예약되는 방식을 변경하는 독립 스레드 스케줄링을 도입합니다. 이전 아키텍처에서 SIMT 스케줄링 의 특정 동작에 의존하는 코드의 경우 독립 스레드 스케줄링은 참여 스레드 세트를 변경하여 잘못된 결과를 초래할 수 있다. Independent Thread Scheduling 에서 자세히 설명한 시정 조치를 구현하는 동안 마이그레이션을 지원하기 위해 Volta 개발자는 컴파일러 옵션 조합 -arch=compute_60  -code=sm_70을 사용하여 Pascal의 스레드 스케줄링을 선택할 수 있습니다.\nnvcc 사용자 설명서에는 -arch, -code 및 -gencode 컴파일러 옵션에 대한 다양한 약어가 나와 있다. 예를 들어, -arch=sm_70 은 -arch=compute_70 -code=compute_70,sm_70 (-gencode arch=compute_70,code=\\\"compute_70,sm_70\\\" 과 동일)의 약어입니다.\n\n\n\n1.5 C++ 호환성\n컴파일러의 프런트 엔드는 C++ 구문 규칙에 따라 CUDA 소스 파일을 처리합니다. 호스트 코드에는 전체 C++가 지원된다. 그러나 C++ 언어 지원 에 설명된 대로 C++의 하위 집합만 장치 코드에 대해 완전히 지원된다.\n\n\n\n1.6 64 비트 호환성\nnvcc의 64비트 버전은 64비트 모드에서 디바이스 코드를 컴파일한다(즉, 포인터는 64비트이다). 64비트 모드에서 컴파일된 디바이스 코드는 64비트 모드에서 컴파일된 호스트 코드에서만 지원된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/gpu/cuda/03_programming_interface.html#cuda-runtime",
    "href": "src/gpu/cuda/03_programming_interface.html#cuda-runtime",
    "title": "Programming Interface",
    "section": "2 CUDA Runtime",
    "text": "2 CUDA Runtime\n런타임은 cudart 라이브러리에서 구현되며, cudart.lib 또는 libcudart.a 를 통해 정적으로 또는 cudart.dll 또는 libcudart.so 를 통해 동적으로 응용프로그램에 링크된다. 동적 링크에 cudart.dll 및/또는 cudart.so 가 필요한 애플리케이션은 일반적으로 이를 애플리케이션 설치 패키지의 일부로 포함한다. CUDA 런타임 심볼의 주소는 CUDA 런타임의 동일한 인스턴스에 링크하는 구성 요소 사이에서만 안전하게 전달할 수 있다.\n모든 진입점에는 cuda 라는 접두사가 붙습니다.\n이기종 프로그래밍(Heterogeneous Programming) 에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리가 있는 호스트와 디바이스로 구성된 시스템을 가정한다. 장치 메모리 에서 장치 메모리를 관리하는 데 사용되는 런타임 함수에 대해 소개한다.\n공유 메모리 에서 스레드 계층에서 도입된 공유 메모리를 사용하여 성능을 극대화하는 방법을 보인다.\n페이지 잠금 호스트 메모리 에서는 호스트와 장치 메모리 간의 데이터 전송과 커널 실행을 겹치게 하는 데 필요한 페이지 잠금 호스트 메모리를 소개한다.\n비동기 동시 실행 에서는 시스템의 다양한 수준에서 비동기 동시 실행을 가능하게 하는 데 사용되는 개념과 API를 설명한다.\n다중 디바이스 시스템 에서는 프로그래밍 모델이 동일한 호스트에 연결된 여러 장치가 있는 시스템으로 확장되는 방식을 보여준다.\n오류 검사 에서는 런타임에서 생성된 오류를 올바르게 검사하는 방법을 설명한다.\n호출 스택 은 CUDA C++ 호출 스택을 관리하는 데 사용되는 런타임 함수를 소개한다.\n텍스처 및 표면 메모리 에서는 장치 메모리에 액세스하는 또 다른 방법을 제공하는 텍스처 및 표면 메모리 공간을 설명한다. 또한 GPU 텍스처링 하드웨어의 하위 집합을 보여준다.\n그래픽 상호 운용성 은 런타임이 두 가지 주요 그래픽 API인 OpenGL 및 Direct3D 와 상호 운용하기 위해 제공하는 다양한 함수를 소개한다\n\n\n2.1 초기화\nCUDA 12.0부터 cudaInitDevice() 및 cudaSetDevice() 를 호출하면 지정된 디바이스와 연관된 런타임 및 기본 컨텍스트를 초기화합니다. 이러한 호출이 없으면 런타임은 암묵적으로 Device 0 을 사용하고 필요에 따라 자체로 초기화하여 다른 런타임 API 요청을 처리합니다. 런타임 함수 호출의 타이밍을 지정하고 첫 번째 호출의 오류 코드를 런타임으로 해석할 때 이 점을 염두에 두어야 합니다. 12.0 이전에는 cudaSetDevice() 가 런타임을 초기화하지 않았고 애플리케이션은 종종 no-op 런타임 호출 cudaFree(0) 를 사용하여 런타임 초기화를 다른 API 활동에서 분리했습니다(타이밍과 오류 처리를 위해).\n런타임은 시스템의 각 장치에 대한 CUDA 컨텍스트를 만듭니다(CUDA 컨텍스트에 대한 자세한 내용은 컨텍스트 참조). 이 컨텍스트는 이 디바이스의 기본 컨텍스트이며 이 디바이스 에서 활성 컨텍스트가 필요한 첫 번째 런타임 함수에서 초기화됩니다. 이 컨텍스트는 애플리케이션의 모든 호스트 스레드에서 공유됩니다. 이 컨텍스트 생성의 일부로, 필요한 경우 디바이스 코드가 JIT 컴파일 되고 디바이스 메모리에 로드됩니다. 이 모든 것이 투명하게 이루어집니다. 예를 들어 드라이버 API 상호 운용성을 위해 필요한 경우 디바이스의 기본 컨텍스트는 런타임 및 드라이버 API 간 상호 운용성에 설명된 대로 드라이버 API에서 액세스할 수 있습니다.\n호스트 스레드가 cudaDeviceReset() 을 호출하면 호스트 스레드가 현재 작동하는 디바이스의 기본 컨텍스트(즉, 디바이스 선택에서 정의된 현재 디바이스)가 파괴됩니다. 이 디바이스를 현재 디바이스로 갖는 호스트 스레드가 다음에 호출하는 런타임 함수는 이 장치에 대한 새 기본 컨텍스트를 생성합니다.\n\n\n\n\n\n\n\n노트\n\n\n\nCUDA 인터페이스는 호스트 프로그램 시작 중에 초기화되고 호스트 프로그램 종료 중에 파괴되는 전역 상태를 사용합니다. CUDA 런타임과 드라이버는 이 상태가 유효하지 않은지 감지할 수 없으므로 main) 이후 프로그램 시작 또는 종료 중에 이러한 인터페이스 중 하나를 명시적으로든 암묵적으로든 사용하면 정의되지 않은 동작이 발생합니다.\nCUDA 12.0부터 cudaSetDevice() 는 이제 호스트 스레드에 대한 현재 장치를 변경한 후 런타임을 명시적으로 초기화합니다. 이전 버전의 CUDA는 cudaSetDevice() 이후 첫 번째 런타임 호출이 이루어질 때까지 새 장치에서 런타임 초기화를 지연했습니다. 이 변경으로 인해 이제 초기화 오류에 대해 cudaSetDevice() 의 반환값을 확인하는 것이 매우 중요합니다.\n참조 매뉴얼의 오류 처리 및 버전 관리 섹션의 런타임 함수는 런타임을 초기화하지 않습니다.\n\n\n\n\n\n2.2 장치 메모리\n이기종 프로그래밍에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리를 가진 호스트와 디바이스로 구성된 시스템을 가정합니다. 커널은 디바이스 메모리에서 작동하므로 런타임은 디바이스 메모리를 할당, 할당 해제, 복사하고 호스트 메모리와 디바이스 메모리 간에 데이터를 전송하는 기능을 제공합니다.\n디바이스 메모리는 선형 메모리(Linear meomry - ?) 또는 CUDA 배열로 할당할 수 있습니다.\nCUDA 배열은 텍스처 페칭에 최적화된 불투명 메모리 레이아웃입니다. 텍스처 및 표면 메모리에서 설명합니다.\n선형 메모리는 단일 통합 주소 공간에 할당되므로 별도로 할당된 개체 예를 들어 이진 트리(binary tree) 또는 연결 리스트(linked list) 내에서 포인터를 통해 서로를 참조할 수 있습니다. 주소 공간의 크기는 호스트 시스템(CPU)과 사용된 GPU의 컴퓨팅 기능에 따라 달라집니다.\n\n\n\n\n표 1: Linear Memory Address Space\n\n\n\n\n\n\n\n\n\n\n\n\nx86_64 (AMD64)\nPOWER (ppc64le)\nARM64\n\n\n\n\nup to compute capability 5.3 (Maxwell)\n40bit\n40bit\n40bit\n\n\ncompute capability 6.0 (Pascal) or newer\nup to 47bit\nup to 49bit\nup to 48bit\n\n\n\n\n\n\n\n선형 메모리는 일반적으로 cudaMalloc() 를 사용하여 할당하고 cudaFree() 를 사용하여 해제하며 호스트 메모리와 장치 메모리 간의 데이터 전송은 일반적으로 cudaMemcpy() 를 사용하여 수행됩니다. 커널의 벡터 추가 코드 샘플에서 벡터는 호스트 메모리에서 장치 메모리로 복사해야 합니다.\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\n// Device code\n__global__ void VecAdd(float* A, float* B, float* C, int N)\n{\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i &lt; N)\n        C[i] = A[i] + B[i];\n}\n\n// Host code\nint main()\n{\n    int N = ...;\n    size_t size = N * sizeof(float);\n\n    // Allocate input vectors h_A and h_B in host memory\n    float* h_A = (float*)malloc(size);\n    float* h_B = (float*)malloc(size);\n    float* h_C = (float*)malloc(size);\n\n    // Initialize input vectors\n    ...\n\n    // Allocate vectors in device memory\n    float* d_A;\n    cudaMalloc(&d_A, size);\n    float* d_B;\n    cudaMalloc(&d_B, size);\n    float* d_C;\n    cudaMalloc(&d_C, size);\n\n    // Copy vectors from host memory to device memory\n    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n\n    // Invoke kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid =\n            (N + threadsPerBlock - 1) / threadsPerBlock;\n    VecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);\n\n    // Copy result from device memory to host memory\n    // h_C contains the result in host memory\n    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n    // Free host memory\n    ...\n}\n선형 메모리는 cudaMallocPitch() 및 cudaMalloc3D() 를 통해 할당할 수도 있습니다. 이러한 함수는 2D 또는 3D 배열의 할당에 권장되며, 디바이스 메모리 액세스에서 설명할 정렬 요구 사항을 충족하도록 할당이 적절하게 패딩되어 행 주소에 액세스하거나 2D 배열과 장치 메모리의 다른 영역 간에 복사를 수행할 때 최상의 성능을 보장합니다(cudaMemcpy2D() 및 cudaMemcpy3D() 함수 사용). 반환된 피치(또는 스트라이드)는 배열 요소에 접근하는 데 사용해야 합니다. 다음 코드 샘플은 부동 소수점 값의 너비 x 높이 2D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.\n// Host code\nint width = 64, height = 64;\nfloat* devPtr;\nsize_t pitch;\ncudaMallocPitch(&devPtr, &pitch,\n                width * sizeof(float), height);\nMyKernel&lt;&lt;&lt;100, 512&gt;&gt;&gt;(devPtr, pitch, width, height);\n\n// Device code\n__global__ void MyKernel(float* devPtr,\n                         size_t pitch, int width, int height)\n{\n    for (int r = 0; r &lt; height; ++r) {\n        float* row = (float*)((char*)devPtr + r * pitch);\n        for (int c = 0; c &lt; width; ++c) {\n            float element = row[c];\n        }\n    }\n}\n 다음 코드 샘플은 부동 소수점 값의 너비 x 높이 x 깊이 3D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.\n// Host code\nint width = 64, height = 64, depth = 64;\ncudaExtent extent = make_cudaExtent(width * sizeof(float),\n                                    height, depth);\ncudaPitchedPtr devPitchedPtr;\ncudaMalloc3D(&devPitchedPtr, extent);\nMyKernel&lt;&lt;&lt;100, 512&gt;&gt;&gt;(devPitchedPtr, width, height, depth);\n\n// Device code\n__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,\n                         int width, int height, int depth)\n{\n    char* devPtr = devPitchedPtr.ptr;\n    size_t pitch = devPitchedPtr.pitch;\n    size_t slicePitch = pitch * height;\n    for (int z = 0; z &lt; depth; ++z) {\n        char* slice = devPtr + z * slicePitch;\n        for (int y = 0; y &lt; height; ++y) {\n            float* row = (float*)(slice + y * pitch);\n            for (int x = 0; x &lt; width; ++x) {\n                float element = row[x];\n            }\n        }\n    }\n}\n\n\n\n\n\n\n\n노트\n\n\n\n너무 많은 메모리를 할당하여 시스템 전체 성능에 영향을 미치지 않도록 하려면 문제 크기에 따라 사용자에게 할당 매개변수를 요청하세요. 할당이 실패하면 다른 느린 메모리 유형(cudaMallocHost(), cudaHostRegister() 등)으로 폴백하거나 거부된 메모리가 얼마나 필요한지 알려주는 오류를 반환할 수 있습니다. 애플리케이션이 어떤 이유로 할당 매개변수를 요청할 수 없는 경우 이를 지원하는 플랫폼에 대해 cudaMallocManaged() 를 사용하는 것이 좋습니다.\n\n\n\n참조 설명서에는 cudaMalloc() 로 할당된 선형 메모리, cudaMallocPitch() 또는 cudaMalloc3D() 로 할당된 선형 메모리, CUDA 배열, 전역 또는 상수 메모리 공간에서 선언된 변수에 할당된 메모리 간에 메모리를 복사하는 데 사용되는 다양한 함수가 모두 나열되어 있습니다.\n\n다음 코드 샘플은 런타임 API를 통해 전역 변수에 액세스하는 다양한 방법을 보여줍니다.\n__constant__ float constData[256];\nfloat data[256];\ncudaMemcpyToSymbol(constData, data, sizeof(data));\ncudaMemcpyFromSymbol(data, constData, sizeof(data));\n\n__device__ float devData;\nfloat value = 3.14f;\ncudaMemcpyToSymbol(devData, &value, sizeof(float));\n\n__device__ float* devPointer;\nfloat* ptr;\ncudaMalloc(&ptr, 256 * sizeof(float));\ncudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));\ncudaGetSymbolAddress() 는 전역 메모리 공간에서 선언된 변수에 할당된 메모리를 가리키는 주소를 검색하는 데 사용됩니다. 할당된 메모리의 크기는 cudaGetSymbolSize() 를 통해 얻습니다.\n\n\n\n2.3 디바이스 메모리 L2 접근 관리\nCUDA 커널이 전역 메모리의 데이터 영역에 반복적으로 액세스하는 경우 이러한 데이터 액세스는 지속되는 것으로 간주될 수 있습니다. 반면, 데이터가 한 번만 액세스되는 경우 이러한 데이터 액세스는 스트리밍으로 간주될 수 있습니다.\nCUDA 11.0부터 컴퓨팅 기능 8.0 이상의 장치는 L2 캐시의 데이터 지속성에 영향을 미칠 수 있는 기능을 갖추고 있어 글로벌 메모리에 대한 더 높은 대역폭과 더 낮은 지연 시간 액세스를 제공할 수 있습니다.\n\n\nL2 Cache Set-Aside for Persisting Accesses\nL2 캐시의 일부는 전역 메모리에 대한 지속적인 데이터 액세스에 사용하도록 따로 보관할 수 있습니다. 지속적인 액세스는 L2 캐시의 이 따로 보관된 부분을 우선적으로 사용하지만, 글로벌 메모리에 대한 일반 또는 스트리밍 액세스는 지속적인 액세스에서 사용되지 않을 때만 L2의 이 부분을 활용할 수 있습니다.\n지속적인 액세스를 위한 L2 캐시 예약 크기는 다음 한도 내에서 조정될 수 있습니다.\ncudaGetDeviceProperties(&prop, device_id);\nsize_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);\ncudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/\n\nGPU가 Multi-Instance GPU(MIG) 모드로 구성된 경우 L2 캐시 예약 기능이 비활성화됩니다.\nMulti-Process Service(MPS)를 사용하는 경우 L2 캐시 예약 크기는 cudaDeviceSetLimit 으로 변경할 수 없습니다. 대신 예약 크기는 환경 변수 CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT 를 통해 MPS 서버를 시작할 때만 지정할 수 있습니다.\n\n\n\nL2 Policy for Persisting Accesses\naccess policy window 는 전역 메모리의 연속 영역과 해당 영역 내의 액세스에 대한 L2 캐시의 지속성 속성을 지정합니다. 아래 코드 예제는 CUDA 스트림을 사용하여 L2 persisting access window 를 설정하는 방법을 보여줍니다.\n#| code-overflow: scroll\ncudaStreamAttrValue stream_attribute;                                         // Stream level attributes data structure\nstream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast&lt;void*&gt;(ptr); // Global Memory data pointer\nstream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persistence access.\n                                                                              // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)\nstream_attribute.accessPolicyWindow.hitRatio  = 0.6;                          // Hint for cache hit ratio\nstream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit\nstream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.\n\n//Set the attributes to a CUDA stream of type cudaStream_t\ncudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);\n * to be done *\n\n\n\n\n2.4 공유 메모리\n가변 메모리 공간 지정자 에서 자세히 설명하겠지만 공유 메모리는 __shared__ 메모리 공간 지정자를 사용하여 할당됩니다.\n스레드 계층 구조에서 언급되고 공유 메모리에서 자세히 설명된 대로 공유 메모리는 전역 메모리보다 훨씬 빠를 것으로 예상됩니다. 다음 행렬 곱셈 예제에서 보여지는 것처럼 스크래치패드 메모리(또는 소프트웨어 관리 캐시)로 사용하여 CUDA 블록에서 글로벌 메모리 액세스를 최소화할 수 있습니다.\n다음 코드 샘플은 공유 메모리를 활용하지 않는 간단한 행렬 곱셈 구현입니다. 각 스레드는 A의 한 행과 B의 한 열을 읽고 C의 해당 요소를 계산합니다. 따라서 A는 글로벌 메모리에서 B.width 번 읽히고 B는 A.height 번 읽힙니다.\n// Matrices are stored in row-major order:\n// M(row, col) = *(M.elements + row * M.width + col)\ntypedef struct {\n    int width;\n    int height;\n    float* elements;\n} Matrix;\n\n// Thread block size\n#define BLOCK_SIZE 16\n\n// Forward declaration of the matrix multiplication kernel\n__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);\n\n// Matrix multiplication - Host code\n// Matrix dimensions are assumed to be multiples of BLOCK_SIZE\nvoid MatMul(const Matrix A, const Matrix B, Matrix C)\n{\n    // Load A and B to device memory\n    Matrix d_A;\n    d_A.width = A.width; d_A.height = A.height;\n    size_t size = A.width * A.height * sizeof(float);\n    cudaMalloc(&d_A.elements, size);\n    cudaMemcpy(d_A.elements, A.elements, size,\n               cudaMemcpyHostToDevice);\n    Matrix d_B;\n    d_B.width = B.width; d_B.height = B.height;\n    size = B.width * B.height * sizeof(float);\n    cudaMalloc(&d_B.elements, size);\n    cudaMemcpy(d_B.elements, B.elements, size,\n               cudaMemcpyHostToDevice);\n\n    // Allocate C in device memory\n    Matrix d_C;\n    d_C.width = C.width; d_C.height = C.height;\n    size = C.width * C.height * sizeof(float);\n    cudaMalloc(&d_C.elements, size);\n\n    // Invoke kernel\n    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);\n\n    // Read C from device memory\n    cudaMemcpy(C.elements, d_C.elements, size,\n               cudaMemcpyDeviceToHost);\n\n    // Free device memory\n    cudaFree(d_A.elements);\n    cudaFree(d_B.elements);\n    cudaFree(d_C.elements);\n}\n\n// Matrix multiplication kernel called by MatMul()\n__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n{\n    // Each thread computes one element of C\n    // by accumulating results into Cvalue\n    float Cvalue = 0;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int e = 0; e &lt; A.width; ++e)\n        Cvalue += A.elements[row * A.width + e]\n                * B.elements[e * B.width + col];\n    C.elements[row * C.width + col] = Cvalue;\n}\n\n\n\n\n\n\n\n그림 1: 공유메모리를 사용하지 않는 행렬곱\n\n\n\n\n다음 코드 샘플은 공유 메모리를 활용하는 행렬 곱셈의 구현입니다. 이 구현에서 각 스레드 블록은 C의 하나의 정사각 부분 행렬 Csub를 계산할 책임이 있고 블록 내의 각 스레드는 Csub의 하나의 요소를 계산할 책임이 있습니다. 그림 9에서 볼 수 있듯이 Csub는 두 개의 직사각형 행렬의 곱과 같습니다. 즉, Csub와 같은 행 인덱스를 갖는 (A.width, block_size) 차원의 A의 부분 행렬과 Csub와 같은 열 인덱스를 갖는 (block_size, A.width) 차원의 B의 부분 행렬입니다. 장치의 리소스에 맞추기 위해 이 두 직사각형 행렬은 필요한 만큼의 block_size 차원의 정사각 행렬로 나뉘고 Csub는 이러한 정사각 행렬의 곱의 합으로 계산됩니다. 이러한 각 곱은 먼저 하나의 스레드가 각 행렬의 한 요소를 로드하여 두 개의 해당 정사각 행렬을 전역 메모리에서 공유 메모리로 로드한 다음 각 스레드가 곱의 한 요소를 계산하여 수행됩니다. 각 스레드는 이들 각각의 곱의 결과를 레지스터에 누적하고, 완료되면 결과를 전역 메모리에 씁니다.\n// Matrices are stored in row-major order:\n// M(row, col) = *(M.elements + row * M.stride + col)\ntypedef struct {\n    int width;\n    int height;\n    int stride;\n    float* elements;\n} Matrix;\n// Get a matrix element\n__device__ float GetElement(const Matrix A, int row, int col)\n{\n    return A.elements[row * A.stride + col];\n}\n// Set a matrix element\n__device__ void SetElement(Matrix A, int row, int col,\n                           float value)\n{\n    A.elements[row * A.stride + col] = value;\n}\n// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is\n// located col sub-matrices to the right and row sub-matrices down\n// from the upper-left corner of A\n __device__ Matrix GetSubMatrix(Matrix A, int row, int col)\n{\n    Matrix Asub;\n    Asub.width    = BLOCK_SIZE;\n    Asub.height   = BLOCK_SIZE;\n    Asub.stride   = A.stride;\n    Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row\n                                         + BLOCK_SIZE * col];\n    return Asub;\n}\n// Thread block size\n#define BLOCK_SIZE 16\n// Forward declaration of the matrix multiplication kernel\n__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);\n// Matrix multiplication - Host code\n// Matrix dimensions are assumed to be multiples of BLOCK_SIZE\nvoid MatMul(const Matrix A, const Matrix B, Matrix C)\n{\n    // Load A and B to device memory\n    Matrix d_A;\n    d_A.width = d_A.stride = A.width; d_A.height = A.height;\n    size_t size = A.width * A.height * sizeof(float);\n    cudaMalloc(&d_A.elements, size);\n    cudaMemcpy(d_A.elements, A.elements, size,\n               cudaMemcpyHostToDevice);\n    Matrix d_B;\n    d_B.width = d_B.stride = B.width; d_B.height = B.height;\n    size = B.width * B.height * sizeof(float);\n    cudaMalloc(&d_B.elements, size);\n    cudaMemcpy(d_B.elements, B.elements, size,\n    cudaMemcpyHostToDevice);\n    // Allocate C in device memory\n    Matrix d_C;\n    d_C.width = d_C.stride = C.width; d_C.height = C.height;\n    size = C.width * C.height * sizeof(float);\n    cudaMalloc(&d_C.elements, size);\n    // Invoke kernel\n    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);\n    // Read C from device memory\n    cudaMemcpy(C.elements, d_C.elements, size,\n               cudaMemcpyDeviceToHost);\n    // Free device memory\n    cudaFree(d_A.elements);\n    cudaFree(d_B.elements);\n    cudaFree(d_C.elements);\n}\n// Matrix multiplication kernel called by MatMul()\n __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n{\n    // Block row and column\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n    // Each thread block computes one sub-matrix Csub of C\n    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);\n    // Each thread computes one element of Csub\n    // by accumulating results into Cvalue\n    float Cvalue = 0;\n    // Thread row and column within Csub\n    int row = threadIdx.y;\n    int col = threadIdx.x;\n    // Loop over all the sub-matrices of A and B that are\n    // required to compute Csub\n    // Multiply each pair of sub-matrices together\n    // and accumulate the results\n    for (int m = 0; m &lt; (A.width / BLOCK_SIZE); ++m) {\n        // Get sub-matrix Asub of A\n        Matrix Asub = GetSubMatrix(A, blockRow, m);\n        // Get sub-matrix Bsub of B\n        Matrix Bsub = GetSubMatrix(B, m, blockCol);\n        // Shared memory used to store Asub and Bsub respectively\n        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n        // Load Asub and Bsub from device memory to shared memory\n        // Each thread loads one element of each sub-matrix\n        As[row][col] = GetElement(Asub, row, col);\n        Bs[row][col] = GetElement(Bsub, row, col);\n        // Synchronize to make sure the sub-matrices are loaded\n        // before starting the computation\n        __syncthreads();\n        // Multiply Asub and Bsub together\n        for (int e = 0; e &lt; BLOCK_SIZE; ++e)\n            Cvalue += As[row][e] * Bs[e][col];\n        // Synchronize to make sure that the preceding\n        // computation is done before loading two new\n        // sub-matrices of A and B in the next iteration\n        __syncthreads();\n    }\n    // Write Csub to device memory\n    // Each thread writes one element\n    SetElement(Csub, row, col, Cvalue);\n}\n\n\n\n\n\n\n그림 2: 공유메모리를 사용않는 행렬곱\n\n\n\n\n\n\n2.5 분산 공유 메모리\ncompute capability 9.0에 도입된 스레드 블록 클러스터는 스레드 블록 클러스터의 스레드가 클러스터에 참여하는 모든 스레드 블록의 공유 메모리에 액세스할 수 있는 기능을 제공합니다. 이 분할된 공유 메모리를 분산 공유 메모리(distributed shared memory)라고 하며, 해당 주소 공간을 분산 공유 메모리 주소 공간이라고 합니다. 스레드 블록 클러스터에 속한 스레드는 주소가 로컬 스레드 블록에 속하는지 원격 스레드 블록에 속하는지에 관계없이 분산 주소 공간에서 읽거나 쓰거나 아토믹 연산을 수행할 수 있습니다. 커널이 분산 공유 메모리를 사용하든 사용하지 않든, 정적이든 동적이든 공유 메모리 크기 사양은 여전히 ​​스레드 블록당입니다. 분산 공유 메모리의 크기는 클러스터당 스레드 블록 수에 스레드 블록당 공유 메모리 크기를 곱한 값일 뿐입니다.\n분산 공유 메모리의 데이터에 액세스하려면 모든 스레드 블록이 존재해야 합니다. 사용자는 Cluster Group API의 cluster.sync() 를 사용하여 모든 스레드 블록이 실행을 시작했는지 보장할 수 있습니다. 사용자는 또한 모든 분산 공유 메모리 작업이 스레드 블록이 종료되기 전에 수행되도록 해야 합니다. 예를 들어 원격 스레드 블록이 주어진 스레드 블록의 공유 메모리를 읽으려고 하는 경우 사용자는 원격 스레드 블록이 읽은 공유 메모리가 종료되기 전에 완료되었는지 확인해야 합니다.\nCUDA는 분산 공유 메모리에 액세스하는 메커니즘을 제공하며, 애플리케이션은 이 기능을 활용하여 이점을 얻을 수 있습니다. 간단한 히스토그램 계산과 스레드 블록 클러스터를 사용하여 GPU에서 최적화하는 방법을 살펴보겠습니다. 히스토그램을 계산하는 표준적인 방법은 각 스레드 블록의 공유 메모리에서 계산을 수행한 다음 글로벌 메모리 아토믹을 수행하는 것입니다. 이 방법의 한계는 공유 메모리 용량입니다. 히스토그램 빈이 더 이상 공유 메모리에 맞지 않으면 사용자는 히스토그램을 직접 계산하고 따라서 글로벌 메모리의 아토믹을 계산해야 합니다. 분산 공유 메모리를 사용하면 CUDA는 중간 단계를 제공하며, 여기서 히스토그램 빈 크기에 따라 히스토그램을 공유 메모리, 분산 공유 메모리 또는 글로벌 메모리에서 직접 계산할 수 있습니다.\n아래의 CUDA 커널 예제는 히스토그램 빈의 수에 따라 공유 메모리 또는 분산 공유 메모리에서 히스토그램을 계산하는 방법을 보여줍니다.\n#include &lt;cooperative_groups.h&gt;\n\n// Distributed Shared memory histogram kernel\n__global__ void clusterHist_kernel(int *bins, const int nbins, const int bins_per_block, const int *__restrict__ input,\n                                   size_t array_size)\n{\n  extern __shared__ int smem[];\n  namespace cg = cooperative_groups;\n  int tid = cg::this_grid().thread_rank();\n\n  // Cluster initialization, size and calculating local bin offsets.\n  cg::cluster_group cluster = cg::this_cluster();\n  unsigned int clusterBlockRank = cluster.block_rank();\n  int cluster_size = cluster.dim_blocks().x;\n\n  for (int i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)\n  {\n    smem[i] = 0; //Initialize shared memory histogram to zeros\n  }\n\n  // cluster synchronization ensures that shared memory is initialized to zero in\n  // all thread blocks in the cluster. It also ensures that all thread blocks\n  // have started executing and they exist concurrently.\n  cluster.sync();\n\n  for (int i = tid; i &lt; array_size; i += blockDim.x * gridDim.x)\n  {\n    int ldata = input[i];\n\n    //Find the right histogram bin.\n    int binid = ldata;\n    if (ldata &lt; 0)\n      binid = 0;\n    else if (ldata &gt;= nbins)\n      binid = nbins - 1;\n\n    //Find destination block rank and offset for computing\n    //distributed shared memory histogram\n    int dst_block_rank = (int)(binid / bins_per_block);\n    int dst_offset = binid % bins_per_block;\n\n    //Pointer to target block shared memory\n    int *dst_smem = cluster.map_shared_rank(smem, dst_block_rank);\n\n    //Perform atomic update of the histogram bin\n    atomicAdd(dst_smem + dst_offset, 1);\n  }\n\n  // cluster synchronization is required to ensure all distributed shared\n  // memory operations are completed and no thread block exits while\n  // other thread blocks are still accessing distributed shared memory\n  cluster.sync();\n\n  // Perform global memory histogram, using the local distributed memory histogram\n  int *lbins = bins + cluster.block_rank() * bins_per_block;\n  for (int i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)\n  {\n    atomicAdd(&lbins[i], smem[i]);\n  }\n}\n위의 커널은 필요한 분산 공유 메모리 양에 따라 클러스터 크기로 런타임에 시작할 수 있습니다. 히스토그램이 한 블록의 공유 메모리에 맞을 만큼 작으면 사용자는 클러스터 크기 1로 커널을 시작할 수 있습니다. 아래 코드 조각은 공유 메모리 요구 사항에 따라 동적으로 클러스터 커널을 시작하는 방법을 보여줍니다.\n\n\n2.6 페이지 잠금 호스트 메모리\n\n\n2.7 비동기 동시 실행 (Asynchronous concurrent Execution)\n\n\n2.8 다중 디바이스 시스템\n\n디바이스 선택\n\n\n\n2.9 오류 검사\n\n\n2.10 호출 스택\n\n\n2.11 텍스쳐와 표면 메모리\n\n\n2.12 그래픽 상호 운용성",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Programming Interface"
    ]
  },
  {
    "objectID": "src/tools/Asymptote/asymptote.html",
    "href": "src/tools/Asymptote/asymptote.html",
    "title": "Asymptote",
    "section": "",
    "text": "1 Examples\nimport graph;\nimport geometry;\nimport math;\nimport settings;\nimport fontsize;\n\n\nsettings.outformat = \"pdf\";\ndefaultpen(fontsize(17pt));\ndefaultpen(1);\n\nsize(400,300,IgnoreAspect);\n\nreal a=1, b=1.5, c=2;\nreal ya=1.0, yb=2.4, yc=3.0;\nreal yb2=2.6, yb3=2.2;\nint t=2;\n\nreal f(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\n\nreal f2(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb2;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\nreal f3(real x) {\n    real r1=(x-b)*(x-c)/(a-b)/(a-c)*ya ;\n    r1 += (x-a)*(x-c)/(b-a)/(b-c)*yb3;\n    r1 += (x-a)*(x-b)/(c-a)/(c-b)*yc;\n    return r1;\n    }\n\npair F(real x) {return (x,f(x));}\n\ndotfactor=7;\n\nxaxis(\"$t$\", xmin=0.5, xmax=2.5, Arrow, ticks=Ticks(DefaultFormat,\n                                        new real[] {1, 2}));\nyaxis(\"$P$\", XEquals(0.7), ymin=-0.2, ymax=4, Arrow);\n\npath g=graph(f,a,c);\npath g2=graph(f2,a,c);\npath g3=graph(f3,a,c);\n\ndraw(g,black);   \ndraw(g2,blue+dashed);  \ndraw(g3,dashed+red);  \n\nint n=2;\n\n\n\ndot(Label(\"$P_1$\",align=W), F(a));\ndot(Label(\"$P_2$\",align=E), F(c));\n\n\n\nEuler Lagrange",
    "crumbs": [
      "Tools",
      "Asymptote"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_functions.html",
    "href": "src/tools/tikz/tikz_functions.html",
    "title": "tikz function plots in Quarto",
    "section": "",
    "text": "코드\n```{r, engine = 'tikz'}\n#| label: fig-linear_approximation\n#| code-fold: true\n#| output: asis\n#| fig-width: 3\n#| fig-align: center\n#| fig-cap: \"tikz in Quarto 예시\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\n\\begin{tikzpicture}\n\\draw[very thin,color=gray] (-0.1,-0.1) grid (4.2,5.2);\n\n\\draw[-&gt;] (-0.6, 0) -- (4.2,0) node[right] {$x$};\n\\draw[-&gt;] (0,-0.6) -- (0,5.2) node[above] {$y$};\n\n\\foreach \\x in {1,...,8}\n{\n  \\draw[thin] (\\x / 2, 0.05) -- (\\x /2, -0.05);\n  }\n\n\\foreach \\x in {2, 4, 6, 8}\n{\n  \\node[below]  at (\\x /2 , -0.05) {$\\x$};\n  }\n\n\\foreach \\y in {1,...,9}\n{\n  \\draw[thin] (0.05 , \\y / 2) -- (-0.05 , \\y /2);\n  }\n\n\\foreach \\y in {2, 4, 6, 8, 10}\n{\n  \\node[left]  at (-0.05, \\y / 2) {$\\y$};\n  }\n\n\\filldraw[black] (0.5,1.1 /2) circle (2pt);\n\\filldraw[black] (2/2, 1.65 /2) circle (2pt);\n\\filldraw[black] (3/2, 3.43/2) circle (2pt);\n\\filldraw[black] (4/2, 4.02/2) circle (2pt);\n\\filldraw[black] (5/2, 4.58/2) circle (2pt);\n\\filldraw[black] (6/2, 5.78/2) circle (2pt);\n\\filldraw[black] (7/2, 7.32/2) circle (2pt);\n\n\\draw[thick, dashed, red] (0, 0) -- (3.8, 3.8);\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 1: tikz in Quarto 예시\n\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-Chevyshev_polynoimal\n#| code-fold: true\n#| output: asis\n#| fig-width: 4\n#| fig-align: center\n#| fig-cap: \"Chevyshev 다항식\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\n\\begin{tikzpicture}[domain=-1:1, samples = 100, scale=3]\n\n\\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$x$};\n\\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$y$};\n\\node[left, scale=0.8] at (0, 1) {$1$};\n\\node[left, scale=0.8] at (0, -1) {$-1$};\n\\node[below, scale=0.8] at (-1, 0) {$-1$};\n\\node[below, scale=0.8] at (1, 0) {$1$};\n\n\\foreach \\x in {-5,...,5}\n{\n  \\draw[thin] (\\x / 5, 0.02) -- (\\x /5, -0.02);\n  }\n\n\\foreach \\y in {-5,...,5}\n{\n  \\draw[thin] (0.02 , \\y / 5) -- (-0.02 , \\y /5);\n  }\n\n\\draw[color=black]   plot (\\x, \\x) ;\n\\node[above, black] at (0.7, 0.73) {$T_1(x)$};\n\\draw[color=blue]   plot (\\x, 2 * \\x * \\x - 1);\n\\node[below right, blue] at (0, -1) {$T_2(x)$};\n\\draw[color=red]   plot (\\x, 4 * \\x * \\x * \\x - 3* \\x);\n\\node[above, red] at (-0.5, 1) {$T_3(x)$};\n\\draw[color=teal]   plot (\\x, 8 * \\x * \\x * \\x * \\x - 8* \\x * \\x + 1);\n\\node[right, teal] at (0.2, 0.8) {$T_4(x)$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 2: Chevyshev 다항식\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-Legendre_polynoimal\n#| code-fold: true\n#| output: asis\n#| fig-width: 4\n#| fig-align: center\n#| fig-cap: \"Legendre 다항식\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations.pathreplacing}\n\n\\begin{tikzpicture}[domain=-1:1, samples = 100, scale=3]\n\n\\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$x$};\n\\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$y$};\n\\node[left, scale=0.8] at (0, 1) {$1$};\n\\node[left, scale=0.8] at (0, -1) {$-1$};\n\\node[below, scale=0.8] at (-1, 0) {$-1$};\n\\node[below, scale=0.8] at (1, 0) {$1$};\n\n\\foreach \\x in {-5,...,5}\n{\n  \\draw[thin] (\\x / 5, 0.02) -- (\\x /5, -0.02);\n  }\n\n\\foreach \\y in {-5,...,5}\n{\n  \\draw[thin] (0.02 , \\y / 5) -- (-0.02 , \\y /5);\n  }\n\n\\draw[color=black]   plot (\\x, 1.5 * \\x * \\x - 0.5 ) ;\n\\node[above, black] at (-0.7, 0.73) {$P_2(x)$};\n\\draw[color=blue]   plot (\\x, 2.5 * \\x * \\x * \\x - 1.5 * \\x);\n\\node[above, blue] at (-0.5, 0.4) {$P_3(x)$};\n\\draw[color=red]   plot (\\x, 35/8 * \\x * \\x * \\x *\\x - 30 / 8 * \\x *\\x + 3/8 );\n\\node[red] at (-0.5, -0.5) {$P_4(x)$};\n\\draw[color=teal]   plot (\\x, 63/8 * \\x * \\x * \\x * \\x * \\x - 70/8* \\x * \\x * \\x + 15/8 * \\x);\n\\node[right, teal] at (0.2, 0.5) {$P_5(x)$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 3: Legendre 다항식",
    "crumbs": [
      "Tools",
      "tikz function plots in Quarto"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_pde.html",
    "href": "src/tools/tikz/tikz_pde.html",
    "title": "tikz in PDE",
    "section": "",
    "text": "그리드와 직선의 두께\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-pde_grid\n#| code-fold: true\n#| output: asis\n#| fig-width: 8\n#| fig-align: center\n#| fig-cap: \"2차원 그리드\"\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}\n\n\n\\draw[-{stealth}] (-1., 0) -- (8,0) node[right] {$x$};\n\\draw[-{stealth}] (0,-1) -- (0,6) node[above] {$y$};\n\n\\foreach \\x in {2,3,4,5,7} {\n  \\draw[] (\\x, 1) -- (\\x, 5); \n  \\draw[] (\\x, 0.1) -- (\\x, -0.1);\n}\n\\foreach \\y in {1,2,3,5} {\n  \\draw[] (2, \\y) -- (7, \\y);\n  \\draw[] (0.1, \\y) -- (-0.1, \\y);\n}\n\n\\node[below, scale=0.8] at (2, -0.1) {$x_0=a$};\n\\node[below, scale=0.8] at (3, -0.1) {$x_1$};\n\\node[below, scale=0.8] at (4, -0.1) {$x_2$};\n\\node[below, scale=0.8] at (5, -0.1) {$x_3$};\n\\node[below, scale=0.8] at (7, -0.1) {$x_N=b$};\n\n\\node[left, scale=0.8] at (-0.1, 1) {$y_0=c$};\n\\node[left, scale=0.8] at (-0.1, 2) {$y_1$};\n\\node[left, scale=0.8] at (-0.1, 3) {$y_2$};\n\\node[left, scale=0.8] at (-0.1, 5) {$y_M=d$};\n\n\\node[] at (6, 1.5) {$\\cdots$};\n\\node[] at (6, 2.5) {$\\cdots$};\n\\node[] at (2.5, 4) {$\\vdots$};\n\\node[] at (3.5, 4) {$\\vdots$};\n\\node[] at (4.5, 4) {$\\vdots$};\n\\node[] at (5.5, 4) {$\\vdots$};\n\\node[] at (6.5, 4) {$\\vdots$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 1: 2차원 그리드",
    "crumbs": [
      "Tools",
      "tikz in PDE"
    ]
  },
  {
    "objectID": "src/tools/tikz/tikz_wavelet.html",
    "href": "src/tools/tikz/tikz_wavelet.html",
    "title": "tikz Wavelet",
    "section": "",
    "text": "코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_sharp_function_1\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Legendre 다항식\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 100, scale=2]\n\n\\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$t$};\n\\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n\n\\draw[color=black, thick, domain=-1:0, variable = \\t]   plot ({\\t}, {(1+\\t)});\n\\draw[color=black, thick, domain=0:1, variable = \\t]   plot ({\\t}, {(1-\\t)});\n\\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-a$};\n\\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$a$};\n\n\\begin{scope}[xshift=3.5cm] \n\n\\draw[-{stealth}] (-1.5, 0) -- (1.5,0) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n\n\\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, { (2 * sin((\\t * 180 / pi) /2)/((\\t)))^2 });\n\\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n\\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n\n\\end{scope}\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 1: Legendre 다항식\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_sharp_function_2\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Legendre 다항식\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n\\draw[-{stealth}] (-1.2, 0) -- (1.22,0) node[right] {$t$};\n\\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$f(t)$};\n\n\\draw[color=black, thick, domain=-6:0, variable = \\t]   plot ({\\t/5}, {exp(\\t)});\n\\draw[color=black, thick, domain=0:6, variable = \\t]   plot ({\\t/5}, {exp(-\\t)});\n\\draw[] (-1, 0.05) -- (-1, -0.05 ) node[below] {$-5a$};\n\\draw[] (1, 0.05) -- (1, -0.05 ) node[below] {$5a$};\n\\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$1$};\n\n\\begin{scope}[xshift=2.8cm] \n\n\\draw[-{stealth}] (-1.2, 0) -- (1.2,0) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,0) -- (0,1.2) node[above] {$\\hat{f}(\\omega)$};\n\n\\draw[color=black, thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/5}, {1/(\\t*\\t + 1)});\n\\draw[] (-0.05, 1) -- (0.05, 1 ) node[right] {$2/a$};\n\\draw[] (-0.2, 0.05) -- (-0.2, -0.05 ) node[below] {$-\\frac{1}{a}$};\n\\draw[] (0.2, 0.05) -- (0.2, -0.05 ) node[below] {$\\frac{1}{a}$};\n\n\\end{scope}\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 2: Legendre 다항식\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_haar_wavelet\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 100, scale=2.5]\n\n\\draw[-{stealth}] (-0.5, 0) -- (1.7,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-1.2) -- (0,1.2) node[above] {$\\psi_H(t)$};\n\n\\draw[very thick] (-0.5, 0) -- (0, 0) -- (0, 1) -- (0.5, 1) -- (0.5, -1) -- (1, -1)-- (1, 0) -- (1.5, 0);\n\\node[] at (0, 0) [below left] {$0$};\n\\node[] at (0.5, 0) [below left] {$\\frac{1}{2}$};\n\\node[] at (1, 0) [below left] {$1$};\n\\draw[] (0.1, 1) -- (-0.1, 1) node[left] {$1$};\n\\node[] at (0, -1) [left] {$-1$};\n\n\\begin{scope}[xshift=3.3cm] \n\n\\draw[-{stealth}] (-1.2, -1) -- (1.2,-1) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,-1.0) -- (0,1.2) node[above] {$|\\hat{\\psi}_H(\\omega)|$};\n\n\\draw[color=black, very thick, domain=0.01:40, samples=100, variable = \\t]   plot ({\\t/40}, {(2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n\\draw[color=black, very thick, domain=-40:-0.01, samples=100, variable = \\t]   plot ({\\t/40}, {(-2*(sin(\\t*180/pi/4))*(sin(\\t*180/pi/4)) / (\\t/4)-1)});\n\\draw[] (0.0, -0.96) -- (0.0, -1.04 ) node[below] {$0$};\n\\draw[] (0.314, -0.96) -- (0.314, -1.04 ) node[below] {$4\\pi$};\n\\draw[] (0.628, -0.96) -- (0.628, -1.04 ) node[below] {$8\\pi$};\n\\draw[] (0.942, -0.96) -- (0.942, -1.04 ) node[below] {$12\\pi$};\n\n\\end{scope}\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 3: Haar wavelet\n\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_convolution_of_haar_1\n#| code-fold: true\n#| output: asis\n#| fig-width: 5\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 100, scale=3]\n\n\\draw[-{stealth}] (-0.5, 0) -- (2.5,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.7) -- (0,0.7) node[above] {$(\\psi_H \\ast \\phi)(t)$};\n\n\\draw[very thick] (-0.5, 0) -- (0, 0) -- (0.5, 0.5) -- (1.5, -0.5) -- (2, 0) -- (2.4, 0);\n\\node[] at (0, 0) [below left] {$0$};\n\\draw [] (0.5, 0.03) -- (0.5, -0.03) node[below] {$\\frac{1}{2}$};\n\\draw [] (1, 0.03) -- (1, -0.03) node[below] {$1$};\n\\draw [] (1.5, 0.03) -- (1.5, -0.03) node[below] {$\\frac{3}{2}$};\n\\draw [] (2, 0.03) -- (2, -0.03) node[below] {$2$};\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 4: Haar wavelet\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_daughter_wavelet_1\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n\\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a=1, b=0}(t)$};\n\\draw[color=black, very thick, domain=-1:1, samples=100, variable = \\t]   plot ({\\t}, {exp(-(4*\\t)^2) * cos(\\t*180/pi*10)});\n\n\n\\begin{scope}[xshift=2.3cm] \n\n\\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&gt;1, b&gt;0} (t)$};\n\n\\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(8*(\\t-0.2))^2) * cos(2*(\\t-0.2)*180/pi*10)});\n\n\\end{scope}\n\n\\begin{scope}[xshift=4.6cm] \n\n\\draw[-{stealth}] (-1, 0) -- (1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.2) -- (0,1.2) node[above] {$\\psi_{a&lt;1, b&gt;0}(t)$};\n\n\\draw[color=black, very thick, domain=-1:1, samples=200, variable = \\t]   plot ({\\t}, {exp(-(2*(\\t-0.2))^2) * cos(0.5*(\\t-0.2)*180/pi*10)});\n\n\\end{scope}\n\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 5: Haar wavelet\n\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_maxican_wavelet\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 200, scale=3]\n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\psi_{1, 0}(t)$};\n\\draw[color=black, very thick, domain=-6:6, samples=100, variable = \\t]   plot ({\\t/6}, {(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw (-0.289, -0.02)-- (-0.289, 0.02) node[above, scale=0.8] {$-\\sqrt{3}$};\n\\draw (0.289, -0.02)-- (0.289, 0.02) node[above, scale=0.8] {$\\sqrt{3}$};\n\n\\begin{scope}[xshift=2.3cm] \n\n\\draw[-{stealth}] (-1.1, 0) -- (1.1,0) node[right] {$\\omega$};\n\\draw[-{stealth}] (0,-0.5) -- (0,1.2) node[above] {$\\hat{\\psi}_{1, 0} (t)$};\n\n\\draw[color=black, very thick, domain=-7:7, samples=200, variable = \\t]   plot ({\\t/7}, {\\t*\\t*exp(-\\t*\\t/2)});\n\n\\draw (-0.202, 0.02)-- (-0.202, -0.02) node[below, scale=0.8] {$-\\sqrt{2}$};\n\\draw (0.202, 0.02)-- (0.202, -0.02) node[below, scale=0.8] {$\\sqrt{2}$};\n\\end{scope}\n\n\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 6: Haar wavelet\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_maxican_wavelet_2\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-6.3, 0) -- (6.3,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-4) -- (0,6) node[above] {$$};\n\\draw[color=black, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, {3*(1-(\\t)^2)*exp(-\\t*\\t/2)});\n\\draw[color=red, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(2)/sqrt(3) * (1-((\\t+2)/1.5)^2)*exp(-(((\\t+2)/1.5)^2)/2)});\n\n\\draw[color=teal, very thick, domain=-6:6, samples=300, variable = \\t]   plot ({\\t}, { 3*sqrt(4)* (1-((\\t-sqrt(2))*4)^2)*exp(-(((\\t-sqrt(2))*4)^2)/2)});\n\n\\node [left] at (-0.1, 3) {$\\psi_{1, 0}(\\omega)$};\n\\node [left, red] at (-3, 1) {$\\psi_{\\frac{3}{2}, -2}(\\omega)$};\n\\node [right, teal] at (1.5, 4) {$\\psi_{\\frac{1}{4}, -\\sqrt{2}}(\\omega)$};\n\n\\foreach \\x in {1,...,5}\n{\n  \\draw (\\x, 0.1)-- (\\x, -0.1) node[below, scale=0.8] {$\\x$};\n  \\draw (-\\x, 0.1)-- (-\\x, -0.1) node[below, scale=0.8] {$-\\x$};\n}\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 7: Haar wavelet\n\n\n\n\n\n\n코드\n```{r, engine = 'tikz'}\n#| label: fig-wavelet_resolution_1\n#| code-fold: true\n#| output: asis\n#| fig-width: 10\n#| fig-align: center\n#| fig-cap: \"Haar wavelet\"\n\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{calc}\n\\usetikzlibrary{arrows}\n\n\\begin{tikzpicture}[samples = 200, scale=1.2]\n\n\\draw[-{stealth}] (-0.5, 0) -- (8,0) node[right] {$t$};\n\\draw[-{stealth}] (0,-0.5) -- (0,4) node[above] {$\\omega$};\n\n\\foreach \\x in {0,...,2}\n{\n  \\draw (2*\\x +2 , 0.1)-- (2*\\x +2, 0.0) node[below, scale=0.8] {$b_\\x + a_\\x t_m$};\n  \\draw (0.1, \\x +1)-- (0.0, \\x +1) node[left, scale=0.8] {$\\omega_m/a_\\x$};\n  \\draw[dashed] (2*\\x+2, 0.0) -- (2*\\x+2, \\x+1) -- (0, \\x+1);\n}\n\n\\draw [thick, red] (1, 0.7) -- (3, 0.7) -- (3, 1.3) -- (1, 1.3) -- cycle;\n\\node [above, red, scale=0.8] at (2, 1.3) {$2a_0 \\sigma_t$};\n\\node [right, red, scale=0.8] at (3, 1.0) {$\\dfrac{2}{a_0}\\sigma_\\omega$};\n\n\\draw [thick, blue] (3.5, 1.6) -- (4.5, 1.6) -- (4.5, 2.4) -- (3.5, 2.4) -- cycle;\n\\node [above, blue, scale=0.8] at (4, 2.4) {$2a_1 \\sigma_t$};\n\\node [right, blue, scale=0.8] at (4.5, 2.0) {$\\dfrac{2}{a_1}\\sigma_\\omega$};\n\n\\draw [thick, teal] (5.8, 2) -- (5.8, 4) -- (6.2, 4) -- (6.2, 2) -- cycle;\n\\node [above, teal, scale=0.8] at (6, 4) {$2a_2 \\sigma_t$};\n\\node [right, teal, scale=0.8] at (6.2, 3.0) {$\\dfrac{2}{a_2}\\sigma_\\omega$};\n\n\n\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n그림 8: Haar wavelet",
    "crumbs": [
      "Tools",
      "tikz Wavelet"
    ]
  },
  {
    "objectID": "src/topics/socket.html",
    "href": "src/topics/socket.html",
    "title": "TCP/UDP 통신",
    "section": "",
    "text": "현재의 많은 장비들은 소켓 통신을 사용하여 데이터와 정보를 주고받는다. 예전의 장비들은 RS232 나 RS485 같은 직렬 통신이나 다른 다양한 통신 방법을 사용하는 경우가 많았지만 최근의 장비들은 소켓 통신을 통해 데이터를 전송하는 경우가 많다. 여기서는 소켓 통신에 대해 짧게 알아보고 실제 구현해 보기로 하자.\n\n\n\n소켓은 같은 컴퓨터의 프로세스 사이에서, 혹은 서로 다른 컴퓨터(혹은 장비)의 프로세스 사이에서 통신을 수행할 때의 종단점 (end point)이다. 같은 컴퓨터의 프로세스 사이의 소켓에는 unix domain socket 이 있으며 다른 컴퓨터 프로세스 사이에서의 통신에는 Datagram socket 과 Streaming socket 이 있다. 각각의 네트워크 하드웨어는 사양과 내부 명령어가 다르지만 운영체제 수준에서 이를 추상화하여 서로 통신할 수 있도록 한 것이 소켓이다. 여기서는 서로 다른 컴퓨터의 프로세스간 통신에 대해서만 다루기로 한다.\n\n\n\n\n전화와 같이 믿을 수 있는 양방향 통신을 제공한다. 즉 데이터를 주고받는 소켓의 양쪽이 성립된 상태에서 한쪽(서버)에서 다른 한쪽(클라이언트)으로의 연결을 초기화하고, 연결이 생성된 후에는 어느 쪽에서든 다른 쪽으로 통신할 수 있다. 데이터를 송신하고 나서 이 데이터가 실제로 도착했는지도 즉각 확인할 수 있다. 보통 전송 제어 프로토콜(Transmission Control Protocol, TCP)이라 불리는 표준 통신 프로토콜을 사용하며 이 외에도 SCTP(Stream Control Transmission Protocol) 나 DCCP (Datagram Congestion Control Protocol) 가 사용되지만 여기서는 다루지 않기로 한다. 컴퓨터 네트워크에서 데이터는 보통 패킷이라는 단위로 전송되는데, TCP는 패킷이 오류 없이 순서대로 도착하도록 설계되었다. 웹서버, 메일서버, 각 클라이언트 애플리케이션 모두는 TCP와 스트림 소켓을 사용한다.\n\n\n\n\n데이터그램 소켓의 연결은 단방향이고 신뢰할 수 없다. IP 와 포트번호를 특정하여 보내지만 수신을 확인하지 않는다. 또한 데이터가 순서대로 전송된다고 보장할 수도 없다. 사용자 데이터그램 프로토콜(User Datagram Protocol, UDP)이라는 표준 프로토콜을 사용한다. TCP 에 비해 단순하고 간단하며 부하가 적고 빠른 방법이다. 패킷 손실이 허용되기도 하며 네트워크 게임이나 음악/동영상 스트리밍에서 자주 쓰인다. UDP 를 통해 신뢰성 있는 데이터 통신을 하고 싶다면 직접 패킷을 통해 구현해야 한다.\n\n\n\n\n여기서는 접속을 기다리는 것이 서버(server), 기다리는 서버에 접근하는 것이 클라이언트(client) 라고 한다.",
    "crumbs": [
      "주제별",
      "TCP/UDP 통신"
    ]
  },
  {
    "objectID": "src/topics/socket.html#소켓-통신",
    "href": "src/topics/socket.html#소켓-통신",
    "title": "TCP/UDP 통신",
    "section": "",
    "text": "현재의 많은 장비들은 소켓 통신을 사용하여 데이터와 정보를 주고받는다. 예전의 장비들은 RS232 나 RS485 같은 직렬 통신이나 다른 다양한 통신 방법을 사용하는 경우가 많았지만 최근의 장비들은 소켓 통신을 통해 데이터를 전송하는 경우가 많다. 여기서는 소켓 통신에 대해 짧게 알아보고 실제 구현해 보기로 하자.\n\n\n\n소켓은 같은 컴퓨터의 프로세스 사이에서, 혹은 서로 다른 컴퓨터(혹은 장비)의 프로세스 사이에서 통신을 수행할 때의 종단점 (end point)이다. 같은 컴퓨터의 프로세스 사이의 소켓에는 unix domain socket 이 있으며 다른 컴퓨터 프로세스 사이에서의 통신에는 Datagram socket 과 Streaming socket 이 있다. 각각의 네트워크 하드웨어는 사양과 내부 명령어가 다르지만 운영체제 수준에서 이를 추상화하여 서로 통신할 수 있도록 한 것이 소켓이다. 여기서는 서로 다른 컴퓨터의 프로세스간 통신에 대해서만 다루기로 한다.\n\n\n\n\n전화와 같이 믿을 수 있는 양방향 통신을 제공한다. 즉 데이터를 주고받는 소켓의 양쪽이 성립된 상태에서 한쪽(서버)에서 다른 한쪽(클라이언트)으로의 연결을 초기화하고, 연결이 생성된 후에는 어느 쪽에서든 다른 쪽으로 통신할 수 있다. 데이터를 송신하고 나서 이 데이터가 실제로 도착했는지도 즉각 확인할 수 있다. 보통 전송 제어 프로토콜(Transmission Control Protocol, TCP)이라 불리는 표준 통신 프로토콜을 사용하며 이 외에도 SCTP(Stream Control Transmission Protocol) 나 DCCP (Datagram Congestion Control Protocol) 가 사용되지만 여기서는 다루지 않기로 한다. 컴퓨터 네트워크에서 데이터는 보통 패킷이라는 단위로 전송되는데, TCP는 패킷이 오류 없이 순서대로 도착하도록 설계되었다. 웹서버, 메일서버, 각 클라이언트 애플리케이션 모두는 TCP와 스트림 소켓을 사용한다.\n\n\n\n\n데이터그램 소켓의 연결은 단방향이고 신뢰할 수 없다. IP 와 포트번호를 특정하여 보내지만 수신을 확인하지 않는다. 또한 데이터가 순서대로 전송된다고 보장할 수도 없다. 사용자 데이터그램 프로토콜(User Datagram Protocol, UDP)이라는 표준 프로토콜을 사용한다. TCP 에 비해 단순하고 간단하며 부하가 적고 빠른 방법이다. 패킷 손실이 허용되기도 하며 네트워크 게임이나 음악/동영상 스트리밍에서 자주 쓰인다. UDP 를 통해 신뢰성 있는 데이터 통신을 하고 싶다면 직접 패킷을 통해 구현해야 한다.\n\n\n\n\n여기서는 접속을 기다리는 것이 서버(server), 기다리는 서버에 접근하는 것이 클라이언트(client) 라고 한다.",
    "crumbs": [
      "주제별",
      "TCP/UDP 통신"
    ]
  },
  {
    "objectID": "src/topics/socket.html#tcp-와-udp-를-이용한-간단한-에코-서버와-클라이언트의-저차원-구현",
    "href": "src/topics/socket.html#tcp-와-udp-를-이용한-간단한-에코-서버와-클라이언트의-저차원-구현",
    "title": "TCP/UDP 통신",
    "section": "2 TCP 와 UDP 를 이용한 간단한 에코 서버와 클라이언트의 저차원 구현",
    "text": "2 TCP 와 UDP 를 이용한 간단한 에코 서버와 클라이언트의 저차원 구현\n여기서는 Python 으로 간단한 서버와 클라이언트를 구현한다. 가장 기본적인 저차원 구현은 C 언어를 이용한 구현이지만 여기서는 이 C 구현과 가장 비슷하며 저차원 구현을 체험해 볼 수 있는 Python 구현을 통해 TCP 와 UDP 통신을 알아보기로 하자.\n\nTCP 통신\n서버가 대기하고 클라이언트와의 접속이 성립하면 클라이언트는 사용자로부터의 입력을 서버에게 보낸다. 서버는 클라이언트로부터 받은 메시지를 출력하고 그 메시지 끝에 # 을 붙여 클라이언트에게 보낸다. 클라이언트는 # 이 붙은 메시지를 받고 출력한다. 사용자로부터 -1 을 받으면 서버는 역시 # 이 붙은 메시지 -1# 을 클리이언트로 보내고 종료하며, 클라이언트 역시 종료한다.\nPython 으로 구현하지만 여기서의 방식은 소켓 통신의 기본적인 즉 low-level 방식으로 거의 모든 언어에서 기본적으로 지원한다. 물론 실제로는 이 방식보다는 이 방식을 좀 더 쓰기 편하게 만든 방식을 쓸 수도 있지만 기본 동작을 일단 확인하기 위해 low-level 방식으로 구현해보자.\n소켓 통신을 위해서는 socket 모듈을 임포트 해야 하고 소켓 객체를 구현해야 한다. 서버에는 두개의 소켓이 필요하다. 하나는 접속을 받아들이고 연결을 위한 역할을 수행하는 소켓(이하 연결 소켓)이며 다른 하나는 클라이언트와 데이터를 주고받기 위한 소캣(이하 서버 소켓)이다. 클라이언트는 데이터를 주고받는 소켓(이하 클라이언트 소켓) 하나만 있으면 된다. 연결 소켓, 서버소켓, 클라이언트 소켓은 편의상 지은 이름이며 다른 곳에서는 다른 이름으로 사용될 수 있다.\n\nTCP 서버\n일단 서버쪽에서 연결 소켓을 성립시켜야 한다. 연결 소켓을 위해서는 서버 IP 와 포트 번호가 필요하다. IP 는 \"123.123.123.12\" 형식의 문자열이며 포트 번호는 0 부터 65535 번까지의 정수이다. IP 와 포트번호는 Internet Assigned Numbers Authority (IANA) 에서 관리하는데 포트 번호중의 일부는 특별한 용도로 지정되어 있다. 자세한 것은 List of TCP and UDP port numbers 를 참고하라. 보통 49152 에서 65535 번호는 자유롭게 사용 할 수 있다.\n\n연결 소켓은 IP 와 포트번호, 그리고 프로토콜로 구성된다. 우선\nimport socket\n을 통해 소캣 모듈을 사용할 수 있도록 한다. 이후 아래와 같이 소켓 인스턴스를 만들고 프로토콜과 옵션을 정한다.\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nsocket.socket 의 첫번째 인자로는 다음이 사용될 수 있다.\n\nsocket.AF_INET 은 IPv4 즉 4 개의 8비트 정수로 지정된 IP 번호를 사용한는 것을 의미한다. 보퉁 많이 사용하는 123.123.123.12 형식이다.\nsocket.AF_INET6 는 IPv6 주소체계를 사용한다는 것을 의미하며 16비트 정수 4개를 사용한다.\nsocket.AF_UNIX 는 네트웍 통신이 아닌 프로세스간의 통신에 대해 사용한다.\n\nsocket.socket 의 두번째 인자로는 다음이 사용될 수 있다.\n\nsocket.SOCK_STREAM 은 TCP 통신 규약을 사용한다는 것을 의미한다.\nsocket.SOCK_DGRAM 은 UDP 통신 규약을 사용한다는 것을 의미한다.\n\n\nsocket.setsockopt 함수는 소켓의 옵션을 지정한다. 첫번째 인자는 프로토콜 레벨이며 두번째 인자는 프로토콜 레벨에서의 옵션 이름, 세번째 인자부터는 앞의 두 인자에 따른 설정값 등이 온다. 자세한 내용은 setsockopt 서브루틴 을 참고하라. 여기서 socket.SO_REUSEADDR 옵션의 옵션값을 1 로 설정하면 클라이언트 소켓을 닫은 후 같은 IP 와 포트번호로 연결할 수 있다.\n\nserver_socket.bind((host, port))\nserver_socket.listen(0)\nclient_soc, addr = server_socket.accept()\nbind((host, port)) 는 연결 소켓에 IP 번호(host) 와 포트(port) 를 지정한다. 이제 listen() 을 통해 클라이언트로부터 접속 요청 대기를 시작한다. listen 함수는 최대 대기 큐의 갯수를 인자로 받을 수 있며, 인자가 없을 경우 자동으로 할당된다.\naccept() 함수는 연결을 시도한 클라이언트와 통신 할 수 있는 소켓(client_soc), 즉 서버 소켓과 연결된 클라이언트의 (IP 주소, 포트) 튜플을 반환한다. 이 서버 소켓을 통해 클라이언트와 통신한다. 이제 서버의 코드를 보자.\n# TCP 서버 프로그램 \nimport socket, time\n\nhost, port = 'localhost', 43333\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nserver_socket.bind((host, port))\nserver_socket.listen()\n\nprint('TCP 서버 시작')\n\nclient_soc, addr = server_socket.accept()\n\nprint('연결된 client (IP, port):', addr)\n\n# 접속 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    try:\n        data = client_soc.recv(1024)\n    except ConnectionResetError:\n        print('ConnectionResetError')\n        connection_retained = False\n    finally:    \n        msg = data.decode() # 읽은 데이터 디코딩\n        print('받은 메시지 :', msg)\n        client_soc.sendall((msg+\"#\").encode(encoding='utf-8')) # 에코메세지 클라이언트로 보냄\n\n        # 클라이언트로부터 받은 메시지가 \"-1\" 이면 접속을 종료시킨다.\n        if msg == \"-1\":\n            connection_retained = False\n            print(\"to be closed\")        \n            client_soc.close()\n        \n    time.sleep(0.2)\n    \nserver_socket.close() # 사용했던 서버 소켓을 닫아줌 \n앞서 말했듯이 클라이언트로부터 -1 메시지를 받기 전까지는 계속 클라이언트와 통신해야 한다. 이를 위해 일단 connection_retianed = True 로 두고 이 변수값이 True 인 상황에서는 계속 주고 받도록 한다. 그러나 클라이언트로부터 -1 을 받으면 connection_refused = False 가 되며 while 루프를 벗어나고 소켓이 종료되고 (server_socket.close()) 프로그램도 종료한다.\n\n\n\nTCP 클라이언트\n클라이언트는 서버보다 간단한다.\nimport socket\nserver_ip, server_port = 'localhost', 43333\nsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n서버에 접속하기 위해서는 서버의 IP 와 포트 번호를 알야야 한다(server_ip, server_port). TCP 통신이므로 소켓은 server 의 연결소켓과 같이 설정한다. 이제 연결을 시도한다\nsocket.connect((server_ip, server_port))\n서버와 마찬가지로 연결 유지를 확인하기 위한 변수 connetion_retained 가 존재한다. 코드는 아래와 같다.\n# TCP 클라이언트 프로그램\nimport socket, time\n\nserver_ip, server_port = 'localhost', 43333\nsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nsocket.connect((server_ip, server_port))\nsocket.settimeout(0.1)\n\n# 연결 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    msg = input('msg:') # 서버로 보낼 msg 입력\n    \n    if msg == \"-1\":\n        connection_retained = False\n    socket.sendall(msg.encode(encoding='utf-8'))\n\n    # 서버가 에코로 되돌려 보낸 메시지를 클라이언트가 받음\n    try:\n        data = socket.recv(1024)\n    except TimeoutError:\n        print('TimeoutError')\n    finally:\n        msg = data.decode() # 읽은 데이터 디코딩\n        print('서버로부터 받은 메시지 :', msg)\nsocket.close()\nsocket.recv(1024) 는 서버로부터 최대 1024 바이트를 는다는 의미이다. 그러나 이 함수 실행 후 지정된 시간동안 응답이 없을 경우 TimeoutError 가 발생하며 소캣이 종료된다. 응답대기시간은 socket.settimeout() 함수를 통해 지정하며 인자로 초 단위의 시간을 입력한다. 인자가 없으면 무한정 기다린다.\n여기서는 응답 대기 시간이 길더라도 문제가 없지만 예를 들어 여러 서버로부터 데이터를 받는 경우를 생각해보자. 여기서 응답을 대기한다면 다른 작업을 수행 할 수가 없으며, 이를 위해 응답 대기시간을 짧게 잡고 다른 작업을 수행한 후 다시 메시지를 기다리는 것이 좋다.\n\n\n\n\nUDP 통신\nUDP 통신과 TCP 통신의 차이는 다음과 같다.\n\n소켓 타입이 socket.SOCK_STREAM 이 아닌 socket.SOCK_DGRAM 이다.\n연결 소켓이 없으며 서버소켓이 직접 bind 된다.\nTCP 에서는 recv, send 를 통해 데이터를 주고받지만 UDP 에서는 recvfrom, sendto 함수를 사용한다. 앞서 사용한 sendall 함수는 기본 함수인 send 를 python socket 모듈에서 확장한 것이다.\nrecvfrom 함수를 통해 데이터와 데이터를 보낸 주소를 얻으며, 이 주소가 원하는 주소가 않을 경우 무시한다.\nsendto 함수의 인자로는 데이터와 데이터를 받을 IP, 포트번호의 튜플을 전달한다.\n\n\n소스코드를 보라.\n\n\nUDP 서버\n# UDP 서버 프로그램\nimport socket, time\n\nhost, port = 'localhost', 43333\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nserver_socket.bind((host, port))\n# server_socket.listen()\n\nprint('UDP 서버 시작')\n\n#client_soc, addr = server_socket.accept()\naddr = (None, None)\n\nprint('연결된 client (IP, port):', addr)\n\n# 접속 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    try:\n        data, addr = server_socket.recvfrom(1024)\n    except ConnectionResetError:\n        print('ConnectionResetError')\n        connection_retained = False\n    finally:    \n        msg = data.decode() # 읽은 데이터 디코딩\n        print(addr, '받은 메시지 :', msg)\n        server_socket.sendto((msg+\"#\").encode(encoding='utf-8'), addr) # 에코메세지 클라이언트로 보냄\n\n        # 클라이언트로부터 받은 메시지가 \"-1\" 이면 접속을 종료시킨다.\n        if msg == \"-1\":\n            connection_retained = False\n            print(\"to be closed\")        \n            # client_soc.close()\n        \n    time.sleep(0.2)\n    \nserver_socket.close() # 사용했던 서버 소켓을 닫아줌 \n\n\n\nUDP 클라이언트\n# UDP 클라이언트 프로그램\nimport socket, time\n\nserver_addr = ('localhost', 43333)\nsocket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n# socket.connect((server_ip, server_port))\nsocket.settimeout(0.1)\n\n# 연결 유지 변수\nconnection_retained = True\n\nwhile connection_retained :\n    msg = input('msg:') # 서버로 보낼 msg 입력\n    \n    if msg == \"-1\":\n        connection_retained = False\n    socket.sendto(msg.encode(encoding='utf-8'),server_addr)\n\n    # 서버가 에코로 되돌려 보낸 메시지를 클라이언트가 받음\n    try:\n        data, addr = socket.recvfrom(1024)\n    except TimeoutError:\n        print('TimeoutError')\n    finally:\n        msg = data.decode() # 읽은 데이터 디코딩\n        print('서버로부터 받은 메시지 :', msg)\nsocket.close()",
    "crumbs": [
      "주제별",
      "TCP/UDP 통신"
    ]
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#gpu-및-병렬-처리",
    "href": "src/gpu/cuda/01_introduction.html#gpu-및-병렬-처리",
    "title": "Introduction",
    "section": "1 GPU 및 병렬 처리",
    "text": "1 GPU 및 병렬 처리\n병렬 처리 시스템은 그 아키텍쳐와 메모리 공유 상테에 따라 분류 할 수 있다.\n\n병렬 처리 하드웨어 아키텍쳐\n플린(M. J. Flynn) 은 병렬 처리 하드웨어를 두가지 기준으로 네가지로 분류하였다.\n (\\(1\\)) 한 셋의 데이터 흐름에 대해 수행하는 명령어(instruction) 의 개수 : 1 혹은 1 이상 (&gt;1)\n (\\(2\\)) 하나의 명령에 의해 수행되는 데이터의 흐름의 수 : 1 혹은 1 이상 (&gt;1)\n\n\n\n아키텍쳐\n설명\n(\\(1\\)) 기준\n(\\(2\\)) 기준\n\n\n\n\nSISD\nSingle instruction stream,single data stream\n1\n1\n\n\nSIMD\nSingle instruction stream, multiple data stream\n1\n&gt;1\n\n\nMISD\nMultiple instruction stream, single data stream\n&gt;1\n1\n\n\nMIMD\nMultiple instruction stream multiple data stream\n&gt;1\n&gt;1\n\n\n\nMISD 구조는 아직 실현되지 않았으며 병렬컴퓨팅에서 사용되는 구조는 SIMD 와 MIMD(멀티코어 CPU) 이다.\n\n\nMIMD\n명령어와 데이터가 1-1 로 매칭되는 SISD 가 하나의 칩 안에 들어있는 구조가 널리 사용된다. 각각의 코어가 자신만의 제어 유닛과 문맥을 가지고 독립적으로 프로세스를 수해 여러개의 코어가 하나의 프로세서 칩 안에 존재한다. 또한 하나의 컴퓨터에 여러개의 프로세서 칩이 있을 경우 멀티코어 멀티 프로세서라고 한다.\n\n\n\nSIMD\n동일한 명령어를 여러 데이터에 대해 수행한다. 데이터 배열의 각 성분에 동일한 연산을 수항한다는 의미에서 벡터 프로세서, 혹은 배열 프로세서라고도 혼다. 대표적으로 GPU 가 있다. 일부 CPU 는 내부적으로 SIMD 유닛을 가지고 있는데 CPU 사양에서 이야기하는 MMX, SSE, AVX, Neon 등이 이러한 SIMD 유닛을 의미한다.\n\nALU(arithmetic logic unit, 산술 논리 유닛) 는 하나의 연산 유닛 혹은 코어를 의미한다. 하나의 제어 유닛이 여러개의 ALU를 제어하는 경우가 SIMD 의 대표적인 경우이고, ALU 와 제어 유닛이 결합된 세트가 여러개일 때가 MIMD 의 대표적인 경우이다.\n\n\n\n\n공유 메모리와 분산 메모리\n\n여러 ALU 가 메모리 공간을 공유하는 시스템을 공유 메모리 시스템이라고 하며, 각 ALU 가 각각의 메모리 공간을 갖고 ALU 간 통신이 필요한 경우 명시적인 통신을 하는 시스템을 분산 메모리 시스템이라고 한다.\n공유 메모리 시스템의 경우 여러 ALU 가 하나의 메모리 공간에 접근하기 때문에 각각의 작업이 간섭을 일으 킬 수 있다. 이 경우 각각의 ALU 의 작업 순서를 맞추는 것을 동기화(synchoronization) 이라고 한다.\nGPU 도 공유 메모리 시스템을 사용한다.\n\n\n\n\nSIMT\nGPU 는 SISD 아키텍쳐와 공유 메모리 시스템을 사용하지만 일반적으로 SIMT(Single instruction multiple threads) 구조로 정의된다.\n\n한 스레드 그룹 내의 스레드들은 하나의 제어 유닛으로 제어된다.\n각 스레드는 자신만의 제어 문맥을 가진다.\n스레드 그룹 내 스레드 들 사이의 분기가 허용된다.",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#maximize-utilization",
    "href": "src/gpu/cuda/05_performance_guideline.html#maximize-utilization",
    "title": "5. Performance Guideline",
    "section": "2 Maximize Utilization",
    "text": "2 Maximize Utilization"
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#메모리-스루풋-극대화",
    "href": "src/gpu/cuda/05_performance_guideline.html#메모리-스루풋-극대화",
    "title": "5. Performance Guideline",
    "section": "3 메모리 스루풋 극대화",
    "text": "3 메모리 스루풋 극대화\n\n호스트와 디바이스 간의 데이터 전송\n\n\n디바이스 메모리 접근"
  },
  {
    "objectID": "src/gpu/cuda/05_performance_guideline.html#instruction-스루풋-극대화",
    "href": "src/gpu/cuda/05_performance_guideline.html#instruction-스루풋-극대화",
    "title": "5. Performance Guideline",
    "section": "4 Instruction 스루풋 극대화",
    "text": "4 Instruction 스루풋 극대화"
  },
  {
    "objectID": "src/gpu/cuda/01_introduction.html#확장가능한-프로그래밍-모델",
    "href": "src/gpu/cuda/01_introduction.html#확장가능한-프로그래밍-모델",
    "title": "Introduction",
    "section": "4 확장가능한 프로그래밍 모델",
    "text": "4 확장가능한 프로그래밍 모델\n\nMulticore(CPU) 와 Manycore(GPU) 의 등장으로 처리 장치의 주류가 병렬 시스템이 되었으며, 3D 그래픽 응용 프로그램이 병렬성을 투명하게 확장하여 코어 수가 매우 다양한 많은 코어 GPU로 확장되는 것처럼 이런 병렬성을 투명하게 확장하여 증가하는 프로세서 코어 수를 활용하는 애플리케이션 소프트웨어를 개발하는 것이 목표가 되었다.\nCUDA 병렬 프로그래밍 모델은 C 와 같은 표준 프로그래밍 언어에 익숙한 프로그래머에게 낮은 학습 곡선을 유지하면서 이러한 과제를 극복하도록 설계되었다.\n핵심에는 스레드 그룹, 공유 메모리, 배리어 동기화의 계층 구조라는 세 가지 핵심 추상화가 있으며, 이는 단순히 최소한의 언어 확장 세트로 프로그래머에게 주어진다.\n이러한 추상화는 거친 데이터 병렬성과 작업 병렬성 내에 포개어진 미세한 데이터 병렬성과 스레드 병렬성을 제공한다. 이는 프로그래머가 문제를 스레드 블록으로 독립적으로 병렬로 해결할 수 있는 거친 하위 문제로 분할하고, 각 하위 문제를 블록 내의 모든 스레드가 협력하여 병렬로 해결할 수 있는 더 미세한 부분으로 분할하도록 안내한다.\n이 분해는 각 하위 문제를 해결할 때 스레드가 협력할 수 있도록 하여 언어 표현력을 보존하고 동시에 자동 확장성을 가능하게 한다. 실제로 각 스레드 블록은 GPU 내의 사용 가능한 모든 멀티프로세서에서 어떤 순서로든 동시에 또는 순차적으로 예약될(scheduled) 수 있으므로 컴파일된 CUDA 프로그램은 그림 3 에서 설명한 대로 멀티프로세서의 갯수가 몇개든 실행할 수 있으며 런타임 시스템만 물리적인 멀티프로세서 수를 알면 된다.\n이 확장 가능한 프로그래밍 모델을 사용하면 GPU 아키텍처가 멀티프로세서 수와 메모리 파티션을 간단히 확장하여 광범위한 시장 범위에 걸쳐 확장할 수 있다. 고성능 매니아용 GeForce GPU와 전문가용 Quadro 및 Tesla 컴퓨팅 제품부터 다양한 저렴한 주류 GeForce GPU까지 다양합니다(모든 CUDA 지원 GPU 목록은 CUDA 지원 GPU 참조).\n\n\n\n\n\n\n\n\n노트\n\n\n\nGPU는 스트리밍 멀티프로세서(streaming multiprocessor, SM) 배열을 중심으로 구축됩니다(자세한 내용은 하드웨어 구현 참조). 멀티스레드 프로그램은 서로 독립적으로 실행되는 스레드 블록으로 분할되므로 멀티프로세서가 더 많은 GPU가 멀티프로세서가 적은 GPU보다 자동으로 프로그램을 더 짧은 시간 내에 실행합니다.\n\n\n\n\n\n\n\n\n\n그림 3: Automatic Scalability",
    "crumbs": [
      "GPU & CUDA",
      "CUDA",
      "Introduction"
    ]
  }
]