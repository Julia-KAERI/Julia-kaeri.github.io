---
title: "CUDA.jl 커널 프로그래밍"

number-sections: true
number-depth: 2
crossref:
  chapters: false
---

## 커널 {#sec-cudajl_kernel}

CPU 에서 호출하고 GPU 에서 실행되는 함수를 커널(kernel) 이라고 한다. 커널은 보통의 julia 함수처럼 정의한다.

```julia
function my_kernel()
    return
end
```

커널을 실행하기 위해서는 `@cuda` 매크로를 사용한다.

```julia
@cuda my_kernel
```

위의 명령을 실행하면 `my_kernel` 함수가 컴파일되며 현재의 GPU 에서 실행된다.

</br>

`@cuda` 매크로에 `launch=false` 인자를 전달하면 컴파일만 되고 실행하지 않으며 호출 가능한 객체를 리턴한다. 

```julia
julia> k = @cuda launch=false my_kernel()
CUDA.HostKernel for my_kernel()

julia> CUDA.registers(k)
4

julia> k()
```

</br>

## 커널 입력과 출력 {#sec-cudalj_kernel_io}

GPU 커널은 반환값을 가질 수 없다. 즉 항상 `return` 이거나 `return nothing` 이어야 한다. 커널과 통신하는 유일한 방법은 `CuArray` 를 쓰는 것 뿐이다.

```julia
function my_kernel(a)
    a[1] = 42
    return
end

a = CuArray{Int}(undef, 1);
@cuda my_kernel(a);
a
```
```txt
42
```

</br>

## 커널 구동 설정과 인덱싱 {#sec-cudajl_kernel_configuration_and_indexing}

`@cuda` 를 통해 커널을 구동하면 단일 스레드만 시작되므로 그다지 유용하지 않다. @cuda 에 대한 `threads` 및 `blocks` 키워드 인수를 사용하면 다수의 스레드를 구동할 수 있으며, 커널 내에서는 인덱싱 내장 함수를 사용하여 각 스레드의 계산을 차별화 할 수 있다.

```julia
function my_kernel(a)
    i = threadIdx().x
    a[i] = 42
    return
end

a = CuArray{Int}(undef, 5);
@cuda threads=length(a) my_kernel(a);
a
```

```txt
5-element CuArray{Int64, 1, CUDA.DeviceMemory}:
 42
 42
 42
 42
 42
```

위에 표시된 대로, CUDA C 의 `threadIdx` 등의 값은 `x`, `y`, `z` 필드가 있는 `NamedTuple` 을 반환하는 함수로 사용할 수 있다. 이런 내장 함수는 1 부터 시작하는 인덱스를 반환한다.

</br>

## 커널 컴파일 요건 {#sec-cudajl_kernel_compilation_requirements}

사용자 정의 커널이 작동하기 위해서는 어떤 요건을 충족해야 한다.

- 메모리는 GPU에서 접근 가능해야 한다. 이는 `CuArray` 등을 사용하여 강제할 수 있다. 사용자 지정 구조체는 해당 [튜토리얼](https://cuda.juliagpu.org/stable/tutorials/custom_structs/)에서 설명한 대로 이식할 수 있다.

- 런타임 디스패치는 불가하며 모든 함수 호출은 컴파일 타임에 결정되어야 합니다. 여기서 런타임 디스패치는 완전히 특정되지 않은 함수에 의해 도입될 수도 있다는 점에 유의해야 한다. [Julia 매뉴얼](https://docs.julialang.org/en/v1/manual/performance-tips/#Be-aware-of-when-Julia-avoids-specializing) 을 참고하고 다음 예를 보자. 

```julia
function my_inner_kernel!(f, t) # does not specialize
    t .= f.(t)
end

function my_outer_kernel(f, a)
    i = threadIdx().x
    my_inner_kernel!(f, @view a[i, :])
    return nothing
end

a = CUDA.rand(Int, (2,2))
id(x) = x

@cuda threads=size(a, 1) my_outer_kernel(id, a)
```

마지막 줄 실행에서 에러가 발생하는데 이는 아래와 같이 회피 할 수 있다.

```julia
function my_inner_kernel!(f::F, t::T) where {F,T}
    t .= f.(t)
end

function my_outer_kernel(f, a)
    i = threadIdx().x
    my_inner_kernel!(f, @view a[i, :])
    return nothing
end

a = CUDA.rand(Int, (2,2))

id(x) = x

@cuda threads=size(a, 1) my_outer_kernel(id, a)
```

단지 첫번째 함수 `my_inner_kernel!` 의 인자의 함수가 파라미터로 특정되었을 뿐이다.

</br>

## 동기화 (Synchronization) {#sec-cudajl_synchronization}

블록에서 스레드를 동기화하려면 `sync_threads()` 함수를 사용한다. predicate 를 취하는 보다 고급 변형도 사용 가능하다.

- `sync_threads_count(pred)` : `pred` 가 `true` 인 스레드의 갯수를 반환한다.

- `sync_threads_and(pred)` : 모든 스레드에서 `pred` 가 참이면 `true` 를 반환한다.

- `sync_threads_or(pred)` : 어떤 스레드에서 `pred` 가 참이면 `true` 를 반환한다.

여러 스레드 동기화 장벽을 유지하려면 장벽을 식별하는 정수 인수를 취하는 `barrier_sync` 함수를 사용한다.

워프에서 레인을 동기화하려면 `sync_warp()` 함수를 사용합니다. 이 함수는 참여할 레인을 선택하는 마스크를 취합니다(기본값은 `FULL_MASK`).

실행 장벽이 아닌 메모리 장벽만 필요한 경우 펜스 내장 함수를 사용합니다.

- `threadfence_block` : 블럭 내의 모든 쓰레드에서 메모리 정렬을 보장한다. 

- `threadfence` : 디바이스 내의 모든 쓰레드에서 메모리 정렬을 보장한다. 

- `threadfence_system` : 호스트 스레드와 peer 디바이스를 포함한 모든 스레드에서 메모리 정렬을 보장한다. 

</br>

### 공유 메모리 (Shared memory) {#sec-cudajl_shared_memory}

스레드 간 통신을 위해 공유 메모리로 백업된 디바이스 배열은 `CuStaticSharedArray` 함수를 통해 할당될 수 있다. 다음은 배열의 순서를 바꾸는 커널이다. 커널 내의 `b` 가 스레드간 통신을 위해 공유 메모리로 백업된 배열이다. 

```julia
function reverse_kernel(a::CuDeviceArray{T}) where T
    i = threadIdx().x
    b = CuStaticSharedArray(T, 2)
    b[2-i+1] = a[i]
    sync_threads()
    a[i] = b[i]
    return
end

a = cu([1,2])
@cuda threads = 2 reverse_kernel(a)
```

결과를 출력해보면 `a` 의 순서가 바뀌었음을 알 수 있다.

</br>

공유 메모리의 크기를 미리 알 수 없고 각 크기에 대해 커널을 다시 컴파일하고 싶지 않은 경우 대신 `CuDynamicSharedArray` 타입을 사용할 수 있다. 이를 위해서는 공유 메모리의 크기(바이트)를 커널에 인수로 전달해야 한다.

```julia
function reverse_kernel(a::CuDeviceArray{T}) where T
    i = threadIdx().x
    b = CuDynamicSharedArray(T, length(a))
    b[length(a)-i+1] = a[i]
    sync_threads()
    a[i] = b[i]
    return
end

a = cu([1,2,3])
@cuda threads=length(a) shmem=sizeof(a) reverse_kernel(a)
```

동적 공유 메모리를 사용하는 다수의 배열이 필요한 경우 후속 `CuDynamicSharedArray` 생성자에 공유 메모리 시작부터 바이트 단위의 오프셋을 나타내는 오프셋 매개변수를 전달한다. `@cuda` 에 대한 `shmem` 키워드는 모든 배열에서 사용하는 총 공유 메모리 양이어야 합니다.

</br>

### 경계 확인 {#sec-cudajl_boundary_check}

기본적으로 `CuDeviceArray` 를 인덱싱하면 경계 검사를 수행하고 인덱스가 경계를 벗어나면 오류를 발생시키는데 이는 비용이 많이 드는 작업이므로 인덱스가 경계 내에 있다는 것이 확실하다면 일반적인 배열과 마찬가지로 `@inbounds` 를 사용 할 수 있다.

</br>

## 표준 출력 {#sec-cudalj_standard_output}

`CUDA.jl` 커널은 아직 Julia의 표준 입출력과 통합되지 않았지만 커널에서 표준 출력으로 인쇄하기 위한 몇 가지 기본 기능을 제공한다.

- `@cuprintf`: 표준 출력으로 형식화된 출력을 내보낸다.

- `@cuprint` 와 `@cuprintln` : 문자를 포함한 값을 표준 출력으로 내보낸다.

- `@cushow` : 객체의 이름과 값을 출력한다.

`@cuprintf` 매크로는 모든 형식 옵션을 지원하지 않는다. 자세한 내용은 `printf` 에 대한 NVIDIA 설명서를 참조하라. `@cuprintln` 과 `CUDA.jl` 을 통해 모든 값을 적절한 문자열 표현으로 변환하는 것이 더 편리한 경우가 많다.

```julia
julia> @cuda threads=2 (()->(@cuprintln("Hello, I'm thread $(threadIdx().x)!"); return))()
Hello, I'm thread 1!
Hello, I'm thread 2!
```

</br>
단순히 값만 출력하길 원한다면, which can be useful during debugging, `@cushow` 를 사용하라.

```julia
julia> @cuda threads=2 (()->(@cushow threadIdx().x; return))()
(threadIdx()).x = 1
(threadIdx()).x = 2
```

이것들은 매우 제한된 수의 유형만 지원한다는 점에 유의하라. 따라서 디버깅 목적으로만 사용하는 것이 좋다. P

</br>

## 난수 발생 {#sec-cudajl_random_number}

커널에서 `rand` 혹은 `randn` 함수를 통해 난수 샘플을 얻을 수 있으며 이 때 GPU-호환 난수 발생기(GPU-compatible random number generator) 가 사용된다. API는 CPU에서 사용되는 난수 생성기와 매우 유사하지만 병렬 RNG의 설계에서 비롯된 몇 가지 차이점과 고려해야할 항이 있다.

- 기본 RNG는 글로벌 상태를 사용한다. 여러개의 인스턴스를 사용하는 것은 정의되지 않은 동작이다.

- 커널은 host 에서 전달된 고유한 seed 로 RNG를 자동으로 시드하여 동일한 커널을 여러 번 호출해도 다른 결과가 생성되도록 한다.
  
- `Random.seed!` 를 호출하여 seed 를 수동으로 지정하는 것이 가능하지만 RNG는 워프 공유 상태를 사용하므로 워프당 최소 하나의 스레드가 seed 되어야 하며 워프 내의 모든 seed 는 동일해야 한다.

- 후속 커널 호출이 난수를 계속 발생시켜야 하는 경우 seed 뿐만 아니라 카운터 값도 `Random.seed!` 를 사용하여 수동으로 구성해야 한다. 여기에 대한 예는 `CUDA.jl` 의 host 측 RNG를 참조하라.

</br>

## Atomics {#sec-cudajl_atomics}

CUDA.jl 은 두 레벨의 추상화를 통해 아토믹 연산을 제공한다

- 저수준 : `atomic_` 함수는 직접 hardware instruction 에 매핑한다.

- 고수준 : `CUDA.@atomic` 은 편리한 성분별 연산(element-wise operation) 을 제공한다.

저수준 추상화는 안정적이고 이후의 동작을 변경하지 않으므로 아토믹 연산을 사용하는 가장 안전한 방법이다. 그러나 이 경우 인터페이스는 제한적이며, 하드웨어가 제공하는 것만 지원하고, 입력 타입이 일차할것을 요구한다. `CUDA.@atomic` API는 훨씬 더 사용자 친화적이지만 Julia Base 의 `@atomic` 매크로와 통합되면 어느 시점에서 사라질 것이다.


</br>

### 저수준 {#sec_cudajl_atomics_low_level}

저수준 아토믹 내재 함수는 포인터 입력을 사용하며 이는 `CuArray` 에서 포인터 함수를 호출하여 얻을 수 있다.

```julia
julia> function atomic_kernel(a)
           CUDA.atomic_add!(pointer(a), Int32(1))
           return
       end

julia> a = cu(Int32[1])
1-element CuArray{Int32, 1, CUDA.DeviceMemory}:
 1

julia> @cuda atomic_kernel(a)

julia> a
1-element CuArray{Int32, 1, CUDA.DeviceMemory}:
 2
```

지원되는 아토믹 연산은 다음과 같다.

- 이항연산 : `add`, `sub`, `and`, `or`, `xor`, `min`, `max`, `xchg`

- NVIDIA-특정적인 이항연산 : `inc`, `dec`

- 비교와 교환 : `cas`

자세한 유형 지원 및 하드웨어 요구 사항은 해당 내장 함수의 설명서를 참조하라.

</br>

### 고수준 {#sec-cudajl_atomic_high_level}

배열에 대한 아토믹 연산을 위해 `CUDA.@atomic` 매크로를 사용한다.

```julia
function atomic_kernel(a)
    CUDA.@atomic a[1] += 1
    return
end

a = CUDA.zeros(3)

@cuda atomic_kernel(a)
```

이 결과로 `a` 는 다음과 같다.

```julia
3-element CuArray{Float32, 1, CUDA.DeviceMemory}:
 0.0
 0.0
 0.0
```

이 매크로는 훨씬 더 관대한 특성을 가지며, 입력값을 적절한 타입으로 자동 변환하고, 지원되지 않는 연산에 대해서는 아토믹 비교-교환 루프로 대체된다. 하지만, 이는 `CUDA.jl` 이 Julia Base의 `@atomic` 매크로와 통합되면 사라질 가능성이 있다.

</br>

## 워프 내장 함수(Warp intrinsics) {#sec-cudajl_warp_intrinsics}

CUDA의 대부분의 워프 내장 함수는 `CUDA.jl` 에서 유사한 이름으로 사용할 수 있다. 이들의 동작은 대부분 동일하지만 몇 가지 차이점이 있다. 이들은 `1` 부터 시작하는 인덱스를 사용하며, 입력을 자동으로 변환하고 분할(일정 부분)하여 더 많은 타입을 지원한다.

- 인덱싱 : `laneid`, `lanemask`, `active_mask`, `warpsize`

- 셔플 : `shfl_sync`, `shfl_up_sync`, `shfl_down_sync`, `shfl_xor_sync`

- 투표 : `vote_all_sync`, `vote_any_sync`, `vote_unisync`, `vote_ballot_sync`

이 내장 함수들 중 많은 수가 마스크 인수를 필요로 하며, 이는 어떤 레인이 연산에 참여해야 하는지를 나타내는 비트 마스크이다. 모든 레인을 기본값으로 설정하려면 `FULL_MASK` 상수를 사용하면 된다.

</br>

## 동적 병렬성 (Dynamic parallelism) {#sec-cudajl_dynamic_parallelism}

커널은 일반적으로 호스트에서 실행되지만, 동적 병렬성을 사용하면 커널 내부에서 다른 커널을 실행할 수도 있다. 이는 재귀 알고리즘이나 동적으로 새로운 작업을 생성해야 하는 알고리즘에 유용하다. 동적 병렬성을 활용하면 GPU 내에서 작업 요구 사항에 맞게 추가적인 병렬 작업을 생성하고 실행할 수 있어 유연성과 효율성을 높일 수 있다.

```julia
function outer()
    @cuprint("Hello ")
    @cuda dynamic=true inner()
    return
end

function inner()
    @cuprintln("World!")
    return
end

@cuda outer();
```
```txt
Hello World!
```

</br>

커널 내에서는 제한된 하위 집합의 CUDA API만 사용할 수 있다. 여기에는 주로 다음과 같은 기능들이 포함된다.

- 동기화 : `device_synchronize` 함수를 통해 동기화를 수행한다.

- 스트림 : `CuDeviceStream` 생성자와 `unsafe_destroy!` 소멸자를 사용할 수 있다. 이러한 스트림은 `@cuda` 매크로를 사용할 때 stream 키워드 인수를 통해 전달할 수 있다.

</br>

## 협력 그룹 {#sec-cudajl_cooperative_groups}