---
title: "CUDA.jl 성능 팁"

number-sections: true
number-depth: 2
crossref:
  chapters: false
---

::: {.callout-note appearance="minimal"}

`CUDA.jl` 의 [Performance Tips](https://cuda.juliagpu.org/stable/tutorials/performance/#Julia-Specific-Tips) 을 요약 & 번역한 것이다.

:::


## 일반적인 팁


항상 코드 프로파일링부터 시작한다(자세한 내용은 [profiling](https://cuda.juliagpu.org/stable/development/profiling/) 페이지 참조). 먼저 `CUDA.@profile` 또는 `NSight Systems` 를 사용하여 프로그램 전체를 분석하고 핫스팟과 병목지점을 파악해야 한다. 여기에 집중하면서 다음을 수행한다.

- CPU와 GPU 간의 데이터 전송을 최소할 것. 불필요한 메모리 사본을 제거하고 다수의 적은 전송을 일괄적인 큰 전송으로 한다. 
  
- 문제가 있는 커널 호출을 식별한다. 단일 호출로 처리할 수 있는 수천 개의 커널을 동작시킬 수 있다.

- CPU가 GPU를 바쁘게 유지할 만큼 빠르게 작업을 제출하지 않는 스톨을 찾습니다.
 
이것으로 충분하지 않고 느리게 실행되는 커널을 식별한 경우 NSight Compute를 사용하여 해당 커널을 자세히 분석해 볼 수 있다. 중요도 순으로 시도해야 할 몇 가지 사항은 다음과 같다.

- 메모리 액세스를 최적한다. 예를 들어 불필요한 전역 접근(대신 공유 메모리에서 버퍼링)을 피하거나 접근을 병합한다.

- 각 스트리밍 멀티프로세서(SM)에서 더 많은 스레드를 실행한다. 이는 레지스터 압력을 낮추거나 공유 메모리 사용을 줄임으로써 달성할 수 있다. 아래 팁은 레지스터 압력을 줄일 수 있는 다양한 방법을 설명한다.

- `Float64` 및 `Int`/`Int64` 와 같은 64비트 유형 대신 `Float32` 및 `Int32` 와 같은 32비트 타입을 사용한다.
  
- 같은 워프의 스레드가 갈라지는 원인이 되는 제어 흐름 사용을 피하세요. 즉, `while` 또는 `for` 루프가 전체 워프에서 동일하게 동작하도록 하고, 워프 내에서 갈라지는 `if` 를 `if else` 로 바꾼다. 
  
- GPU가 메모리 액세스의 지연 시간을 숨길 수 있도록 계산 강도를 높인다.

</br>

### Inlining

Inlining 은 레지스터 사용을 줄여 커널 속도를 높일 수 있다. 모든 함수의 인라인을 강제로 실행하려면 `@cuda always_inline=true` 를 사용하면 된다.

</br>

### 쓰레드 당 최대 레지스터 개수를 제한한다.

시작할 수 있는 스레드 수는 부분적으로 커널이 사용하는 레지스터 수에 의해 결정된다. 이는 멀티프로세서의 모든 스레드에서 레지스터가 공유되기 때문이다. 스레드당 최대 레지스터 수를 설정하면 사용되는 레지스터가 줄어들어 스레드 수가 늘어나고 레지스터를 로컬 메모리에 분산시키는 댓가로 성능이 향상될 수 있다. 최대 레지스터 수를 32로 설정하려면 `@cuda maxregs=32` 라고 한다.


### `FastMath`

일반적인 산술 함수의 고속 버젼을 사용하는 `@fastmath` 매크로를 사용할 수 있다. 다음의 매크로를 사용하라. `@cuda fastmath=true`

</br>

## Julia 전용 팁

### 런타임 예외를 최소화한다

Julia 에서 많은 일반적인 연산은 런타임에 Error 를 발생시킬 수 있으며, Error 는 종종 분기를 만들고 해당 분기에서 함수를 호출하는데, 이 둘 다 GPU 에서는 느리다. 배열을 인덱싱할 때 `@inbounds` 를 사용하면 경계 검사로 인한 예외가 제거된다. `--check-bounds=yes` (Pkg.test의 기본값) 로 코드를 실행하면 항상 경계 검사가 발생한다. `LLVM.jl` 패키지의 `assume` 을 사용하여 예외를 제거할 수도 있다. 다음을 보라.

```julia
using LLVM.Interop

function test(x, y)
    assume(x > 0)
    div(y, x)
end
```

`assume(x > 0)` 은 컴파일러에게 `0` 으로 나누는 에러가 발생하지 않을 것임을 말해준다.


</br>

### 32 비트 정수

가능하면 32비트 정수를 하용한다. 레지스터 압력의 일반적인 원인은 32비트만 필요한데 64비트 정수를 사용하는 것이다. 예를 들어, 하드웨어의 인덱스는 32비트 정수이지만 Julia의 리터럴은 `Int64` 로, `blockIdx().x-1` 과 같은 표현식은 64비트 정수로 승격된다. 32비트 정수를 사용하려면 `1` 을 `Int32(1)` 로 대체하거나 CUDA를 사용하여 실행하는 경우 더 간결하게 `1i32` 로 대체할 수 있다.

이것이 얼마나 큰 차이를 만드는지 확인하기 위해 소개 튜토리얼에서 소개한 커널을 사용해 보자.

```julia
using CUDA, BenchmarkTools

function gpu_add3!(y, x)
    index = (blockIdx().x - 1) * blockDim().x + threadIdx().x
    stride = gridDim().x * blockDim().x
    for i = index:stride:length(y)
        @inbounds y[i] += x[i]
    end
    return
end
```

```txt
gpu_add3! (generic function with 1 method)
```

몇개의 레지스터가 사용되었는지 확인해보자.

```julia
x_d = CUDA.fill(1.0f0, 2^28)
y_d = CUDA.fill(2.0f0, 2^28)

CUDA.registers(@cuda gpu_add3!(y_d, x_d))
```

```txt
29
```

위의 결과는 기기마다 다를 수 있다. 이제 32 비트 정수를 사용하는 커널은 아래와 같다.

```julia
function gpu_add4!(y, x)
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    stride = gridDim().x * blockDim().x
    for i = index:stride:length(y)
        @inbounds y[i] += x[i]
    end
    return
end
```

```txt
gpu_add4! (generic function with 1 method)
```

```julia
CUDA.registers(@cuda gpu_add4!(y_d, x_d))
```

```txt
28
```

따라서 32비트 정수로 전환하여 레지스터 하나를 덜 사용하게 되고, 64비트 정수를 더 많이 사용하는 커널에서는 레지스터 수가 더 크게 감소할 것으로 예상된다.

</br>

### StepRange 사용을 피하라

`for` 루프의 이전 커널에서 `StepRange` 인 `index:stride:length(y)` 를 순회했다. 안타깝게도 `StepRange` 를 구성하는 것은 오류가 발생할 수 있고, 단순히 반복하고 싶을 경우에는 불필요한 계산을 포함하기 때문에 느립니다. 대신 다음과 같이 `while` 루프를 사용하는 것이 더 빠르다.

```julia
function gpu_add5!(y, x)
    index = (blockIdx().x - Int32(1)) * blockDim().x + threadIdx().x
    stride = gridDim().x * blockDim().x

    i = index
    while i <= length(y)
        @inbounds y[i] += x[i]
        i += stride
    end
    return
end
```

```txt
gpu_add5! (generic function with 1 method)
```

벤치마크는

```julia
function bench_gpu4!(y, x)
    kernel = @cuda launch=false gpu_add4!(y, x)
    config = launch_configuration(kernel.fun)
    threads = min(length(y), config.threads)
    blocks = cld(length(y), threads)

    CUDA.@sync kernel(y, x; threads, blocks)
end

function bench_gpu5!(y, x)
    kernel = @cuda launch=false gpu_add5!(y, x)
    config = launch_configuration(kernel.fun)
    threads = min(length(y), config.threads)
    blocks = cld(length(y), threads)

    CUDA.@sync kernel(y, x; threads, blocks)
end
```

```txt
bench_gpu5! (generic function with 1 method)
```

```julia
@btime bench_gpu4!($y_d, $x_d)
```

```txt
  76.149 ms (57 allocations: 3.70 KiB)
```

```julia
@btime bench_gpu5!($y_d, $x_d)
```

```txt
  75.732 ms (58 allocations: 3.73 KiB)
```

이 벤치마크는 이 커널에 대한 성능 이점에서는 미미하지만 `StepRange` 를 사용할 때 28개의 레지스터가 사용되었다는 점을 상기하면 사용된 레지스터의 양에 큰 차이가 있음을 확인 할 수 있다.

```julia
CUDA.registers(@cuda gpu_add5!(y_d, x_d))
```

```txt
12
```