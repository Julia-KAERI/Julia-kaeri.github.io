---
title: "Programming Model"

number-sections: true
number-depth: 2
crossref:
  chapters: false
---

## 커널

- CUDA C++는 프로그래머가 커널이라는 C++ 함수를 정의할 수 있도록 하여 C++를 확장한 것.

- 이 함수는 호출되면 N개의 서로 다른 CUDA 스레드에 의해 병렬로 N번 실행된다. 반면 일반 C++ 함수는 한 번만 실행된다.

</br>

간단한 커널의 예는 아래와 같다.

```c++
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}
```


</br>

## 스레드 계층 구조 : 스레드 $\rightarrow$ 블럭 $\rightarrow$ 그리드 {#sec-cuda_thread_hierarchy}

- `threadIdx` 는 3차원 벡터로 1, 2, 3 차원 스레드 인덱스를 이용하여 스레드를 특정할 수 있다. 

- 스레드 블럭(Thread block) : 1, 2 혹은 3차원의 스레드 인덱스의 집합


</br>

- 스레드 인덱스와 스레드 ID

    - 1차원 블럭 : ID = 인덱스

    - 2차원 블럭 : 블럭 사이즈가 $(D_x, D_y)$ 일 경우 스레드 인덱스 $(x,\,y)$ 에 대한 $\text{ID} = x+yD_x$.

    - 3차원 블럭 : 블걱 사이즈가 $(D_x, D_y, D_z)$ 일 경우 스레드 인덱스 $(x,\,y,\,z)$ 에 대한 $\text{ID} = x+yD_x + zD_x D_y$. 


</br>

- 블럭당 스레드 수 제한  : 블럭의 모든 스레드는 같은 SM(streaming multiprocess) 코어 안에 있어야 하며 그 코어의 제한된 메모리 자원을 공유한다. 현재의 GPU 에서 하나의 블럭은 최대 1024 개의 스레드를 포함 할 수 있다.


- 커널은 <u>같은 형상의 다수의 스레드 블럭</u>에서 실행되며, 따라서 총 스레드 수는 블럭수 와 블럭당 스레드 수의 곱이다.

- 그리드 (Grid) 는 블럭의 집합이며, 1, 2 혹은 3차원 인덱스를 갖는다. <u>한 그리드의 스레드 블럭의 갯수는 처리하고자 하는 데이터의 크기에 의해 지정되며, 보통 시스템의 프로세서 수 보다 크다</u>.

</br>

![Grid of Thread Blocks](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/grid-of-thread-blocks.png){#fig-cuda_grid_of_thread_blocks width=500}

</br>

행렬곱에 대한 코드와 실행에 대한 아래의 코드를 보자. 

```c++
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```


- `<<<...>>>` 구문에서 지정된 블록당 스레드 수와 그리드당 블록 수는 `int` 또는 `dim3` 타입이다. 2차원 블록 또는 그리드는 위의 예와 같이 지정할 수 있다.

- 그리드 내의 각 블록은 커널 내에서 내장된 `blockIdx` 변수를 통해 액세스할 수 있는 1차원, 2차원 또는 3차원 고유 인덱스로 식별할 수 있다. 스레드 블록의 차원은 커널 내에서 내장된 `blockDim` 변수를 통해 액세스할 수 있다.

- 위 코드의 경우 이 경우 임의적이기는 하지만 16x16(256개 스레드)의 스레드 블록 크기가 일반적인 선택이다. 그리드는 행렬 요소당 하나의 스레드를 가질 수 있을 만큼 충분한 블록으로 생성된다. 단순화를 위해 이 예에서는 각 차원의 그리드당 스레드 수가 해당 차원의 블록당 스레드 수로 균등하게 나누어진다고 가정하지만 반드시 그럴 필요는 없다.

- 스레드 블록은 독립적으로 실행될 수 있어야 한다. 병렬 또는 직렬로 어떤 순서로든 실행할 수 있어야 한다. 이러한 독립성으로 인해 [앞장의 그림 3](01_introduction.qmd#fig-cuda_scalability) 에서 볼 수 있듯이 스레드 블록을 모든 코어 수에 걸쳐 어떤 순서로든 예약할 수 있으므로 프로그래머는 코어 수에 따라 확장되는 코드를 작성할 수 있다.

- 블록 내의 스레드는 일부 공유 메모리를 통해 데이터를 공유하고 실행을 동기화하여 메모리 액세스를 조정함으로써 협력할 수 있다. 더 정확하게 말하면 `__syncthreads()` 내장 함수를 호출하여 커널에서 동기화 지점을 지정할 수 있다. `__syncthreads()` 는 어떤 허락이 떨어지기 전까지 블록의 모든 스레드가 진행을 멈추고 기다려야 하는 장벽 역할을 한다. [공유 메모리](03_programming_interface.qmd#sec-shared_memory) 에는 공유 메모리를 사용하는 예를 제시한다. `__syncthreads()` 외에도 [Cooperative Groups](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cooperative-groups) API는 풍부한 스레드 동기화 기본 요소를 제공한다.

- 효율적인 협력을 위해 공유 메모리는 각 프로세서 코어 근처의 저지연 메모리(L1 캐시와 매우 유사)가 될 것으로 예상되고 `__syncthreads()` 는 가벼울 것으로 예상된다.

</br>

### Thread Block Cluseter {#sec-cuda_thread_block_cluster}

[NVIDIA Compute Capability 9.0](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-capability-9-0) 부터 CUDA 프로그래밍 모델은 스레드 블록으로 구성된 스레드 블록 클러스터라는 선택적 계층 구조를 도입한다. 스레드 블록의 스레드가 스트리밍 멀티프로세서에서 공동 스케줄링되는 것과 유사하게 클러스터의 스레드 블록도 GPU의 GPU 처리 클러스터(GPC)에서 공동 스케줄링된다.

스레드 블록과 유사하게 클러스터도 스레드 블록 클러스터 그리드에서 설명한 대로 1차원, 2차원 또는 3차원으로 구성된다. 클러스터의 스레드 블록 수는 사용자가 정의할 수 있으며 클러스터의 최대 8개 스레드 블록이 CUDA에서 이식 가능한 클러스터 크기로 지원된다. 8개의 멀티프로세서를 지원하기에는 너무 작은 GPU 하드웨어 또는 MIG 구성에서는 최대 클러스터 크기가 그에 따라 줄어들게된다. 8개 이상의 스레드 블록 클러스터 크기를 지원하는 더 큰 구성뿐만 아니라 이러한 더 작은 구성을 식별하는 것은 아키텍처에 따라 다르며 `cudaOccupancyMaxPotentialClusterSize` API를 사용하여 쿼리할 수 있다.

![Grid of Thread Block Clusters](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/grid-of-clusters.png){#fig-cuda_gird_of_thread_block_clusters}


::: {.callout-note}

In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the [Cluster Group](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cluster-group-cg) API.

:::

</br>

스레드 블록 클러스터는 `__cluster_dims__(X,Y,Z)` 를 사용하는 컴파일러 시간 커널 속성을 사용하거나 CUDA 커널 실행 API `cudaLaunchKernelEx` 를 사용하여 커널에서 활성화할 수 있다. 아래 예는 컴파일러 시간 커널 속성을 사용하여 클러스터를 실행하는 방법을 보여준다. 커널 속성을 사용하는 클러스터 크기는 컴파일 시간에 고정되고 그런 다음 고전적인 `<<< , >>>` 를 사용하여 커널을 실행할 수 있다. 커널이 이런 컴파일 시간의 클러스터 크기를 사용하는 경우 커널을 실행할 때 클러스터 크기를 수정할 수 없다.


```c++
// Kernel definition
// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension
__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    // Kernel invocation with compile time cluster size
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // The grid dimension is not affected by cluster launch, and is still enumerated
    // using number of blocks.
    // The grid dimension must be a multiple of cluster size.
    cluster_kernel<<<numBlocks, threadsPerBlock>>>(input, output);
}
```
</br>

스레드 블록 클러스터 크기는 런타임에 설정할 수도 있고, 커널은 CUDA 커널 실행 API `cudaLaunchKernelEx` 를 사용하여 실행할 수 있다. 아래 코드 예제는 확장 가능한 API를 사용하여 클러스터 커널을 실행하는 방법을 보여준다.

```c++
// Kernel definition
// No compile time attribute attached to the kernel
__global__ void cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // Kernel invocation with runtime cluster size
    {
        cudaLaunchConfig_t config = {0};
        // The grid dimension is not affected by cluster launch, and is still enumerated
        // using number of blocks.
        // The grid dimension should be a multiple of cluster size.
        config.gridDim = numBlocks;
        config.blockDim = threadsPerBlock;

        cudaLaunchAttribute attribute[1];
        attribute[0].id = cudaLaunchAttributeClusterDimension;
        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension
        attribute[0].val.clusterDim.y = 1;
        attribute[0].val.clusterDim.z = 1;
        config.attrs = attribute;
        config.numAttrs = 1;

        cudaLaunchKernelEx(&config, cluster_kernel, input, output);
    }
}
```

compute capability 9.0 호환 GPU에서 클러스터의 모든 스레드 블록은 단일 GPU 처리 클러스터(GPC)에서 공동 스케줄링되도록 보장되며 클러스터의 스레드 블록이 [Cluster Group](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cluster-group-cg) API `cluster.sync()` 를 사용하여 하드웨어가 지원하는 동기화를 수행할 수 있다. Cluster Group은 또한 `num_threads()` 및 `num_blocks()` API를 사용하여 스레드 수 또는 블록 수 측면에서 클러스터 그룹 크기를 쿼리하는 멤버 함수를 제공한다. 클러스터 그룹의 스레드 또는 블록 순위는 `dim_threads()` 및 `dim_blocks()` API를 사용하여 각각 쿼리할 수 있다.

클러스터에 속한 스레드 블록은 [분산 공유 메모리(Distributed shared memory)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#distributed-shared-memory)에 액세스할 수 있다. 클러스터의 스레드 블록은 분산 공유 메모리의 모든 주소를 읽고, 쓰고, 원자성을 수행할 수 있다. [분산 공유 메모리(Distributed shared memory)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#distributed-shared-memory)에는 분산 공유 메모리에서 히스토그램을 수행하는 예를 제시한다.

</br>

## 메모리 계층 구조 {#sec-cuda_memory_hierarchy}

![Memory Hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/memory-hierarchy.png){#fig-cuda_memory_hierarchy}

CUDA 스레드는 @fig-cuda_memory_hierarchy 에서 설명한 대로 실행 중에 여러 메모리 공간에서 데이터에 액세스할 수 있다. 각 스레드에는 개별적인 로컬 메모리가 있다. 각 스레드 블록에는 블록의 모든 스레드에서 볼 수 있고 블록과 동일한 수명을 가진 공유 메모리가 있다. 스레드 블록 클러스터의 스레드 블록은 서로의 공유 메모리에서 읽기, 쓰기 및 아토믹 연산을 수행할 수 있다. 모든 스레드는 동일한 전역 메모리(Global memory)에 액세스할 수 있다.

또한 모든 스레드에서 액세스할 수 있는 읽기 전용 메모리 공간이 추가로 존재한다 - 상수 메모리 공간과 텍스처 메모리 공간. 전역, 상수 및 텍스처 메모리 공간은 다양한 메모리 사용에 맞게 최적화되어 있다([장치 메모리 액세스](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-accesses) 참조). 텍스처 메모리는 또한 일부 특정 데이터 형식에 대해 다양한 주소 지정 모드와 데이터 필터링을 제공한다([텍스처 및 표면 메모리](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#texture-and-surface-memory) 참조).

글로벌, 상수 및 텍스처 메모리 공간은 동일한 애플리케이션에서 커널을 시작할 때에도 지속된다.


</br>

## 이기종 프로그래밍 (Heterogeneous Programming) {#sec-cuda_heterogeneous_programming}

![Heterogeneous Programming](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/heterogeneous-programming.png){#fig-cuda_hetergeneous_programming}

</br>

- @fig-cuda_hetergeneous_programming 에서 알 수 있듯이 CUDA 프로그래밍 모델은  C++ 프로그램을 실행하는 호스트에 대한 보조 프로세서로 작동하는 물리적으로 분리된 장치에서 CUDA 스레드가 실행된다고 가정한다. 예를 들자면 커널이 GPU에서 실행되고 나머지 C++ 프로그램이 CPU에서 실행되는 경우이다. 이후 GPU 가 실행되는 분리된 장치를 디바이스라고 부르겠다. (호스트와 디바이스라는 명칭으로 나누는 것은 일종의 관례이다)

- CUDA 프로그래밍 모델은 또한 호스트와 디바이스가 각각 호스트 메모리와 디바이스 메모리라고 하는 DRAM에서 자체적인 별도 메모리 공간을 유지한다고 가정한다. 따라서 프로그램은 CUDA 런타임([프로그래밍 인터페이스](03_programming_interface.qmd)에서 설명)에 대한 호출을 통해 커널에서 볼 수 있는 전역, 상수 및 텍스처 메모리 공간을 관리한다. 여기에는 디바이스 메모리 할당 및 할당 해제와 호스트와 디바이스 메모리 간의 데이터 전송이 포함된다.

- Unified Memory 는 호스트와 디바이스 메모리 공간을 연결하는 관리되는 메모리를 제공한다. 관리되는 메모리는 시스템의 모든 CPU와 GPU에서 공통 주소 공간이 있는 단일의 일관된 메모리 이미지로 액세스할 수 있다. 이 기능은 디바이스 메모리에 대한 초과 예약(oversubcription)을 가능하게 하며 호스트와 디바이스에서 데이터를 명시적으로 미러링할 필요성을 없애 애플리케이션 포팅 작업을 크게 단순화할 수 있다. Unified Memory에 대한 소개는 [Unified Memory Programming](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#um-unified-memory-programming-hd) 을 참조하라.

</br>

## 비동기적 SIMT 프로그래밍 모델 {#sec-cuda-asynchronous_simt_programming_model}

CUDA 프로그래밍 모델에서 스레드는 계산이나 메모리 작업을 수행하기 위한 가장 낮은 수준의 추상화이다. NVIDIA Ampere GPU 아키텍처 기반 장치부터 시작하여 CUDA 프로그래밍 모델은 비동기 프로그래밍 모델을 통해 메모리 작업에 가속을 제공한다. 비동기 프로그래밍 모델은 CUDA 스레드와 관련하여 비동기 연산의 행동을 정의한다.

비동기 프로그래밍 모델은 CUDA 스레드 간 동기화를 위한 [비동기 배리어](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#aw-barrier) 의 동작을 정의한다. 이 모델은 또한 [`cuda::memcpy_async`](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#asynchronous-data-copies)를 사용하여 GPU에서 계산하는 동안 글로벌 메모리에서 비동기적으로 데이터를 이동하는 방법을 설명하고 정의한다.

</br>

### 비동기 연산 {#sec-asynchronous_operations}

비동기 연산은 한 CUDA 스레드에서 시작하여 다른 스레드에서와 같이 비동기적으로 실행되는 연산으로 정의된다. 잘 구성된 프로그램에서 하나 이상의 CUDA 스레드가 비동기 연산과 동기화된다. 비동기 작업을 시작한 CUDA 스레드는 동기화 스레드에 포함될 필요가 없다.

이러한 비동기 스레드(as-if 스레드)는 항상 비동기 연산을 시작한 CUDA 스레드와 연결됩니다. 비동기 연산은 동기화 객체를 사용하여 작업 완료를 동기화한다. 이러한 동기화 객체는 사용자가 명시적으로 관리하거나(예: `cuda::memcpy_async`) 라이브러리 내에서 암묵적으로 관리할 수 있습니다(예: `cooperative_groups::memcpy_async`).

동기화 객체는 `cuda::barrier` 또는 `cuda::pipeline` 일 수 있다. 이러한 객체는  [비동기 Barrier](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#aw-barrier) 및 [`cuda::pipeline` 을 사용한비동기 데이터 복사](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#asynchronous-data-copies)에서 자세히 설명한다. 이러한 동기화 객체는 다른 스레드 범위에서 사용할 수 있다. scope 는 비동기 작업과 동기화하기 위해 동기화 객체를 사용할 수 있는 스레드 집합을 정의한다. 다음 표는 CUDA C++에서 사용 가능한 스레드 범위와 각각과 동기화할 수 있는 스레드를 정의합니다.

| Thread Scope | Description |
|:---------------------|:------------------------|
| `cuda::thread_scope::thread_scope_thread` | Only the CUDA thread which initiated asynchronous operations synchronizes.
| `cuda::thread_scope::thread_scope_block` | All or any CUDA threads within the same thread block as the initiating thread synchronizes.|
| `cuda::thread_scope::thread_scope_device` | All or any CUDA threads in the same GPU device as the initiating thread synchronizes.|
| `cuda::thread_scope::thread_scope_system` | All or any CUDA or CPU threads in the same system as the initiating thread synchronizes.|

</br>

이러한 스레드 scope 는 표준 C++ 라이브러리에 대한 확장인 [CUDA Standard C++ library](https://nvidia.github.io/libcudacxx/extended_api/memory_model.html#thread-scopes) 로 구현되었다.


</br>

## Compute Capability

장치의 컴퓨팅 기능은 버전 번호로 표현되며, 때로는 "SM 버전"이라고도 한다. 이 버전 번호는 GPU 하드웨어에서 지원하는 기능을 식별하며, 런타임에 애플리케이션에서 현재 GPU에서 사용할 수 있는 하드웨어 기능 및/또는 명령어를 확인하는 데 사용됩다. 컴퓨팅 기능은 주요 개정 번호 X와 부차 개정 번호 Y로 구성되며 X.Y로 표시된다.

동일한 주요 개정 번호가 있는 장치는 동일한 코어 아키텍처입니다. 주요 개정 번호는 NVIDIA Hopper GPU 아키텍처 기반 장치의 경우 9, NVIDIA Ampere GPU 아키텍처 기반 장치의 경우 8, Volta 아키텍처 기반 장치의 경우 7, ​​Pascal 아키텍처 기반 장치의 경우 6, Maxwell 아키텍처 기반 장치의 경우 5, Kepler 아키텍처 기반 장치의 경우 3 이다.

부차 개정 번호는 코어 아키텍처에 대한 점진적 개선에 해당하며, 새로운 기능이 포함될 수 있다.

Turing은 컴퓨팅 기능 7.5의 장치에 대한 아키텍처이며 Volta 아키텍처를 기반으로 하는 점진적 업데이트이다.

CUDA 지원 GPU는 모든 CUDA 지원 장치와 해당 컴퓨팅 기능을 나열한다. 컴퓨팅 기능은 각 컴퓨팅 기능의 기술 사양을 제공한다.

</br>

::: {.callout-warning icon="false"}

특정 GPU의 컴퓨팅 기능 버전은 CUDA 소프트웨어 플랫폼의 버전인 CUDA 버전(예: CUDA 7.5, CUDA 8, CUDA 9)과 다르다. CUDA 플랫폼은 애플리케이션 개발자가 아직 발명되지 않은 미래의 GPU 아키텍처를 포함하여 여러 세대의 GPU 아키텍처에서 실행되는 애플리케이션을 만드는 데 사용된다. CUDA 플랫폼의 새 버전은 종종 해당 아키텍처의 컴퓨팅 기능 버전을 지원하여 새 GPU 아키텍처에 대한 기본 지원을 추가하지만, CUDA 플랫폼의 새 버전은 일반적으로 하드웨어 세대와 독립적인 소프트웨어 기능도 포함한다.

:::