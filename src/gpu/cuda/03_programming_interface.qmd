---
title: "Programming Interface"

number-sections: true
number-depth: 3
crossref:
  chapters: false

---


- CUDA C++는 C++ 프로그래밍 언어에 익숙한 사용자에게 장치에서 실행할 프로그램을 쉽게 작성할 수 있는 단순한 경로를 제공한다. C++ 언어에 대한 최소한의 확장 세트와 런타임 라이브러리로 구성되며 핵심 언어 확장은 [Programming Model](02_programming_model.qmd)에 소개되었다. 프로그래머는 커널을 C++ 함수로 정의하고 새로운 구문을 사용하여 함수가 호출될 때마다 그리드와 블록 차원을 지정할 수 있다. 모든 확장에 대한 전체 설명은 [C++ 언어 확장](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#c-language-extensions)에서 찾을 수 있다. 이러한 확장 중 일부를 포함하는 모든 소스 파일은 [NVCC 컴파일](#sec-cuda_nvcc_compile)에 설명된 대로 `nvcc` 로 컴파일해야 합니다.

- 런타임에 대해서는 [CUDA 런타임](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-c-runtime) 을 참고하라. 호스트에서 실행되어 장치 메모리를 할당 및 할당 해제하고, 호스트 메모리와 장치 메모리 간에 데이터를 전송하고, 여러 장치가 있는 시스템을 관리하는 등의 작업을 수행하는 C 및 C++ 함수를 제공한다. 런타임에 대한 전체 설명은 CUDA Reference Manumal 에서 찾을 수 있다.

- 런타임은 하위 수준 C API인 CUDA 드라이버 API 위에 구축되며 애플리케이션에서도 액세스할 수 있다. 드라이버 API는 CUDA 컨텍스트(장치의 호스트 프로세스와 유사) 및 CUDA 모듈(장치의 동적으로 로드된 라이브러리와 유사)과 같은 하위 수준 개념을 노출하여 추가적인 제어 수준을 제공한다. 대부분의 애플리케이션은 드라이버 API를 사용하지 않는다. 이 추가적인 제어 수준이 필요하지 않고 런타임을 사용할 때 컨텍스트 및 모듈 관리가 암묵적이기 때문에 코드가 더 간결해진다. 런타임은 드라이버 API와 상호 운용 가능하므로 일부 드라이버 API 기능이 필요한 대부분의 애플리케이션은 기본적으로 런타임 API를 사용하고 필요한 경우에만 드라이버 API를 사용할 수 있다. 드라이버 API는 [드라이버 API](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#driver-api)에 소개되어 있으며 Reference manual 에 자세히 설명되어 있습니다.

</br>

## NVCC 컴파일 {#sec-cuda_nvcc_compile}

커널은 *PTX* 라고 하는 CUDA 명령어 세트 아키텍처를 사용하여 작성할 수 있으며, 이는 PTX Reference manual 에 설명되어 있다. 그러나 일반적으로 C++와 같은 고급 프로그래밍 언어를 사용하는 것이 더 효과적이다. 두 경우 모두 커널은 `nvcc` 에서 바이너리 코드로 컴파일하여 장치에서 실행해야 한다.

`nvcc` 는 C++ 또는 PTX 코드를 컴파일하는 프로세스를 간소화하는 컴파일러 드라이버이다. 간단하고 친숙한 명령줄 옵션을 제공하고 다양한 컴파일 단계를 구현하는 도구 모음을 호출하여 실행한다. 이 섹션에서는 `nvcc` 워크플로 및 명령 옵션에 대한 개요를 설명한다. 전체 설명은 nvcc user manual 에서 찾을 수 있다.

</br>

### 컴파일 workflow {#sec-cuda_compilation_workflow}

`nvcc` 로 컴파일된 소스 파일에는 호스트 코드(즉, 호스트에서 실행되는 코드)와 디바이스 코드(즉, 디바이스에서 실행되는 코드)가 혼합되어 포함될 수 있다. `nvcc` 의 기본 워크플로는 디바이스 코드를 호스트 코드에서 분리한 다음 다음을 수행하는 것으로 구성됩니다.

- 디바이스 코드를 어셈블리 형태(PTX 코드) 및/또는 바이너리 형태(cubin 객체)로 컴파일하고,
  
- Kernels에서 도입된 `<<<...>>>` 구문을 필요한 CUDA 런타임 함수 호출로 대체하여 호스트 코드를 수정하여 PTX 코드 및/또는 cubin 객체에서 각 컴파일된 커널을 로드하고 시작한다.


수정된 호스트 코드는 다른 도구를 사용하여 컴파일할 수 있는 C++ 코드로 출력되거나, 마지막 컴파일 단계에서 `nvcc` 가 호스트 컴파일러를 호출하여 직접 object code 로 출력된다. 이제 애플리케이션은 다음을 수행할 수 있다.

- 컴파일된 호스트 코드에 링크(가장 일반적인 경우)하거나

- 수정된 호스트 코드(있는 경우)를 무시하고 CUDA 드라이버 API(드라이버 API 참조)를 사용하여 PTX 코드 또는 cubin 객체를 로드하고 실행한다.

</br>

#### JIT 컴파일 {#sec-cuda_jit_compilation}

런타임에 애플리케이션에서 로드한 모든 PTX 코드는 장치 드라이버에 의해 바이너리 코드로 추가로 컴파일된다. 이를 JIT(just-in-time) 컴파일이라고 한다. JIT 컴파일은 애플리케이션 로드 시간을 늘리지만 애플리케이션이 각 새 장치 드라이버와 함께 제공되는 모든 새 컴파일러 개선 사항의 이점을 누릴 수 있도록 한다. 또한 애플리케이션이 컴파일된 당시 존재하지 않았던 장치에서 애플리케이션을 실행할 수 있는 유일한 방법이기도 하다([애플리케이션 호환성](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#application-compatibility)에서 자세히 설명).


장치 드라이버가 일부 애플리케이션에 대한 일부 PTX 코드를 JIT 컴파일할 때, 이후 애플리케이션 호출에서 컴파일을 반복하지 않기 위해 생성된 바이너리 코드의 사본을 자동으로 캐시한다. 캐시(컴퓨트 캐시라고 함)는 장치 드라이버가 업그레이드될 때 자동으로 무효화되므로 애플리케이션은 장치 드라이버에 내장된 새로운 JIT 컴파일러의 개선 사항의 이점을 누릴 수 있다.

환경 변수는 [CUDA 환경 변수](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#env-vars)에 설명된 대로 적시 컴파일을 제어하는 ​​데 사용할 수 있습니다.

`nvcc` 를 사용하여 CUDA C++ 장치 코드를 컴파일하는 대신 NVRTC를 사용하여 런타임에 CUDA C++ 장치 코드를 PTX로 컴파일할 수 있다. NVRTC는 CUDA C++용 런타임 컴파일 라이브러리이다. 자세한 내용은 NVRTC 사용자 가이드에서 확인할 수 있다.

</br>

### 바이너리 호환성 {#sec-cuda_binary_compatibility}

이진 코드는 아키텍처에 따라 다르다. cubin 객체는 대상 아키텍처를 지정하는 컴파일러 옵션 `-code` 를 사용하여 생성된다. 예를 들어, `-code=sm_80` 으로 컴파일하면 Compute Capability 8.0의 장치에 대한 이진 코드가 생성된다. 이진 호환성은 한 마이너 리비전에서 다음 리비전으로 보장되지만, 한 마이너 리비전에서 이전 리비전으로 또는 주요 리비전 간에는 보장되지 않는다. 즉, 컴퓨팅 기능 X.y에 대해 생성된 cubin 객체는 z≥y인 컴퓨팅 기능 X.z의 장치에서만 실행된다.

</br>

::: {.callout-warning}
바이너리 호환성은 데스크톱에서만 지원되며 Tegra에서는 지원되지 않는다. 또한 데스크톱과 Tegra 간의 바이너리 호환성도 지원되지 않는다.
:::

</br>

### PTX 호환성 {#sec-cuda_ptx_compatibility}

일부 PTX 명령어는 고성능 장치에서만 지원된다. 예를 들어, `Warp Shuffle`(https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions) 함수는 compute capability 5.0 이상의 장치에서만 지원된다. `-arch` 컴파일러 옵션은 C++를 PTX 코드로 컴파일할 때 가정하는 compute capability 를 지정합니다. 따라서 예를 들어 Warp Shuffle이 포함된 코드는 `-arch=compute_50` (또는 그 이상)으로 컴파일해야 한다.

특정 compute capability 를 위해 생성된 PTX 코드는 항상 더 크거나 같은 컴퓨팅 기능의 바이너리 코드로 컴파일할 수 있다. 이전 PTX 버전에서 컴파일된 바이너리는 일부 하드웨어 기능을 사용하지 못할 수 있다. 예를 들어, compute capability 6.0(Pascal)을 위해 생성된 PTX에서 컴파일된 compute capability 7.0(Volta) 장치를 대상으로 하는 바이너리는 Pascal에서 사용할 수 없는 Tensor Core 명령어를 사용하지 않는다. 결과적으로 최종 바이너리는 최신 버전의 PTX를 사용하여 바이너리를 생성한 경우보다 성능이 떨어질 수 있다.

아키텍처 조건부 기능을 대상으로 컴파일된 PTX 코드는 정확히 동일한 물리적 아키텍처에서만 실행되고 다른 곳에서는 실행되지 않습니다. 아치텍쳐 조건부 PTX 코드는 이전 혹은 이후와 호환되지 않는다. `sm_90a` 또는 `compute_90a` 로 컴파일된 예제 코드는 compute capability 9.0 이 있는 장치에서만 실행되며 이전 혹은 이후와 호환되지 않는다.

</br>

### 응용 프로그램 호환성 {#sec-cuda_application_compatibility}

특정 compute capability 의 장치에서 코드를 실행하려면 애플리케이션은 바이너리 호환성 및 PTX 호환성에서 설명한 대로 이 compute capability 과 호환되는 바이너리 또는 PTX 코드를 로드해야 한다. 특히, 더 높은 컴퓨팅 기능을 갖춘 미래 아키텍처에서 코드를 실행하려면(아직 바이너리 코드를 생성할 수 없는 경우) 애플리케이션은 이러한 장치에 대해 JIT 컴파일되는 PTX 코드를 로드해야 합니다([JIT 컴파일](#sec-cuda_jit_compilation) 참조).

CUDA C++ 애플리케이션에 어떤 PTX와 바이너리 코드가 포함되는지는 `nvcc` 사용자 매뉴얼에 자세히 설명된 대로 `-arch` 및 `-code` 컴파일러 옵션 또는 `-gencode` 컴파일러 옵션에 의해 제어된다. 예를 들어보자.

```txt
nvcc x.cu
        -gencode arch=compute_50,code=sm_50
        -gencode arch=compute_60,code=sm_60
        -gencode arch=compute_70,code=\"compute_70,sm_70\"
```

위의 컴파일은 compute capability 5.0 및 6.0(첫 번째 및 두 번째 `-gencode` 옵션)과 호환되는 바이너리 코드와 compute capability 7.0(세 번째 `-gencode` 옵션)과 호환되는 PTX 및 바이너리 코드를 embeds 한다.

호스트 코드는 런타임에 로드하고 실행할 가장 적합한 코드를 자동으로 선택하도록 생성됩니다. 위의 예에서 이는 다음과 같다.

- compute capability 5.0 및 5.2가 있는 장치의 경우 5.0 바이너리 코드,
- compute capability 6.0 및 6.1이 있는 장치의 경우 6.0 바이너리 코드,
- compute capability 7.0 및 7.5가 있는 장치의 경우 7.0 바이너리 코드,
- compute capability 8.0 및 8.6이 있는 장치의 경우 런타임에 바이너리 코드로 컴파일되는 PTX 코드.

</br>


`x.cu` 는 예를 들어, 워프 감소 연산을 사용하는 최적화된 코드 경로를 가질 수 있으며, 이는 compute capability 8.0 이상의 장치에서만 지원됩니다. `__CUDA_ARCH__` 매크로는 compute capability 에 따라 다양한 코드 경로를 구분하는 데 사용할 수 있다. 이는 장치 코드에 대해서만 정의됩니다. 예를 들어 `-arch=compute_80` 으로 컴파일할 때 `__CUDA_ARCH__` 는 `800` 과 같다.

`x.cu` 가 `sm_90a` 또는 `compute_90a` 를 사용하여 [아키텍처 조건부 기능](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#feature-availability) 예제에 대해 컴파일된 경우, 코드는 compute capability 9.0이 있는 장치에서만 실행할 수 있다.

드라이버 API를 사용하는 애플리케이션은 코드를 컴파일하여 파일을 분리하고 런타임에 가장 적합한 파일을 명시적으로 로드하여 실행해야 한다.

Volta 아키텍처는 GPU에서 스레드가 예약되는 방식을 변경하는 독립 스레드 스케줄링을 도입합니다. 이전 아키텍처에서 [SIMT 스케줄링](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#simt-architecture) 의 특정 동작에 의존하는 코드의 경우 독립 스레드 스케줄링은 참여 스레드 세트를 변경하여 잘못된 결과를 초래할 수 있다. [Independent Thread Scheduling](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#independent-thread-scheduling-7-x) 에서 자세히 설명한 시정 조치를 구현하는 동안 마이그레이션을 지원하기 위해 Volta 개발자는 컴파일러 옵션 조합 `-arch=compute_60  -code=sm_70`을 사용하여 Pascal의 스레드 스케줄링을 선택할 수 있습니다.

`nvcc` 사용자 설명서에는 `-arch`, `-code` 및 `-gencode` 컴파일러 옵션에 대한 다양한 약어가 나와 있다. 예를 들어, `-arch=sm_70` 은 `-arch=compute_70 -code=compute_70,sm_70` (`-gencode arch=compute_70,code=\"compute_70,sm_70\"` 과 동일)의 약어입니다.

</br>

### C++ 호환성 {#sec-cuda_cpp_compatibility}

컴파일러의 프런트 엔드는 C++ 구문 규칙에 따라 CUDA 소스 파일을 처리합니다. 호스트 코드에는 전체 C++가 지원된다. 그러나 [C++ 언어 지원](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#c-cplusplus-language-support) 에 설명된 대로 C++의 하위 집합만 장치 코드에 대해 완전히 지원된다.

</br>

### 64 비트 호환성 {#sec-cuda_64bit_compatibility}

`nvcc`의 64비트 버전은 64비트 모드에서 디바이스 코드를 컴파일한다(즉, 포인터는 64비트이다). 64비트 모드에서 컴파일된 디바이스 코드는 64비트 모드에서 컴파일된 호스트 코드에서만 지원된다.


## CUDA Runtime

런타임은 `cudart` 라이브러리에서 구현되며, `cudart.lib` 또는 `libcudart.a` 를 통해 정적으로 또는 `cudart.dll` 또는 `libcudart.so` 를 통해 동적으로 응용프로그램에 링크된다. 동적 링크에 `cudart.dll` 및/또는 `cudart.so` 가 필요한 애플리케이션은 일반적으로 이를 애플리케이션 설치 패키지의 일부로 포함한다. CUDA 런타임 심볼의 주소는 CUDA 런타임의 동일한 인스턴스에 링크하는 구성 요소 사이에서만 안전하게 전달할 수 있다.

모든 진입점에는 `cuda` 라는 접두사가 붙습니다.

[이기종 프로그래밍(Heterogeneous Programming)](02_programming_model.qmd#sec-cuda_heterogeneous_programming) 에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리가 있는 호스트와 디바이스로 구성된 시스템을 가정한다. [장치 메모리](#sec-cuda_device_memory) 에서 장치 메모리를 관리하는 데 사용되는 런타임 함수에 대해 소개한다.

[공유 메모리](#sec-cuda_shared_memory) 에서 스레드 계층에서 도입된 공유 메모리를 사용하여 성능을 극대화하는 방법을 보인다.

[페이지 잠금 호스트 메모리](#sec-cuda_page_locked_host_meory) 에서는 호스트와 장치 메모리 간의 데이터 전송과 커널 실행을 겹치게 하는 데 필요한 페이지 잠금 호스트 메모리를 소개한다.

[비동기 동시 실행](#sec-cuda_asynchronous_concurrent_execution) 에서는 시스템의 다양한 수준에서 비동기 동시 실행을 가능하게 하는 데 사용되는 개념과 API를 설명한다.

[다중 디바이스 시스템](#sec-cuda_multi_device_system) 에서는 프로그래밍 모델이 동일한 호스트에 연결된 여러 장치가 있는 시스템으로 확장되는 방식을 보여준다.

[오류 검사](#sec-cuda_error_checking) 에서는 런타임에서 생성된 오류를 올바르게 검사하는 방법을 설명한다.

[호출 스택](#sec-cuda_call_stack) 은 CUDA C++ 호출 스택을 관리하는 데 사용되는 런타임 함수를 소개한다.

[텍스처 및 표면 메모리](#sec-cuda_texture_and_surface_memory) 에서는 장치 메모리에 액세스하는 또 다른 방법을 제공하는 텍스처 및 표면 메모리 공간을 설명한다. 또한 GPU 텍스처링 하드웨어의 하위 집합을 보여준다.

[그래픽 상호 운용성](#sec-cuda_graphics_interoperability) 은 런타임이 두 가지 주요 그래픽 API인 OpenGL 및 Direct3D 와 상호 운용하기 위해 제공하는 다양한 함수를 소개한다

</br>

### 초기화 {#sec-cuda_initialization}

CUDA 12.0부터 `cudaInitDevice()` 및 `cudaSetDevice()` 를 호출하면 지정된 디바이스와 연관된 런타임 및 기본 컨텍스트를 초기화합니다. 이러한 호출이 없으면 런타임은 암묵적으로 Device 0 을 사용하고 필요에 따라 자체로 초기화하여 다른 런타임 API 요청을 처리합니다. 런타임 함수 호출의 타이밍을 지정하고 첫 번째 호출의 오류 코드를 런타임으로 해석할 때 이 점을 염두에 두어야 합니다. 12.0 이전에는 `cudaSetDevice()` 가 런타임을 초기화하지 않았고 애플리케이션은 종종 no-op 런타임 호출 `cudaFree(0)` 를 사용하여 런타임 초기화를 다른 API 활동에서 분리했습니다(타이밍과 오류 처리를 위해).

런타임은 시스템의 각 장치에 대한 CUDA 컨텍스트를 만듭니다(CUDA 컨텍스트에 대한 자세한 내용은 [컨텍스트](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#context) 참조). 이 컨텍스트는 이 디바이스의 기본 컨텍스트이며 이 디바이스 에서 활성 컨텍스트가 필요한 첫 번째 런타임 함수에서 초기화됩니다. 이 컨텍스트는 애플리케이션의 모든 호스트 스레드에서 공유됩니다. 이 컨텍스트 생성의 일부로, 필요한 경우 디바이스 코드가 [JIT 컴파일](#sec-cuda_jit_compilation) 되고 디바이스 메모리에 로드됩니다. 이 모든 것이 투명하게 이루어집니다. 예를 들어 드라이버 API 상호 운용성을 위해 필요한 경우 디바이스의 기본 컨텍스트는 [런타임 및 드라이버 API 간 상호 운용성](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#interoperability-between-runtime-and-driver-apis)에 설명된 대로 드라이버 API에서 액세스할 수 있습니다.

호스트 스레드가 `cudaDeviceReset()` 을 호출하면 호스트 스레드가 현재 작동하는 디바이스의 기본 컨텍스트(즉, [디바이스 선택](#sec-cuda-device_selection)에서 정의된 현재 디바이스)가 파괴됩니다. 이 디바이스를 현재 디바이스로 갖는 호스트 스레드가 다음에 호출하는 런타임 함수는 이 장치에 대한 새 기본 컨텍스트를 생성합니다.

</br>

::: {.callout-note}

CUDA 인터페이스는 호스트 프로그램 시작 중에 초기화되고 호스트 프로그램 종료 중에 파괴되는 전역 상태를 사용합니다. CUDA 런타임과 드라이버는 이 상태가 유효하지 않은지 감지할 수 없으므로 main) 이후 프로그램 시작 또는 종료 중에 이러한 인터페이스 중 하나를 명시적으로든 암묵적으로든 사용하면 정의되지 않은 동작이 발생합니다.

CUDA 12.0부터 `cudaSetDevice()` 는 이제 호스트 스레드에 대한 현재 장치를 변경한 후 런타임을 명시적으로 초기화합니다. 이전 버전의 CUDA는 `cudaSetDevice()` 이후 첫 번째 런타임 호출이 이루어질 때까지 새 장치에서 런타임 초기화를 지연했습니다. 이 변경으로 인해 이제 초기화 오류에 대해 `cudaSetDevice()` 의 반환값을 확인하는 것이 매우 중요합니다.

참조 매뉴얼의 오류 처리 및 버전 관리 섹션의 런타임 함수는 런타임을 초기화하지 않습니다.
:::

</br>

### 장치 메모리 {#sec-cuda_device_memory}

이기종 프로그래밍에서 언급했듯이 CUDA 프로그래밍 모델은 각각 별도의 메모리를 가진 호스트와 디바이스로 구성된 시스템을 가정합니다. 커널은 디바이스 메모리에서 작동하므로 런타임은 디바이스 메모리를 할당, 할당 해제, 복사하고 호스트 메모리와 디바이스 메모리 간에 데이터를 전송하는 기능을 제공합니다.

디바이스 메모리는 선형 메모리(Linear meomry - ?) 또는 CUDA 배열로 할당할 수 있습니다.

CUDA 배열은 텍스처 페칭에 최적화된 불투명 메모리 레이아웃입니다. [텍스처 및 표면 메모리](#sec-cuda_texture_and_surface_memory)에서 설명합니다.

선형 메모리는 단일 통합 주소 공간에 할당되므로 별도로 할당된 개체 예를 들어 이진 트리(binary tree) 또는 연결 리스트(linked list) 내에서 포인터를 통해 서로를 참조할 수 있습니다. 주소 공간의 크기는 호스트 시스템(CPU)과 사용된 GPU의 컴퓨팅 기능에 따라 달라집니다.

</br>

| | x86_64 (AMD64) | POWER (ppc64le) | ARM64 |
|:------------------------------------|:---:|:---:|:---:|
| up to compute capability 5.3 (Maxwell) | 40bit | 40bit | 40bit |
| compute capability 6.0 (Pascal) or newer | up to 47bit | up to 49bit | up to 48bit |
:Linear Memory Address Space {#tbl-cuda_linear_memory_address_space}

</br>

선형 메모리는 일반적으로 `cudaMalloc()` 를 사용하여 할당하고 `cudaFree()` 를 사용하여 해제하며 호스트 메모리와 장치 메모리 간의 데이터 전송은 일반적으로 `cudaMemcpy()` 를 사용하여 수행됩니다. 커널의 벡터 추가 코드 샘플에서 벡터는 호스트 메모리에서 장치 메모리로 복사해야 합니다.

```cpp 
#| code-fold: true
#| code-summary: "Show the code"

// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N)
        C[i] = A[i] + B[i];
}

// Host code
int main()
{
    int N = ...;
    size_t size = N * sizeof(float);

    // Allocate input vectors h_A and h_B in host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize input vectors
    ...

    // Allocate vectors in device memory
    float* d_A;
    cudaMalloc(&d_A, size);
    float* d_B;
    cudaMalloc(&d_B, size);
    float* d_C;
    cudaMalloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    ...
}
```

선형 메모리는 `cudaMallocPitch()` 및 `cudaMalloc3D()` 를 통해 할당할 수도 있습니다. 이러한 함수는 2D 또는 3D 배열의 할당에 권장되며, [디바이스 메모리 액세스](#sec-cuda_device_memory_access)에서 설명할 정렬 요구 사항을 충족하도록 할당이 적절하게 패딩되어 행 주소에 액세스하거나 2D 배열과 장치 메모리의 다른 영역 간에 복사를 수행할 때 최상의 성능을 보장합니다(`cudaMemcpy2D()` 및 `cudaMemcpy3D()` 함수 사용). 반환된 피치(또는 스트라이드)는 배열 요소에 접근하는 데 사용해야 합니다. 다음 코드 샘플은 부동 소수점 값의 너비 x 높이 2D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.

``` cpp
// Host code
int width = 64, height = 64;
float* devPtr;
size_t pitch;
cudaMallocPitch(&devPtr, &pitch,
                width * sizeof(float), height);
MyKernel<<<100, 512>>>(devPtr, pitch, width, height);

// Device code
__global__ void MyKernel(float* devPtr,
                         size_t pitch, int width, int height)
{
    for (int r = 0; r < height; ++r) {
        float* row = (float*)((char*)devPtr + r * pitch);
        for (int c = 0; c < width; ++c) {
            float element = row[c];
        }
    }
}
```

</br>
다음 코드 샘플은 부동 소수점 값의 너비 x 높이 x 깊이 3D 배열을 할당하고 장치 코드에서 배열 요소를 반복하는 방법을 보여줍니다.

```cpp
// Host code
int width = 64, height = 64, depth = 64;
cudaExtent extent = make_cudaExtent(width * sizeof(float),
                                    height, depth);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&devPitchedPtr, extent);
MyKernel<<<100, 512>>>(devPitchedPtr, width, height, depth);

// Device code
__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,
                         int width, int height, int depth)
{
    char* devPtr = devPitchedPtr.ptr;
    size_t pitch = devPitchedPtr.pitch;
    size_t slicePitch = pitch * height;
    for (int z = 0; z < depth; ++z) {
        char* slice = devPtr + z * slicePitch;
        for (int y = 0; y < height; ++y) {
            float* row = (float*)(slice + y * pitch);
            for (int x = 0; x < width; ++x) {
                float element = row[x];
            }
        }
    }
}
```

</br>

::: {.callout-note}
너무 많은 메모리를 할당하여 시스템 전체 성능에 영향을 미치지 않도록 하려면 문제 크기에 따라 사용자에게 할당 매개변수를 요청하세요. 할당이 실패하면 다른 느린 메모리 유형(`cudaMallocHost()`, `cudaHostRegister()` 등)으로 폴백하거나 거부된 메모리가 얼마나 필요한지 알려주는 오류를 반환할 수 있습니다. 애플리케이션이 어떤 이유로 할당 매개변수를 요청할 수 없는 경우 이를 지원하는 플랫폼에 대해 `cudaMallocManaged()` 를 사용하는 것이 좋습니다.
:::

</br>

참조 설명서에는 `cudaMalloc()` 로 할당된 선형 메모리, `cudaMallocPitch()` 또는 `cudaMalloc3D()` 로 할당된 선형 메모리, CUDA 배열, 전역 또는 상수 메모리 공간에서 선언된 변수에 할당된 메모리 간에 메모리를 복사하는 데 사용되는 다양한 함수가 모두 나열되어 있습니다.

</br>

다음 코드 샘플은 런타임 API를 통해 전역 변수에 액세스하는 다양한 방법을 보여줍니다.

```cpp
__constant__ float constData[256];
float data[256];
cudaMemcpyToSymbol(constData, data, sizeof(data));
cudaMemcpyFromSymbol(data, constData, sizeof(data));

__device__ float devData;
float value = 3.14f;
cudaMemcpyToSymbol(devData, &value, sizeof(float));

__device__ float* devPointer;
float* ptr;
cudaMalloc(&ptr, 256 * sizeof(float));
cudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));
```

`cudaGetSymbolAddress()` 는 전역 메모리 공간에서 선언된 변수에 할당된 메모리를 가리키는 주소를 검색하는 데 사용됩니다. 할당된 메모리의 크기는 `cudaGetSymbolSize()` 를 통해 얻습니다.


</br>

### 디바이스 메모리 L2 접근 관리

CUDA 커널이 전역 메모리의 데이터 영역에 반복적으로 액세스하는 경우 이러한 데이터 액세스는 지속되는 것으로 간주될 수 있습니다. 반면, 데이터가 한 번만 액세스되는 경우 이러한 데이터 액세스는 스트리밍으로 간주될 수 있습니다.

CUDA 11.0부터 컴퓨팅 기능 8.0 이상의 장치는 L2 캐시의 데이터 지속성에 영향을 미칠 수 있는 기능을 갖추고 있어 글로벌 메모리에 대한 더 높은 대역폭과 더 낮은 지연 시간 액세스를 제공할 수 있습니다.

</br>

#### L2 Cache Set-Aside for Persisting Accesses

L2 캐시의 일부는 전역 메모리에 대한 지속적인 데이터 액세스에 사용하도록 따로 보관할 수 있습니다. 지속적인 액세스는 L2 캐시의 이 따로 보관된 부분을 우선적으로 사용하지만, 글로벌 메모리에 대한 일반 또는 스트리밍 액세스는 지속적인 액세스에서 사용되지 않을 때만 L2의 이 부분을 활용할 수 있습니다.

지속적인 액세스를 위한 L2 캐시 예약 크기는 다음 한도 내에서 조정될 수 있습니다.

```cpp
cudaGetDeviceProperties(&prop, device_id);
size_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/
```

</br>

GPU가 Multi-Instance GPU(MIG) 모드로 구성된 경우 L2 캐시 예약 기능이 비활성화됩니다.

Multi-Process Service(MPS)를 사용하는 경우 L2 캐시 예약 크기는 `cudaDeviceSetLimit` 으로 변경할 수 없습니다. 대신 예약 크기는 환경 변수 `CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT` 를 통해 MPS 서버를 시작할 때만 지정할 수 있습니다.

</br>

#### L2 Policy for Persisting Accesses

access policy window 는 전역 메모리의 연속 영역과 해당 영역 내의 액세스에 대한 L2 캐시의 지속성 속성을 지정합니다. 아래 코드 예제는 CUDA 스트림을 사용하여 L2 persisting access window 를 설정하는 방법을 보여줍니다.


``` {.txt}
#| code-overflow: scroll
cudaStreamAttrValue stream_attribute;                                         // Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(ptr); // Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persistence access.
                                                                              // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)
stream_attribute.accessPolicyWindow.hitRatio  = 0.6;                          // Hint for cache hit ratio
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.

//Set the attributes to a CUDA stream of type cudaStream_t
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);
```

</br>
* to be done *

</br>


### 공유 메모리 {#sec-cuda_shared_memory}

[가변 메모리 공간 지정자](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#variable-memory-space-specifiers) 에서 자세히 설명하겠지만 공유 메모리는 `__shared__` 메모리 공간 지정자를 사용하여 할당됩니다.

 [스레드 계층 구조](02_programming_model.qmd#sec-cuda_thread_hierarchy)에서 언급되고 [공유 메모리](#sec-cuda_shared_memory)에서 자세히 설명된 대로 공유 메모리는 전역 메모리보다 훨씬 빠를 것으로 예상됩니다. 다음 행렬 곱셈 예제에서 보여지는 것처럼 스크래치패드 메모리(또는 소프트웨어 관리 캐시)로 사용하여 CUDA 블록에서 글로벌 메모리 액세스를 최소화할 수 있습니다.

다음 코드 샘플은 공유 메모리를 활용하지 않는 간단한 행렬 곱셈 구현입니다. 각 스레드는 A의 한 행과 B의 한 열을 읽고 C의 해당 요소를 계산합니다. 따라서 A는 글로벌 메모리에서 B.width 번 읽히고 B는 A.height 번 읽힙니다.

```cpp
// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.width + col)
typedef struct {
    int width;
    int height;
    float* elements;
} Matrix;

// Thread block size
#define BLOCK_SIZE 16

// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);

// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
    // Load A and B to device memory
    Matrix d_A;
    d_A.width = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * sizeof(float);
    cudaMalloc(&d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = B.width; d_B.height = B.height;
    size = B.width * B.height * sizeof(float);
    cudaMalloc(&d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
               cudaMemcpyHostToDevice);

    // Allocate C in device memory
    Matrix d_C;
    d_C.width = C.width; d_C.height = C.height;
    size = C.width * C.height * sizeof(float);
    cudaMalloc(&d_C.elements, size);

    // Invoke kernel
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);

    // Read C from device memory
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}

// Matrix multiplication kernel called by MatMul()
__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    // Each thread computes one element of C
    // by accumulating results into Cvalue
    float Cvalue = 0;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    for (int e = 0; e < A.width; ++e)
        Cvalue += A.elements[row * A.width + e]
                * B.elements[e * B.width + col];
    C.elements[row * C.width + col] = Cvalue;
}
```
</br>

![공유메모리를 사용하지 않는 행렬곱](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/matrix-multiplication-without-shared-memory.png){#fig-cuda_matrix_multiplication_without_shared_memory}


</br>

다음 코드 샘플은 공유 메모리를 활용하는 행렬 곱셈의 구현입니다. 이 구현에서 각 스레드 블록은 C의 하나의 정사각 부분 행렬 Csub를 계산할 책임이 있고 블록 내의 각 스레드는 Csub의 하나의 요소를 계산할 책임이 있습니다. 그림 9에서 볼 수 있듯이 Csub는 두 개의 직사각형 행렬의 곱과 같습니다. 즉, Csub와 같은 행 인덱스를 갖는 (A.width, block_size) 차원의 A의 부분 행렬과 Csub와 같은 열 인덱스를 갖는 (block_size, A.width) 차원의 B의 부분 행렬입니다. 장치의 리소스에 맞추기 위해 이 두 직사각형 행렬은 필요한 만큼의 block_size 차원의 정사각 행렬로 나뉘고 Csub는 이러한 정사각 행렬의 곱의 합으로 계산됩니다. 이러한 각 곱은 먼저 하나의 스레드가 각 행렬의 한 요소를 로드하여 두 개의 해당 정사각 행렬을 전역 메모리에서 공유 메모리로 로드한 다음 각 스레드가 곱의 한 요소를 계산하여 수행됩니다. 각 스레드는 이들 각각의 곱의 결과를 레지스터에 누적하고, 완료되면 결과를 전역 메모리에 씁니다.

```cpp
// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.stride + col)
typedef struct {
    int width;
    int height;
    int stride;
    float* elements;
} Matrix;
// Get a matrix element
__device__ float GetElement(const Matrix A, int row, int col)
{
    return A.elements[row * A.stride + col];
}
// Set a matrix element
__device__ void SetElement(Matrix A, int row, int col,
                           float value)
{
    A.elements[row * A.stride + col] = value;
}
// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is
// located col sub-matrices to the right and row sub-matrices down
// from the upper-left corner of A
 __device__ Matrix GetSubMatrix(Matrix A, int row, int col)
{
    Matrix Asub;
    Asub.width    = BLOCK_SIZE;
    Asub.height   = BLOCK_SIZE;
    Asub.stride   = A.stride;
    Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row
                                         + BLOCK_SIZE * col];
    return Asub;
}
// Thread block size
#define BLOCK_SIZE 16
// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);
// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
    // Load A and B to device memory
    Matrix d_A;
    d_A.width = d_A.stride = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * sizeof(float);
    cudaMalloc(&d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = d_B.stride = B.width; d_B.height = B.height;
    size = B.width * B.height * sizeof(float);
    cudaMalloc(&d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
    cudaMemcpyHostToDevice);
    // Allocate C in device memory
    Matrix d_C;
    d_C.width = d_C.stride = C.width; d_C.height = C.height;
    size = C.width * C.height * sizeof(float);
    cudaMalloc(&d_C.elements, size);
    // Invoke kernel
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);
    // Read C from device memory
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);
    // Free device memory
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}
// Matrix multiplication kernel called by MatMul()
 __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    // Block row and column
    int blockRow = blockIdx.y;
    int blockCol = blockIdx.x;
    // Each thread block computes one sub-matrix Csub of C
    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);
    // Each thread computes one element of Csub
    // by accumulating results into Cvalue
    float Cvalue = 0;
    // Thread row and column within Csub
    int row = threadIdx.y;
    int col = threadIdx.x;
    // Loop over all the sub-matrices of A and B that are
    // required to compute Csub
    // Multiply each pair of sub-matrices together
    // and accumulate the results
    for (int m = 0; m < (A.width / BLOCK_SIZE); ++m) {
        // Get sub-matrix Asub of A
        Matrix Asub = GetSubMatrix(A, blockRow, m);
        // Get sub-matrix Bsub of B
        Matrix Bsub = GetSubMatrix(B, m, blockCol);
        // Shared memory used to store Asub and Bsub respectively
        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
        // Load Asub and Bsub from device memory to shared memory
        // Each thread loads one element of each sub-matrix
        As[row][col] = GetElement(Asub, row, col);
        Bs[row][col] = GetElement(Bsub, row, col);
        // Synchronize to make sure the sub-matrices are loaded
        // before starting the computation
        __syncthreads();
        // Multiply Asub and Bsub together
        for (int e = 0; e < BLOCK_SIZE; ++e)
            Cvalue += As[row][e] * Bs[e][col];
        // Synchronize to make sure that the preceding
        // computation is done before loading two new
        // sub-matrices of A and B in the next iteration
        __syncthreads();
    }
    // Write Csub to device memory
    // Each thread writes one element
    SetElement(Csub, row, col, Cvalue);
}
```

![공유메모리를 사용않는 행렬곱](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/matrix-multiplication-with-shared-memory.png){#fig-cuda_matrix_multiplication_with_shared_memory}


</br>

### 분산 공유 메모리 {#sec-cuda_distributed_shared_memory}

compute capability 9.0에 도입된 스레드 블록 클러스터는 스레드 블록 클러스터의 스레드가 클러스터에 참여하는 모든 스레드 블록의 공유 메모리에 액세스할 수 있는 기능을 제공합니다. 이 분할된 공유 메모리를 분산 공유 메모리(distributed shared memory)라고 하며, 해당 주소 공간을 분산 공유 메모리 주소 공간이라고 합니다. 스레드 블록 클러스터에 속한 스레드는 주소가 로컬 스레드 블록에 속하는지 원격 스레드 블록에 속하는지에 관계없이 분산 주소 공간에서 읽거나 쓰거나 아토믹 연산을 수행할 수 있습니다. 커널이 분산 공유 메모리를 사용하든 사용하지 않든, 정적이든 동적이든 공유 메모리 크기 사양은 여전히 ​​스레드 블록당입니다. 분산 공유 메모리의 크기는 클러스터당 스레드 블록 수에 스레드 블록당 공유 메모리 크기를 곱한 값일 뿐입니다.

분산 공유 메모리의 데이터에 액세스하려면 모든 스레드 블록이 존재해야 합니다. 사용자는 [Cluster Group](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cluster-group-cg) API의 `cluster.sync()` 를 사용하여 모든 스레드 블록이 실행을 시작했는지 보장할 수 있습니다. 사용자는 또한 모든 분산 공유 메모리 작업이 스레드 블록이 종료되기 전에 수행되도록 해야 합니다. 예를 들어 원격 스레드 블록이 주어진 스레드 블록의 공유 메모리를 읽으려고 하는 경우 사용자는 원격 스레드 블록이 읽은 공유 메모리가 종료되기 전에 완료되었는지 확인해야 합니다.

CUDA는 분산 공유 메모리에 액세스하는 메커니즘을 제공하며, 애플리케이션은 이 기능을 활용하여 이점을 얻을 수 있습니다. 간단한 히스토그램 계산과 스레드 블록 클러스터를 사용하여 GPU에서 최적화하는 방법을 살펴보겠습니다. 히스토그램을 계산하는 표준적인 방법은 각 스레드 블록의 공유 메모리에서 계산을 수행한 다음 글로벌 메모리 아토믹을 수행하는 것입니다. 이 방법의 한계는 공유 메모리 용량입니다. 히스토그램 빈이 더 이상 공유 메모리에 맞지 않으면 사용자는 히스토그램을 직접 계산하고 따라서 글로벌 메모리의 아토믹을 계산해야 합니다. 분산 공유 메모리를 사용하면 CUDA는 중간 단계를 제공하며, 여기서 히스토그램 빈 크기에 따라 히스토그램을 공유 메모리, 분산 공유 메모리 또는 글로벌 메모리에서 직접 계산할 수 있습니다.

아래의 CUDA 커널 예제는 히스토그램 빈의 수에 따라 공유 메모리 또는 분산 공유 메모리에서 히스토그램을 계산하는 방법을 보여줍니다.

```cpp
#include <cooperative_groups.h>

// Distributed Shared memory histogram kernel
__global__ void clusterHist_kernel(int *bins, const int nbins, const int bins_per_block, const int *__restrict__ input,
                                   size_t array_size)
{
  extern __shared__ int smem[];
  namespace cg = cooperative_groups;
  int tid = cg::this_grid().thread_rank();

  // Cluster initialization, size and calculating local bin offsets.
  cg::cluster_group cluster = cg::this_cluster();
  unsigned int clusterBlockRank = cluster.block_rank();
  int cluster_size = cluster.dim_blocks().x;

  for (int i = threadIdx.x; i < bins_per_block; i += blockDim.x)
  {
    smem[i] = 0; //Initialize shared memory histogram to zeros
  }

  // cluster synchronization ensures that shared memory is initialized to zero in
  // all thread blocks in the cluster. It also ensures that all thread blocks
  // have started executing and they exist concurrently.
  cluster.sync();

  for (int i = tid; i < array_size; i += blockDim.x * gridDim.x)
  {
    int ldata = input[i];

    //Find the right histogram bin.
    int binid = ldata;
    if (ldata < 0)
      binid = 0;
    else if (ldata >= nbins)
      binid = nbins - 1;

    //Find destination block rank and offset for computing
    //distributed shared memory histogram
    int dst_block_rank = (int)(binid / bins_per_block);
    int dst_offset = binid % bins_per_block;

    //Pointer to target block shared memory
    int *dst_smem = cluster.map_shared_rank(smem, dst_block_rank);

    //Perform atomic update of the histogram bin
    atomicAdd(dst_smem + dst_offset, 1);
  }

  // cluster synchronization is required to ensure all distributed shared
  // memory operations are completed and no thread block exits while
  // other thread blocks are still accessing distributed shared memory
  cluster.sync();

  // Perform global memory histogram, using the local distributed memory histogram
  int *lbins = bins + cluster.block_rank() * bins_per_block;
  for (int i = threadIdx.x; i < bins_per_block; i += blockDim.x)
  {
    atomicAdd(&lbins[i], smem[i]);
  }
}
```
위의 커널은 필요한 분산 공유 메모리 양에 따라 클러스터 크기로 런타임에 시작할 수 있습니다. 히스토그램이 한 블록의 공유 메모리에 맞을 만큼 작으면 사용자는 클러스터 크기 1로 커널을 시작할 수 있습니다. 아래 코드 조각은 공유 메모리 요구 사항에 따라 동적으로 클러스터 커널을 시작하는 방법을 보여줍니다.

</br>

### 페이지 잠금 호스트 메모리 {#sec-cuda_page_locked_host_meory}


### 비동기 동시 실행 (Asynchronous concurrent Execution) {#sec-cuda_asynchronous_concurrent_execution}


### 다중 디바이스 시스템 {#sec-cuda_multi_device_system}

#### 디바이스 선택 {#sec-cuda-device_selection}

### 오류 검사 {#sec-cuda_error_checking}

### 호출 스택 {#sec-cuda_call_stack}

### 텍스쳐와 표면 메모리 {#sec-cuda_texture_and_surface_memory}

### 그래픽 상호 운용성 {#sec-cuda_graphics_interoperability}