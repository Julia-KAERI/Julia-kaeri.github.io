---
title: "소개"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../../../latexmacros.qmd >}}

</br>

## 시작하기 {#sec-pytorch_introduction}

앞으로의 모든 코드에는 아래의 `import` 가 이미 실행되었다고 가정한다.

```python
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import numpy as np
import matplotlib.pyplot as plt
```

</br>

### 데이터셋(Dataset) 과 데이터 로더(Data Loader) {#sec-pytorch_dataset_and_dataloader}

파이토치에서 데이터에 작업을 할 때 `torch.utils.data.DataLoader` 와 `torch.utils.data.Dataset` 을 사용한다. `Dataset` 샘플과 정답을 저장하는 컨테이너이며 `DataLoader` 는 `Dataset` 의 데이터들을 iterable 하게 처리 할 수 있도록 한다. [Pytorch 의 Datasets](https://pytorch.org/vision/stable/datasets.html) 은 pytorch 에서 제공하는 데이터셋을 설명한다.

`FashionMNIST` 데이터셋을 다운로드 받아 보자. 이 `FashionMNIST` 데이터셋은 가방이나 악세사리와 같은 패션 아이템의 이미지를 학습하기 위한 저해상도 이미지의 모음이다. 우선 학습 데이터를 다운받는다.

```{.python filename="In"}
training_data = datasets.FashionMNIST(
    root="/home/asc/torchdata",
    train=True,
    download=True,
    transform=ToTensor(),
)
```

`root` 는 데이터를 다운 받는 디렉토리를 말한다. `root` 디렉토리에 `FashionMNIST` 서브디렉토리를 만들고 이 서브디렉토리에 데이터가 저장된다.

이제 테스트 데이터를 다운받아보자.

```{.python filename="In"}
test_data = datasets.FashionMNIST(
    root="/home/asc/torchdata",
    train=False,
    download=True,
    transform=ToTensor(),
)
```

데이터를 확인해보자.

```{.python filename="In"}
training_data
```
```{.txt filename="Out"}
Dataset FashionMNIST
    Number of datapoints: 60000
    Root location: /home/asc/torchdata
    Split: Train
    StandardTransform
Transform: ToTensor()
```
</br>

```{.python filename="In"}
test_data
```
```{.txt filename="Out"}
Dataset FashionMNIST
    Number of datapoints: 10000
    Root location: /home/ast/torchdata
    Split: Test
    StandardTransform
Transform: ToTensor()
```

`training_data` 와 `test_data` 는 텐서화된 이미지 데이터를 포함한다.

```{.python filename="In"}
training_data.data
```

```{.txt filename="Out"}
tensor([[[0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         ...,
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0]],

        [[0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         [0, 0, 0,  ..., 0, 0, 0],
         ...,
```

</br>


배치 사이즈를 정하고 데이터 로더를 생성한다. 

```{.python filename="In"}
batch_size = 64

# 데이터로더를 생성합니다.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break
```

```{.txt filename="Out"}
Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64
```

</br>

이제 `FashionMNIST` 의 몇몇 아이템들을 보자.
</br>

```{.python filename="In"}
labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()
```

![Out](https://tutorials.pytorch.kr/_images/sphx_glr_data_tutorial_001.png){#fig-FashonMNIST width=500}

</br>

### 사용자 정의 데이터셋과 데이터 로더 {#sec-pytorch_custom_dataset_and_dataloader}

사용자 정의 `Dataset` 은 클래스 `Dataset` 클래스를 상속받아 만들며 아래의 세 함수가 정의되어야 한다.

- `__init__`
- `__len__`
- `__getitem__`

즉 python 의 list 와 유사하게 행동해야 한다는 의미이다. [Pytorch tutorial 의 사용자 정의 Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) 에서 제시한 샘플 코드는 아래와 같다.

```python
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```
</br>

이제 아주 간단한 Dataset 을 만들어 보자.

```{.python filename="In"}
from torch.utils.data import Dataset

class SimpleCustomDataset(Dataset):
    def __init__(self, data:str):
        self.data = data
        
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

ds = SimpleCustomDataset("abcdefghijk")
dloader = DataLoader(ds, batch_size=3, shuffle=True)
```

이제 `ds` 라는 Dataset 과 `dloader` 라는 DataLoader 가 만들어졌다. 아래와 같이 `batch_size` 크기 대로 순회 할 수 있다.

```{.python filename="In"}
for i in range(4):
    print(next(iter(dloader)))
```
```{.txt filename="Out"}
['d', 'i', 'g']
['f', 'd', 'c']
['e', 'h', 'c']
['h', 'c', 'a']
```


</br>

### CUDA {#sec-pytorch_cuda_check}

`torch.cuda.is_available()` 함수를 통해 CUDA 를 사용 할 수 있는지 확인 할 수 있다.

```{.python filename="In"}
torch.cuda.is_available()
```

```{.txt filename="Out"}
True
```
</br>

CUDA GPU 가 몇개인지 확인할 수 있다.

```{.python filename="In"}
torch.cuda.device_count()
```

```{.txt filename="Out"}
2
```

CUDA 장치는 `0` 부터 시작하는 인덱스를 갖는다. 따라서 2개의 CUDA GPU 장치의 인덱스는 `0`, `1` 이며 그 이름은 다음과 같이 확인 할 수 있다.

```{.python filename="In"}
for i in (0, 1):
    print(torch.cuda.get_device_name(i))
```
```{.txt filename="Out"}
NVIDIA RTX A5000
NVIDIA RTX A5000
```

</br>

CUDA 를 사용하기 위해서는 다음과 host 즉 CPU 상에서 텐서를 만든 후 CUDA 로 텐서를 복사할 수 있다.

```{.python filename="In"}
a=torch.tensor([[1,2,3,4], [5,6,7,8]])
print("a.divice = ", a.device)
b = a.to(cuda0)
c = a.to(cuda1)
print("b.divice = ", b.device)
print("c.divice = ", c.device)
b
```
```{.txt filename="Out"}
a.divice =  cpu
b.divice =  cuda:0
c.divice =  cuda:1
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]], device='cuda:0')
```

</br>

혹은 처음부터 CUDA 장치에 텐서를 만들 수 있다.

```{.python filename="In"}
d=torch.tensor([[1,2], [3,4]], device="cuda:0")
```

</br>

## 텐서

`numpy.array` 와 같이 `torch.tensor` 는 pytorch 에서 데이터를 다루는데 사용되는 객체이다. 많은 경우 `numpy` (혹은 `np`) 를 `torch`, `array` 를 `tensor` 로 바꾸면 numpy 와 거의 같게 동작한다. [pytorch-for-numpy-users](https://github.com/wkentaro/pytorch-for-numpy-users) 를 참고하라. 

numpy 의 array 는 cpu 에서 동작하지만 앞서 보였듯이 torch 의 tensor 는 cpu 에서 동작할 수도 있고 cuda 에서 동작할 수도 있다. 이후로 특별한 언급이 없다면 `array` 는 `np.array` 이며 `tensor` 는 `torch.tensor` 이다.

</br>

### 초기화

numpy 의 array 로부터 tensor 를 초기화 할 수 있다.

```{.python filename="In"}
nparr1 = np.array([1,2,3,4])
thtns1 = torch.from_numpy(nparr1)
thtns2 = thtns1.to("cuda:0")

nparr1[0]=-1
print(nparr1)
print(thtns1)
print(thtns2)
print("-----")
thtns1[1]=-2
print(nparr1)
print(thtns1)
print(thtns2)
```

```{.txt filename="Out"}
[-1  2  3  4]
tensor([-1,  2,  3,  4])
tensor([1, 2, 3, 4], device='cuda:0')
-----
[-1 -2  3  4]
tensor([-1, -2,  3,  4])
tensor([1, 2, 3, 4], device='cuda:0')
```

`nparr` 을 `[1, 2, 3, 4]` 로 초기화 하였고 이를 이용하여 cpu tensor `thtns1` 을 초기화하고 다시 이를 `tensor.to()` 함수를 사용하여 cuda tensor `thtns2` 로 초기화 하였다. `torch.from_numpy` 로 초기화 할 경우 tensor 와 array 는 동기화되어 하나를 변경하면 나머지 하나도 변화하지만 cuda tensor 인 `thtns2` 는 어떤 경우에도 변경되지 않는다.

</br>

이제 거꾸로 cuda tensor 에서 cpu tensor, 그리고 np.array 로 가보자. `tensor.cpu()` 는 cuda tensor 로부터 cpu tensor 를 생성하며 `tensor.numpy()` 함수는 `np.array` 배열을 만든다. 역시 마찬가지로 이렇게 생성된 array 의 성분을 바꾸면 cpu tensor 는 같이 바뀌지만 cuda tensor 는 변경되지 않는다.

```{.python filename="In"}
ts1 = torch.tensor([1,2,3,4], device="cuda:0")
ts2 = ts1.cpu()
ar2 = ts2.numpy()

ar2[-1]=0
print(ts1)
print(ts2)
print(ar2)
```

```{.txt filename="Out"}
tensor([1, 2, 3, 4], device='cuda:0')
tensor([1, 2, 3, 0])
[1 2 3 0]
```

</br>

### 텐서 타입

텐서 타입은 텐서 성분의 타입과 텐서가 저장된 위치(cpu or cuda) 에 따라 결정된다. 성분의 타입이 `dtype` 이며 이에 따른 `cpu` 와 `gpu` 에서의 타입은 아래와 같다. 

| 성분의 타입	 | `dtype` | CPU tensor 타입 | GPU tensor 타입 |
|:-------:|:----------:|:---------:|:---------:|
| 32-bit 부동소수 | `torch.float32`, `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor` |
| 64-bit 부동소수 | `torch.float64`, `torch.double` |	`torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16-bit 부동소수 |	`torch.float16`, `torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |
| 8-bit 부호 없는 정수 | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor`|
| 8-bit 정수 | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor`|
| 16-bit 정수 | `torch.int16`  or `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor`|
| 32-bit 정수 | `torch.int32` or `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor`|
| 64-bit integer | `torch.int64` or `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor`|
| Boolean | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor`|



</br>

array 혹은 tensor 의 dtype 은 생성시 지정할 수도 있고 변경할 수도 있다. numpy 의 `astype()` 메서드는 사용할 수 없고 `torch.as_tensor()` 함수를 아래와 같이 사용 할 수 있다.

```{.python filename="In"}
x1 = torch.tensor([1,2,3], dtype=torch.float16)
print(x1.dtype)
x2 = torch.as_tensor(x1, dtype = torch.float32)
print(x2.dtype)
```

```{.txt filename="Out"}
torch.float16
torch.float32
```

</br>

::: {.callout-important icon="false"}

#### 기본 타입

`np.array([1.0, 2.0])` 의 `dtype` 은 `np.float64` 이지만 `torch.tensor([1.0, 2.0])` 의 `dtype` 은 cpu, cuda 모두 `torch.float32` 이다. 특별히 명시적으로 dtype 을 변환시키지 않고 array 와 tensor 사이의 변환은 dtype 을 유지한다. 

성분이 정수일 경우 array 와 tensor 모두 기본적인 dtype 은 `int64` 이다.

:::

</br>

### `tensor` 를 생성할 때의 파라미터들

- `data` : 텐서를 생성할 때 텐서의 성분이 되는 list, array 등의 데이터
- `dtype` : 텐서 성분 타입
- `device` : `cpu`, `cuda`, 'cuda:0' 등
- `requires_grad` : `True`, `False` 중 하나의 값을 가지며 `torch.autograd` 에 의한 텐서의 gradient 추적할지 여부가 결정된다. 
- `pin_memory` : `device=cpu` 일 경우만 가능. CPU 에서 CUDA 로의 전송이 빠르다.


```{.python filename="In"}
ts1 = torch.tensor(data = [1, 2, 2, 3], dtype = torch.float32, device="cpu", requires_grad=True, pin_memory=True)
print(ts1)
```
```{.txt filename="Out"}
tensor([1., 2., 2., 3.], requires_grad=True)
```

</br>

### `_` 로 끝나는 메서드

다음 코드를 보자.

```{.python filename="In"}
p1 = torch.tensor([-1.0, 2.0])
p2 = p1.abs()
q1 = torch.tensor([-1.0, 2.0])
q1.abs_()
print("p1 = ", p1)
print("p2 = ", p2)
print("q1 = ", q1)
```
```{.text filename="Out"}
p1 =  tensor([-1.,  2.])
p2 =  tensor([1., 2.])
q1 =  tensor([1., 2.])
```


`p1.abs()` 는 `p1` 의 각 성분의 절대값으로 이루어진 텐서를 반환하지만 `p1` 을 바꾸지는 않는다. 그러나 `q1.abs_()` 는 `q1` 의 각 성분을 원래 성분의 절대값으로 바꾼다. Pytorch 에서 다른 것이 다 같고 맨 끝에 `_` 가 붙지 않은 메서드와 붙은 메서드 가 있다면 `_` 가 붙은 메서드는 결과값을 새로운 tensor 로 반환하는 것이 아니라 원래의 텐서를 변경한다. 이것을 In place 연산이라고 한다. `_` 가 마지막에 붙지 않은 메서드는 연산 결과를 새로운 텐서로 반환한다.

</br>

### `View` 와 `tensor.contiguous()`

- 참고자료 : [tensor view](https://pytorch.org/docs/stable/tensor_view.html#tensor-view-doc)

예를 들어 텐서의 축 순서를 바꾸는 `tensor.transpose()` 는 원래의 텐서의 데이터로부터 새로운 데이터를 생성하지 않고 원본 텐서에 데이터를 참조하면서 그것이 보여지거나 연산되는 방식을 바꾼다. 이렇게 원본 텐서를 참조하면서 모양, 기능, 출력 등만 바뀌는 것을 `View` 라고 한다. 한 텐서가 다른 텐서의 `View` 라면 둘 중 하나의 성분을 바꾸면 나머지 하나도 당연히 같이 바뀌게 된다.

이렇게 텐서의 내용을 복사 변경하지 않고 `View` 를 지원하는 메서드는 `tensor.transpose()`, `tensor.expand()`, `tensor.diagonal()` 등 다수가 있다. 기본적으로 `tensor[1:-1:2, 2:]` 와 같은 슬라이싱도 `View` 이다. 

`tensor.is_contiguous()` 메서드는 텐서가 `View` 인지 아닌지를 반환하며 `tensor.contiguous()` 메서드는 원본 텐서의 데이터로부터 원하는 모양과 기능을 가진 새로운 데이터셋을 가진 텐서를 만든다.

```{.python filename="In"}
t0 = torch.tensor([1,2,3,4,5.0])
t1 = t0[1:-1:2]
print(t1.is_contiguous())
t1=t1.contiguous() 
print(t1.is_contiguous())
```
```{.txt filename="Out"}
False
True
```

</br>

### 텐서를 복사하는 세가지 방법

Tensor `p0` 에 대해 

- `p1 = p0.detatch()` : `p1` 는 `p0` 와 데이터를 공유하며 둘중 하나의 성분을 바꿔도 나머지에 반영되지만 `p1.requires_grad` 는 `False` 이다. 
- `p2 = p0.clone()` : 내용을 복사하여 하나를 변경해도 다른 하나에 반영되지 않는다.
- `p3 = p0.data` : 사용하지 말것. [pytorch 0.4.0 reliase note](https://github.com/pytorch/pytorch/releases/tag/v0.4.0) 의 *What about .data* 섹션 참고.


</br>

### Tensor class reference

- [Tensor class reference](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference)



