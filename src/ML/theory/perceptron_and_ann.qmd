---
title: "퍼셉트론과 인공신경망"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../../../latexmacros.qmd >}}

</br>

## 퍼셉트론

### 퍼셉트론 {#sec-perceptron}

![퍼셉트론](figure/perceptron01.png){#fig-perceptron_perceptron width=400}

퍼셉트론은 

&emsp;($1$) $n$ 개의 입력 변수 $\bf{x}\in \R^n$,

&emsp;($2$) $n+1$ 개의 내부 파라미터 : 1개의 bias $b$ 와 $n$ 개의 값을 갖는 $\bf{w}=\begin{bmatrix} w_1 & \ldots &w_n\end{bmatrix}^T$,

&emsp;($3$) 활성화 함수(activation function) $\sigma(z)$ 

로 정의된다. 

$$
z=b+ \sum_{i=1}^n \bf{x}\cdot \bf{w} = b + w_1x_1 + \cdots + w_n x_n
$$

을 계산 한 후, 활성화 함수값 $\sigma(z)$ 를 출력한다.  

</br>

### 이진분류

활성화 함수 $\sigma$ 가 $\sigma:\R \to \{0,\,1\}$ 일 경우 2진분류 문제이며 $\bf{w}$ 에 의해 결정표면이 정해진다. 물론 활성화 함수가 항상 이런 형태일 필요는 없다. 


</br>

### 활성화 함수의 비선형성

일반적으로 활성화 함수는 비선형 함수이다. 즉 $\sigma (z)$ 는 $az+b$ 꼴이 아니다. 

</br>

## 다층 퍼셉트론

### 다층 퍼셉트론

![다층 퍼셉트론](figure/perceptron02.png){#fig-perceptron_multi_layer_perceptron width=600}

입력층 $\bf{x} = \begin{bmatrix}x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 첫번째 은닉층의 $h^{(1)}_j$ 의 내부 파라미터는 $\bf{w}^{(1j)}$

$$
h^{(1)}_j = \sigma (\bf{w}^{1j} \bf{\cdot x}+b^{(1)}_j)
$$

이다. 첫번째 은닉층에 $N_1$ 개의 퍼셉트론이 있다고 하고 행렬 $\bf{W}^{(1)}$ 와 $\tilde{\bf{h}}$ 를 다음과 같이 정의하자.

$$
\bf{W}^{(1)} = \begin{bmatrix} b^{(1)}_1 & w^{11}_1 & w^{11}_2 & \cdots & w^{11}_n \\ b^{(1)}_2 & w^{12}_1 & w^{12}_2 & \cdots & w^{12}_n \\ \vdots & & & & \vdots \\ b^{(1)}_{N_1} & w^{1N_1}_1 & w^{1N_1}_2 & \cdots & w^{1N_1}_n\end{bmatrix},\qquad \tilde{\bf{x}} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}.
$$

$\bf{W}^{(1)}\tilde{\bf{x}}$ 의 $j$ 번 째 행을 $\left[\bf{W}^{(1)}\tilde{\bf{x}}\right]_j$ 라고 표기한다면 $h_j^{(1)} = \sigma \left(\left[\bf{W}^{(1)}\tilde{\bf{x}}\right]_j\right)$ 이다. 이제 $\sigma$ 함수를 확장하여

$$
\bf{\sigma}^{(1)}\left(\begin{bmatrix} z_1 \\ \vdots \\ z_m \end{bmatrix}\right) = \begin{bmatrix} 1 \\ \sigma(z_1) \\ \vdots \\ \sigma(z_m)\end{bmatrix}
$$

으로 놓자. 또한 $\tilde{\bf{h}}^{(1)} =\begin{bmatrix} 1 & h^{(1)}_1 & \cdots & h^{(1)}_{N_1}\end{bmatrix}^T$ 라 놓으면

$$
\tilde{\bf{h}}^{(1)} = \bf{\sigma}^{(1)}\left(\bf{W}^{(1)}\tilde{\bf{x}}\right)
$$

이다. 같은 방법으로 $\tilde{\bf{h}}^{(2)} = \bf{\sigma}^{(2)}\left(\bf{W}^{(2)}\tilde{\bf{h}}^{(1)}\right) = \bf{\sigma}^{(2)}\left(\bf{W}^{(2)}\bf{\sigma}^{(1)}\left(\bf{W}^{(1)}\tilde{\bf{x}}\right)\right)$ 이며, 이 과정을 반복하면 

$$
\hat{\bf{y}} =  \bf{\sigma}^{(3)} \left(\bf{W}^{(3)} \bf{\sigma}^{(2)} \left(\bf{W}^{(2)} \bf{\sigma}^{(1)}\left(\bf{W}^{(1)}\tilde{\bf{x}}\right)\right) \right)
$$

임을 안다. 즉 매개변수에 의해 정해지는 행렬 $\bf{W}^{(1)},\ldots$ 와 활성화 함수 $\bf{\sigma}$ 에 의해 입력벡터 $\bf{x}$ 에 따른 출력 벡터 $\hat{\bf{y}}$ 가 자동적으로 정해진다.

</br>

### Softmax

이산적인 표적공간에 대해 $\hat{\bf{y}}$ 를 내는 활성화 함수는 **소프트 맥스(softmax)** 함수이다. 소프트 맥스 함수 $\text{softmax}:\R^n \to \R^n$ 은 $\bf{z}=\begin{bmatrix}z_1 & \cdots & z_n\end{bmatrix}^T$ 에 대해 다음과 같이 정의된다.

$$
\text{softmax}(\bf{z}) := \dfrac{1}{\sum_{i=1}^n e^{z_i}}\begin{bmatrix}e^{z_1} \\ \vdots \\ e^{z_n}\end{bmatrix}
$$ {#eq-perceptron_softmax}

즉 $p_i = \text{softmax}(\bf{z})_i$ 일 때 $p_i \ge 0$ 이며 $\displaystyle \sum_{i=1}^{n} p_i = 1$ 로 확률의 성질을 가진다. 




