---
title: "퍼셉트론과 인공신경망"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../../../latexmacros.qmd >}}

</br>

## 퍼셉트론

### 퍼셉트론 {#sec-ML_ann_perceptron}

앞서의 [일반화된 선형 모델](classification.qmd#sec-ML_classification_generalized_linear_model) 은 원래 생물의 신경 세포(neuron) 을 수학적으로 모사한 것이다. 로지스틱 회귀에서는 입력변수와 매개변수의 내적에 대한 시그모이드 함수로 처리하여 출력했으며, 다중 클래스 로지스틱 회귀에서는 소프트 맥스 함수를 사용하였다. 이 퍼셉트론을 여러 층으로 쌓은 것을 신경망(neural network) 이라고 하며 현재 인공지능 시스템의 핵심이 되었다. 여기서 입력변수와 매개변수의 내적을 처리하는 함수를 **활성화 함수(activation function)** 라고 하며 신경망 에서는 시그모이드나 소프트 맥스 이외에 각각의 장점과 목적에 맞게 다양한 함수를 사용 할 수 있다.

![퍼셉트론](figure/perceptron01.png){#fig-perceptron_perceptron width=400}

요약하자면

&emsp;($1$) $n$ 개의 입력 변수 $\bf{x}\in \R^n$,

&emsp;($2$) $n+1$ 개의 내부 파라미터 : 1개의 bias $b$ 와 $n$ 개의 값을 갖는 $\bf{w}=\begin{bmatrix} w_1 & \ldots &w_n\end{bmatrix}^T$,

&emsp;($3$) 활성화 함수(activation function) $\sigma(z)$ : 비선형 함수.

로 정의된다. 

</br>

## 다층 신경망

### 다층 신경망 {#sec-ML_ANN_neural_network}

![다층 신경망](figure/perceptron02.png){#fig-perceptron_multi_layer_perceptron width=600}

- **입력층(input layer)** 는 퍼셉트론이 아닌 입력 데이터 벡터이다.
- 입력층과 출력층을 제외한 신경망의 각 층을 **은닉층(hidden layer)** 라고 한다.
- 첫번째 은닉층(hidden layer)의 퍼셉트론을 $h_1^{(1)},\,h_2^{(1)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론을 $h_1^{(k)},\,h_2^{(k)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론의 갯수를 $N_k$ 로 표기한다.
- 출력층은 출력값 벡터를 출력하는 마지막 층의 퍼셉트론이다.
- 신경망의 층수는 입력층을 제외하고 은닉층의 수에 출력층 1 을 더하여 표기한다. 즉 위의 그림은 4층 신경망이다.

다층 퍼셉트론은 입력값 벡터 $\bf{x}\in \R^D$ 에 대해 출력값 $\hat{\bf{y}}\in \R^K$ 를 내는 함수로 내부에 각각의 퍼셉트론의 매개변수 전체 신경망의 매개변수로 가지는 함수이다. 


입력층 $\bf{x} = \begin{bmatrix}x_1 & \cdots & x_D\end{bmatrix}^T$ 에 대해 첫번째 은닉층의 $j$ 번째 퍼셉트론의 내부 파라미터 벡터를 $\bf{w}^{(1j)}$ 와 $b^{(1)}_j$ 로 표기한다면 활성화 함수 $\sigma$ 에 대해 출력값 $y^{(1)}_j$ 는 
$$
z^{(1)}_j = \sigma \left(\bf{w}^{(1j)} \bf{\cdot x}+b^{(1)}_j\right)
$$

이다. 첫번째 은닉층에 $N_1$ 개의 퍼셉트론이 있다고 하고 행렬 $\bf{W}^{(1)}$ 와 $\tilde{\bf{h}}$ 를 다음과 같이 정의하자. 

$$
\bf{W}^{(1)} = \begin{bmatrix} b^{(1)}_1 & w^{(11)}_1 & w^{(11)}_2 & \cdots & w^{(11)}_n \\ b^{(1)}_2 & w^{(12)}_1 & w^{(12)}_2 & \cdots & w^{(12)}_n \\ \vdots & & & & \vdots \\ b^{(1)}_{N_1} & w^{(1,N_1)}_1 & w^{(1,N_1)}_2 & \cdots & w^{(1,N_1)}_n\end{bmatrix},\qquad \tilde{\bf{x}} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}.
$$ {#eq-ML_parameters_representations}

즉 $\bf{W}^{(1)}\bf{\tilde{x}}$ 의 $j$ 번째 행은 첫번째 은닉층의 $j$ 번째 퍼셉트론의 활성화 함수에 적용되는 값이다. 따라서 $\bf{z}^{(1)} = \begin{bmatrix} z^{(1)}_1 & \cdots & z^{(1)}_{N_1}\end{bmatrix}^{T}$ 에 대해

$$
\bf{z}^{(1)}= F^{(1)}\left(\bf{W}^{(1)}\tilde{\bf{x}}\right) = \begin{bmatrix} z_1^{(1)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  z_{N_1}^{(1)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_1}\right)  \end{bmatrix}
$$

놓자. 출력값은 다음 층의 입력값이 되며 bias 를 고려해야 하므로

$$
\tilde{\bf{z}}^{(1)} = \begin{bmatrix} 1 \\ \bf{z}^{(1)}\end{bmatrix} = \begin{bmatrix} 1 \\ z_1^{(1)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  z_{N_1}^{(1)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_1}\right) \end{bmatrix}
$$

라 놓으면 두번째 층에서 첫번째 층과 같은 방법으로 구성한 $\bf{W}^{(2)}$ 와 두번째 층의 퍼셉트론의 갯수 $N_2$ 에 대해 

$$
\tilde{\bf{z}}^{(2)} = F^{(2)}\left(\bf{W}^{(2)}\tilde{\bf{z}}^{(1)}\right) = \begin{bmatrix} 1 \\ z_1^{(2)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{1}\right) \\ \vdots \\  z_{N_2}^{(2)}\left(\left[\bf{W}^{(1)}\bf{\tilde{x}}\right]_{N_2}\right) \end{bmatrix}
$$

라 할 수 있다. 이것을 독같이 세번째 은닉층에도 할 수 있다. 이제 출력층을 보자. 보통 출력층에서의 활성화 함수는 다른 은닉층의 활성화 함수와 다르기 때문에 $\sigma_F$ 라고 하자. 은닉층이 3개이므로 


$$
\hat{\bf{y}} = \begin{bmatrix}  \sigma_F \left( \tilde{z}_1^{(3)}\right) \\ \vdots \\\sigma_F \left( \tilde{z}^{(3)}_{N_3} \right) \end{bmatrix}
$$ {#eq-ML_output_of_mlnn}

이다. 

</br>


### 활성화 함수


은닉층, 즉 출력층을 제외한 신경망의 퍼셉트론에 대표적으로 많이 사용되는 활성화 함수에는 다음과 같은 것이 있다. 

![대표적인 활성화 함수](figure/activation_function.png){#fig-ML_activation_functions width=450}

</br>

활성화 함수를 구별짓는 특징으로는 다음과 같은 것들이 있다.

- 함수의 치역(range) 는 어디인가?

- 매끄러운 정도는? 즉 몇번 미분 가능한가?

- 도함수를 원래 함수, 또는 쉽게 계산 할수 있는 함수를 이용하여 쉽게 계산 할 수 있는가?

</br>

이 외에 특정한 목적으로 사용되는 $\text{Maxout}$ 함수가 있다. $\bf{x}=\begin{bmatrix}x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 

$$
\text{Maxout}(\bf{x}) := \max \{x_1,\ldots,\,x_n\}
$$ {#eq-ML_maxout}

으로 정의된다.

</br>

### 비용함수 {#sec-ML_cost_function_of_nn}

정해진 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 하고 입력 $\bf{x}$ 에 대한 출력값을 $\hat{\bf{y}}=F(\bf{x};\bf{\theta})$ 라고 쓰자. 우리가 데이터를 학습시킬 때 $i$ 번째 입력 벡터를 $\bf{x}_{(i)}$ 라고 쓰면 마찬가지로 label 은 원-핫 벡터 $\bf{y}_{(i)}$ 로 표현 할 수 있으며 출력값은 $\hat{\bf{y}}_{(i)}=F\left(\bf{x}_{(i)};\bf{\theta}\right)$ 로 표기 할 수 있다. 다층신경망에 사용된 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 표기하기로 하자. 다층 신경망에서의 오차함수는 신경망에서의 출력값의 특성에 따라 최소 제곱합이나 교차 엔트로피 함수 혹은 다른 특정한 목적을 위한 함수를 사용 할 수 있다. 


</br>

### 경사 하강법

결국은 다층 신경망을 학습시키는 것은 입력 데이터 $\bf{x}^{(1)}, \,\bf{x}^{(2)},\ldots$ 와 정답(label) $\bf{y}^{(1)},\,\bf{y}^{(2)},\ldots$, 모델함수 $f(\bf{x};\bf{\theta})$ 에 대해 손실함수 $L(\bf{\theta})$ 를 최소화 하는 파라미터 $\bf{\theta}$ 를 찾는 문제이다. 문제는 입력 및 정답의 갯수 뿐만 아니라 $\bf{\theta}$ 의 갯수도 아주 크다는 것이다. 어쨌든 신경망은 일단 파라미터에 제한이 없으므로 unrestricted condtion 의 최적화 문제로 볼 수 있다. 이 경우 사용하는 것이 경사하강법이다. 매개변수 $\bf{\theta}$ 는 $k$ 번째 update 에서의 학습률 $\eta_k$ 에 대해 다음과 같이 update 된다.

$$
\bf{\theta}_{k+1} = \bf{\theta}_k - \eta_k \nabla_\bf{\theta}L(\bf{\theta}_k) 
$$


한번의 업데이트에 사용되는 데이터셋을 **배치(batch)** 라고 한다. 데이터셋의 크기가 매우 클 경우 모든 데이터셋을 포함하여 손실함수를 계산한다면, 손실함수 및 그 그래디언트를 계산하는데 계산 비용이 많이 소모된다. 만약 소수의 데이터셋만 사용한다면 계산량은 줄지만 노이즈에 취약하게 된다. 이를 위해 전체 데이터셋을 몇등분하며 각각의 등분된 부분의 데이터셋으로 파라미터를 업데이트 할 수 도 있다. 이렇게 등분된 데이터셋을 **미니 배치(mini batch)** 라고 한다. 전체의 데이터셋으로 파라미터를 업데이트 시켰을 경우를 1 **에포크(epoch)** 라고 한다.

즉 1000 개의 훈련용 데이터 셋이 있다면 1000 개의 데이터셋으로 이루어진 1개의 배치가 있다. 이것을 250 개의 미니배치로 나누어 250 개의 데이터셋을 이용하여 4번 파라미터를 업데이트 할 수도 있다. 어쨌든 1000 개의 준비된 데이터셋을 사용하여 파라미터를 업데이트 하였다면 1 에포크 의 훈련 혹은 학습을 수행한 것이다. 

</br>

배치와 미니배치에 따라 세가지 방법을 사용 할 수 있다.

 - **배치 경사 하강법 (Batch gradient discent)** : 각각의 $\bf{\theta}$ 업데이트에 대해 전체 데이터셋을 사용한다. 즉 미니배치를 사용하지 않는다.노이즈에 대해 강건하지만(영향을 적게 받지만) 계산량이 많다.
 - **확률적 경사 하강법 (Stochastic gradient discent)** : 데이터셋 하나가 미니배치를 이룬다. 즉 $\bf{\theta}$ 업데이트에 하나의 데이터셋만 사용한다. 계산량이 가장 적지만 노이즈에 취약하다.
 - **미니 배치 경사 하강법(Mini batch gradient discent)** : 전체 데이터셋보다 작고 하나보다 큰 미니배치를 사용한다.


</br>

## 역전파 (Backpropagation)

### 출력층에서의 역전파 

- 입력 벡터 $\bf{x}$ 와 $M$ 개의 은닉층 그리고 마지막 출력층으로 이루어진 신경망을 생각하자. $l$ 번째 은닉층에는 $N_l$ 개의 퍼셉트론이 있으며 출력층의 원-핫 벡터는 $N_F$ 개의 성분을 갖는다고 하자. 
- 레이블과 함께 데이터셋이 $\left(\bf{x}_{(1)},\,\bf{y}_{(1)}\right),\ldots,\,\left(\bf{x}_{(N)},\, \bf{y}_{(N)}\right)$ 이 준비되었다고 하자. 
- 내부의 모든 매개변수를 일단 $\bf{\theta}$ 로 표기하자. 
  

오차제곱합 함수 $L$ 은 @eq-ML_cost_function_for_mlnn 로 주어지며 이 때

$$
L_k(\bf{\theta}) = \dfrac{1}{2}\|\hat{\bf{y}}_{(k)}-\bf{y}_{(k)}\|^2
$$

라고 하면 (앞의 $1/2$ 는 뒤에 나올 미분을 고려하여 넣어주었다)

$$
L = \sum_{k=1}^N L_k (\bf{\theta})
$$


이며 매개변수 $\theta_m$ 에 대한 편미분은
$$
\dfrac{\partial L}{\partial \theta_m} = \sum_{k=1}^N \left(\hat{\bf{y}}_{(i)} - \bf{y}_{(i)}\right)^T \dfrac{\partial \hat{\bf{y}}^{(i)}}{\partial \theta_m} = \sum_{k=1}^N \dfrac{\partial L_k}{\partial \theta_m}
$$ {#eq-ML_general_partial_differential_of_cost_function}

이다. $\bf{z}^{(F)}=\bf{W}^{(F)}\tilde{\bf{h}}^{(M)}$ 에 대해 

$$
\hat{\bf{y}}= \bf{\sigma}^{(F)}(\bf{z}^{(F)}) = \begin{bmatrix} \sigma_1^{(F)} \left(z_1^{(F)}\right) \\ \vdots \\ \sigma_{N_F}^{(F)} \left(z_{N_F}^{(F)}\right)\end{bmatrix} = \begin{bmatrix} \sigma_1^{(F)} \left(W^{(F)}_{11}\tilde{h}^{(M)}_1 + \cdots + W^{(F)}_{1,N_M} \tilde{h}^{(M)}_{N_M}\right) \\ \vdots \\ \sigma_{N_F}^{(F)} \left(W^{(F)}_{N_F,1}\tilde{h}^{(M)}_1 + \cdots + W^{(F)}_{N_F,N_M} \tilde{h}^{(M)}_{N_M}\right) \end{bmatrix},\qquad 
$$

이므로 $W^{(F)}_{ij} = \left(\bf{W}^{(F)}\right)_{ij}$ 에 대한 편미분은

$$
\dfrac{\partial L}{\partial W^{(F)}_{ij}} = \sum_{k=1}^N \left(\hat{\bf{y}}_{(k)}-\bf{y}_{(k)}\right)_i \tilde{h}^{(M)}_j \dfrac{d\sigma_i^{(F)}(z_i)}{dz_i}
$$


여기서

$$
\sigma^{(F)}_m(\bf{z}) = \dfrac{e^{z_m}}{\sum_{q=1} e^{z_q}}
$$

이므로

$$
\dfrac{\partial \sigma^{(F)}_m(\bf{z})}{\partial z_m} = \sigma_m(\bf{z}) - (\sigma_m(\bf{z}))^2
$$

이다. $\bf{y}_{(k)}$ 나 $\hat{\bf{y}}_{(k)}$ 의 $i$ 번째 성분을 $[\cdot]_{i}$ 로 쓴다면 

$$
\dfrac{\partial L}{\partial W^{(F)}_{ij}} = \sum_{k=1}^N \left([\hat{\bf{y}}_{(k)}]_i-[\bf{y}_{(k)}]_i\right) \left( [\hat{\bf{y}}_{(k)}]_i - [\hat{\bf{y}}_{(k)}]_i^2\right)\tilde{h}^{(M)}_j
$$

이다. 즉 최소한 출력층의 매개변수 $W_{ij}^{(F)}$ 에 대해 $\dfrac{\partial L}{\partial W^{(F)}_{ij}}$ 의 값은 이미 주어진 데이터셋과 오차제곱을 위해 미리 계산한 적이 있는 모델값과 출력층의 입력값($\tilde{\bf{h}}^{(M)}$) 만 있으면 사칙연산만을 이용하여 계산 할 수 있다는 것이다. 


$$
\begin{aligned}
\dfrac{\partial L}{\partial W^{(m)}_{ij}} \sum_{k=1}^n (\hat{\bf{y}}_{(k)}- \bf{y}_{(k)})^T \dfrac{\partial \hat{\bf{y}}_{(k)}}{\partial W_{ij}^{(m)}}
\end{aligned}
$$






@eq-ML_output_of_mlnn 에서 

$$
\dfrac{\partial L}{\partial W^{(F)}_{jk}} = \sum_{i=1}^N \left(y_j^{(i)} - \hat{y}_j^{(i)}\right)  \dfrac{\partial h_j}{\partial W^{(F)}_{jk}}
$$

이며, $\bf{z}^{(F)}=\bf{W}^{(F)}\bf{\cdot}\tilde{\bf{h}}^{(M)}$ 에 대해 

$$
h_j = \dfrac{e^{z_j^{(F)}}}{\sum_{i=1}^{N_F} e^{z_i^{(F)}}} 
$$

이므로

$$
\dfrac{\partial h_j}{\partial W^{(F)}_{jk}} = W_{jk}^{(F)}(h_j - h_j^2)
$$


$$
\text{softmax}(\bf{z})_j = \bf{W}\bf{h} \dfrac{\partial h_j}{\partial }
$$