---
title: "퍼셉트론과 인공신경망"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../../../latexmacros.qmd >}}

</br>

## 퍼셉트론

### 퍼셉트론 {#sec-perceptron}

![퍼셉트론](figure/perceptron01.png){#fig-perceptron_perceptron width=400}

퍼셉트론은 

&emsp;($1$) $n$ 개의 입력 변수 $\bf{x}\in \R^n$,

&emsp;($2$) $n+1$ 개의 내부 파라미터 : 1개의 bias $b$ 와 $n$ 개의 값을 갖는 $\bf{w}=\begin{bmatrix} w_1 & \ldots &w_n\end{bmatrix}^T$,

&emsp;($3$) 활성화 함수(activation function) $\sigma(z)$ : 비선형 함수.

로 정의된다. 

$$
z=b+ \sum_{i=1}^n \bf{x}\cdot \bf{w} = b + w_1x_1 + \cdots + w_n x_n
$$

을 계산 한 후, 활성화 함수값 $\sigma(z)$ 를 출력한다.  

</br>

### 이진분류

활성화 함수 $\sigma$ 가 $\sigma:\R \to \{0,\,1\}$ 일 경우 2진분류 문제이며 $\bf{w}$ 에 의해 결정표면이 정해진다. 물론 활성화 함수가 항상 이런 형태일 필요는 없다. 


</br>

### 활성화 함수의 비선형성

일반적으로 활성화 함수는 비선형 함수이다. 즉 $\sigma (z)$ 는 $az+b$ 꼴이 아니다. 

</br>

## 다층 신경망

### 다층 신경망

![다층 퍼셉트론](figure/perceptron02.png){#fig-perceptron_multi_layer_perceptron width=600}

- 입력층(input layer) 는 퍼셉트론이 아닌 입력 데이터 벡터이다.
- 첫번째 은닉층(hidden layer)의 퍼셉트론을 $h_1^{(1)},\,h_2^{(1)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론을 $h_1^{(k)},\,h_2^{(k)},\ldots$ 와 같이 표기한다.
- $k$ 번째 은닉층의 퍼셉트론의 갯수를 $N_k$ 로 표기한다.
- 출력층은 출력값 벡터를 출력하는 마지막 층의 퍼셉트론이다.
- 신경망의 층수는 입력층을 제외하고 은닉층의 수에 출력층 1 을 더하여 표기한다. 즉 위의 그림은 4층 신경망이다.

다층 퍼셉트론은 입력값 벡터 $\bf{x}\in \R^D$ 에 대해 출력값 $\hat{\bf{y}}\in \R^K$ 를 내는 함수로 내부에 각각의 퍼셉트론의 매개변수 전체를 매개변수로 가지는 함수이다. 


입력층 $\bf{x} = \begin{bmatrix}x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 첫번째 은닉층의 $j$ 번째 퍼셉트론의 내부 파라미터 벡터를 $\bf{w}^{(1j)}$ 와 $b^{(1)}_j$ 로 표기하기로 하고

$$
z_j^{(1)} = \bf{w}^{(1j)}\bf{\cdot x} + b_j^{(1)}
$$

라고 하자. 그렇다면 활성화 함수 $\sigma$ 에 대해 출력값 $y^{(1)}_j$ 는 
$$
h^{(1)}_j = \sigma(z_j) = \sigma \left(\bf{w}^{1j} \bf{\cdot x}+b^{(1)}_j\right)
$$

이다. 첫번째 은닉층에 $N_1$ 개의 퍼셉트론이 있다고 하고 행렬 $\bf{W}^{(1)}$ 와 $\tilde{\bf{h}}$ 를 다음과 같이 정의하자. 

$$
\bf{W}^{(1)} = \begin{bmatrix} b^{(1)}_1 & w^{11}_1 & w^{11}_2 & \cdots & w^{11}_n \\ b^{(1)}_2 & w^{12}_1 & w^{12}_2 & \cdots & w^{12}_n \\ \vdots & & & & \vdots \\ b^{(1)}_{N_1} & w^{1N_1}_1 & w^{1N_1}_2 & \cdots & w^{1N_1}_n\end{bmatrix},\qquad \tilde{\bf{x}} = \begin{bmatrix} 1 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}.
$$ {#eq-ML_parameters_representations}

$\bf{z}^{(1)} = \begin{bmatrix} z^{(1)}_1 & \cdots & z^{(1)}_{N_1}\end{bmatrix}^{T}$ 에 대해

$$
\bf{z}^{(1)} =  \bf{W}^{(1)}\tilde{\bf{x}}
$$

이며 출력값을 역시 벡터로 

$$
\bf{h}^{(1)}= \bf{\sigma}\left(\bf{z}^{(1)}\right) = \begin{bmatrix} \sigma \left(z^{(1)}_1\right) \\ \vdots \\ \sigma \left({z}^{(1)}_{N_1}\right) \end{bmatrix}
$$

놓자. 출력값은 다음 층의 입력값이 되며 bias 를 고려해야 하므로

$$
\tilde{\bf{h}}^{(1)} = \begin{bmatrix} 1 \\ \bf{h}^{(1)}\end{bmatrix} = \begin{bmatrix} 1 \\ \sigma \left(z^{(1)}_1\right) \\ \vdots \\ \sigma \left(z^{(1)}_{(N_1)}\right) \end{bmatrix}
$$

라 놓으면 두번째 층에서 첫번째 층과 같은 방법으로 구성한 $\bf{W}^{(2)}$ 와 두번째 층의 퍼셉트론의 갯수 $N_2$ 에 대해 $\bf{z}^{(2)} = \bf{W}^{(2)}\bf{\cdot}\tilde{\bf{h}}^{(1)}$ 로 놓고

$$
\tilde{\bf{h}}^{(2)} = \begin{bmatrix} 1 \\ \sigma \left( z^{(2)}_1\right) \\ \vdots \\\sigma \left( z^{(2)}_{N_2} \right) \end{bmatrix}
$$

라 할 수 있다. 보통 출력층에서의 활성화 함수는 다른 은닉층의 활성화 함수와 다르기 때문에 $\sigma_F$ 라고 하면 


$$
\hat{\bf{y}} = \begin{bmatrix}  \sigma_F \left( z^{(4)}_1\right) \\ \vdots \\\sigma_F \left( z^{(4)}_{N_F} \right) \end{bmatrix}
$$

이며

$$
\begin{aligned}
\hat{y}_i = \sigma_F\left(z^{(4)}_i\right) = \sigma_F \left(\bf{W}^{(4)}_{i:} \tilde{\bf{h}}^{(3)}\right) 
\end{aligned}
$$ {#eq-ML_output_of_mlnn}

이다.


</br>


### 활성화 함수


은닉층, 즉 출력층을 제외한 신경망의 퍼셉트론에 대표적으로 많이 사용되는 활성화 함수에는 다음과 같은 것이 있다. 

![대표적인 활성화 함수](figure/activation_function.png){#fig-ML_activation_functions width=450}

</br>

활성화 함수를 구별짓는 특징으로는 다음과 같은 것들이 있다.

- 함수의 치역(range) 는 어디인가?

- 매끄러운 정도는? 즉 몇번 미분 가능한가?

- 도함수를 원래 함수, 또는 쉽게 계산 할수 있는 함수를 이용하여 쉽게 계산 할 수 있는가?

</br>

이 외에 특정한 목적으로 사용되는 $\text{Maxout}$ 함수가 있다. $\bf{x}=\begin{bmatrix}x_1 & \cdots & x_n\end{bmatrix}^T$ 에 대해 

$$
\text{Maxout}(\bf{x}) := \max \{x_1,\ldots,\,x_n\}
$$ {#eq-ML_maxout}

으로 정의된다.

</br>

#### **Softmax**

출력 벡터를 내는 이산적인 표적공간에 대해 $\hat{\bf{y}}$ 를 내는 활성화 함수는 **소프트 맥스(softmax)** 함수이다. 소프트 맥스 함수 $\text{softmax}:\R^n \to \R^n$ 은 $\bf{z}=\begin{bmatrix}z_1 & \cdots & z_n\end{bmatrix}^T$ 에 대해 다음과 같이 정의된다.

$$
\text{softmax}(\bf{z}) := \dfrac{1}{\sum_{i=1}^n e^{z_i}}\begin{bmatrix}e^{z_1} \\ \vdots \\ e^{z_n}\end{bmatrix}
$$ {#eq-perceptron_softmax}

즉 $p_i = \text{softmax}(\bf{z})_i$ 일 때 $p_i \ge 0$ 이며 $\displaystyle \sum_{i=1}^{n} p_i = 1$ 로 확률의 성질을 가진다. 


</br>

### 비용함수 {#sec-ML_cost_function_of_nn}

정해진 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 하고 입력 $\bf{x}$ 에 대한 출력값을 $\hat{\bf{y}}=\bf{h}(\bf{x};\bf{\theta})$ 라고 쓰자. 우리가 데이터를 학습시킬 때 $i$ 번째 입력 벡터를 $\bf{x}_{(i)}$ 라고 쓰면 마찬가지로 label 은 원-핫 벡터 $\bf{y}_{(i)}$ 로 표현 할 수 있으며 출력값은 $\hat{\bf{y}}_{(i)}=\bf{h}\left(\bf{x}_{(i)};\bf{\theta}\right)$ 로 표기 할 수 있다. 다층신경망에 사용된 모든 퍼셉트론의 파라미터를 $\bf{\theta}$ 라고 표기하기로 하자. 


다층신경망에서의 출력은 $\R^K$ 벡터이며 각 성분은 $[0,\,1]$ 의 범위이고 그 합이 $1$ 이다. 이경우 손실함수는 제곱합 오차 함수를 사용하자. 우선 학습을 시킬 때 

$$
L(\bf{\theta}) = \sum_{i=1}^N \left\|\bf{y}_{(i)} - \hat{\bf{y}}_{(i)}\right\|^2 = \sum_{i=1}^N \left\|\bf{y}_{(i)} - \bf{h}\left(\bf{x}_{(i)};\,\bf{\theta}\right)\right\|^2
$$ {#eq-ML_cost_function_for_mlnn}
를 사용한다.


</br>

### 경사 하강법

결국은 다층 신경망을 학습시키는 것은 입력 데이터 $\bf{x}^{(1)}, \,\bf{x}^{(2)},\ldots$ 와 정답(label) $\bf{y}^{(1)},\,\bf{y}^{(2)},\ldots$, 모델함수 $f(\bf{x};\bf{\theta})$ 에 대해 손실함수 $L(\bf{\theta})$ 를 최소화 하는 파라미터 $\bf{\theta}$ 를 찾는 문제이다. 문제는 입력 및 정답의 갯수 뿐만 아니라 $\bf{\theta}$ 의 갯수도 아주 크다는 것이다. 어쨌든 신경망은 일단 파라미터에 제한이 없으므로 unrestricted condtion 의 최적화 문제로 볼 수 있다. 이 경우 사용하는 것이 경사하강법이다. 매개변수 $\bf{\theta}$ 는 $k$ 번째 update 에서의 학습률 $\eta_k$ 에 대해 다음과 같이 update 된다.

$$
\bf{\theta}_{k+1} = \bf{\theta}_k - \eta_k \nabla_\bf{\theta}L(\bf{\theta}_k) 
$$


한번의 업데이트에 사용되는 데이터셋을 **배치(batch)** 라고 한다. 데이터셋의 크기가 매우 클 경우 모든 데이터셋을 포함하여 손실함수를 계산한다면, 손실함수 및 그 그래디언트를 계산하는데 계산 비용이 많이 소모된다. 만약 소수의 데이터셋만 사용한다면 계산량은 줄지만 노이즈에 취약하게 된다. 이를 위해 전체 데이터셋을 몇등분하며 각각의 등분된 부분의 데이터셋으로 파라미터를 업데이트 할 수 도 있다. 이렇게 등분된 데이터셋을 **미니 배치(mini batch)** 라고 한다. 전체의 데이터셋으로 파라미터를 업데이트 시켰을 경우를 1 **에포크(epoch)** 라고 한다.

즉 1000 개의 훈련용 데이터 셋이 있다면 1000 개의 데이터셋으로 이루어진 1개의 배치가 있다. 이것을 250 개의 미니배치로 나누어 250 개의 데이터셋을 이용하여 4번 파라미터를 업데이트 할 수도 있다. 어쨌든 1000 개의 준비된 데이터셋을 사용하여 파라미터를 업데이트 하였다면 1 에포크 의 훈련 혹은 학습을 수행한 것이다. 

</br>

배치와 미니배치에 따라 세가지 방법을 사용 할 수 있다.

 - **배치 경사 하강법 (Batch gradient discent)** : 각각의 $\bf{\theta}$ 업데이트에 대해 전체 데이터셋을 사용한다. 즉 미니배치를 사용하지 않는다.노이즈에 대해 강건하지만(영향을 적게 받지만) 계산량이 많다.
 - **확률적 경사 하강법 (Stochastic gradient discent)** : 데이터셋 하나가 미니배치를 이룬다. 즉 $\bf{\theta}$ 업데이트에 하나의 데이터셋만 사용한다. 계산량이 가장 적지만 노이즈에 취약하다.
 - **미니 배치 경사 하강법(Mini batch gradient discent)** : 전체 데이터셋보다 작고 하나보다 큰 미니배치를 사용한다.


</br>

## 역전파 (Backpropagation)

### 출력층에서의 역전파 

- 입력 벡터 $\bf{x}$ 와 $M$ 개의 은닉층 그리고 마지막 출력층으로 이루어진 신경망을 생각하자. $l$ 번째 은닉층에는 $N_l$ 개의 퍼셉트론이 있으며 출력층의 원-핫 벡터는 $N_F$ 개의 성분을 갖는다고 하자. 
- 레이블과 함께 데이터셋이 $\left(\bf{x}_{(1)},\,\bf{y}_{(1)}\right),\ldots,\,\left(\bf{x}_{(N)},\, \bf{y}_{(N)}\right)$ 이 준비되었다고 하자. 
- 내부의 모든 매개변수를 일단 $\bf{\theta}$ 로 표기하자. 
  

오차제곱합 함수 $L$ 은 @eq-ML_cost_function_for_mlnn 로 주어지며 이 때

$$
L_k(\bf{\theta}) = \dfrac{1}{2}\|\hat{\bf{y}}_{(k)}-\bf{y}_{(k)}\|^2
$$

라고 하면 (앞의 $1/2$ 는 뒤에 나올 미분을 고려하여 넣어주었다)

$$
L = \sum_{k=1}^N L_k (\bf{\theta})
$$


이며 매개변수 $\theta_m$ 에 대한 편미분은
$$
\dfrac{\partial L}{\partial \theta_m} = \sum_{k=1}^N \left(\hat{\bf{y}}_{(i)} - \bf{y}_{(i)}\right)^T \dfrac{\partial \hat{\bf{y}}^{(i)}}{\partial \theta_m} = \sum_{k=1}^N \dfrac{\partial L_k}{\partial \theta_m}
$$ {#eq-ML_general_partial_differential_of_cost_function}

이다. $\bf{z}^{(F)}=\bf{W}^{(F)}\tilde{\bf{h}}^{(M)}$ 에 대해 

$$
\hat{\bf{y}}= \bf{\sigma}^{(F)}(\bf{z}^{(F)}) = \begin{bmatrix} \sigma_1^{(F)} \left(z_1^{(F)}\right) \\ \vdots \\ \sigma_{N_F}^{(F)} \left(z_{N_F}^{(F)}\right)\end{bmatrix} = \begin{bmatrix} \sigma_1^{(F)} \left(W^{(F)}_{11}\tilde{h}^{(M)}_1 + \cdots + W^{(F)}_{1,N_M} \tilde{h}^{(M)}_{N_M}\right) \\ \vdots \\ \sigma_{N_F}^{(F)} \left(W^{(F)}_{N_F,1}\tilde{h}^{(M)}_1 + \cdots + W^{(F)}_{N_F,N_M} \tilde{h}^{(M)}_{N_M}\right) \end{bmatrix},\qquad 
$$

이므로 $W^{(F)}_{ij} = \left(\bf{W}^{(F)}\right)_{ij}$ 에 대한 편미분은

$$
\dfrac{\partial L}{\partial W^{(F)}_{ij}} = \sum_{k=1}^N \left(\hat{\bf{y}}_{(k)}-\bf{y}_{(k)}\right)_i \tilde{h}^{(M)}_j \dfrac{d\sigma_i^{(F)}(z_i)}{dz_i}
$$


여기서

$$
\sigma^{(F)}_m(\bf{z}) = \dfrac{e^{z_m}}{\sum_{q=1} e^{z_q}}
$$

이므로

$$
\dfrac{\partial \sigma^{(F)}_m(\bf{z})}{\partial z_m} = \sigma_m(\bf{z}) - (\sigma_m(\bf{z}))^2
$$

이다. $\bf{y}_{(k)}$ 나 $\hat{\bf{y}}_{(k)}$ 의 $i$ 번째 성분을 $[\cdot]_{i}$ 로 쓴다면 

$$
\dfrac{\partial L}{\partial W^{(F)}_{ij}} = \sum_{k=1}^N \left([\hat{\bf{y}}_{(k)}]_i-[\bf{y}_{(k)}]_i\right) \left( [\hat{\bf{y}}_{(k)}]_i - [\hat{\bf{y}}_{(k)}]_i^2\right)\tilde{h}^{(M)}_j
$$

이다. 즉 최소한 출력층의 매개변수 $W_{ij}^{(F)}$ 에 대해 $\dfrac{\partial L}{\partial W^{(F)}_{ij}}$ 의 값은 이미 주어진 데이터셋과 오차제곱을 위해 미리 계산한 적이 있는 모델값과 출력층의 입력값($\tilde{\bf{h}}^{(M)}$) 만 있으면 사칙연산만을 이용하여 계산 할 수 있다는 것이다. 


$$
\begin{aligned}
\dfrac{\partial L}{\partial W^{(m)}_{ij}} \sum_{k=1}^n (\hat{\bf{y}}_{(k)}- \bf{y}_{(k)})^T \dfrac{\partial \hat{\bf{y}}_{(k)}}{\partial W_{ij}^{(m)}}
\end{aligned}
$$






@eq-ML_output_of_mlnn 에서 

$$
\dfrac{\partial L}{\partial W^{(F)}_{jk}} = \sum_{i=1}^N \left(y_j^{(i)} - \hat{y}_j^{(i)}\right)  \dfrac{\partial h_j}{\partial W^{(F)}_{jk}}
$$

이며, $\bf{z}^{(F)}=\bf{W}^{(F)}\bf{\cdot}\tilde{\bf{h}}^{(M)}$ 에 대해 

$$
h_j = \dfrac{e^{z_j^{(F)}}}{\sum_{i=1}^{N_F} e^{z_i^{(F)}}} 
$$

이므로

$$
\dfrac{\partial h_j}{\partial W^{(F)}_{jk}} = W_{jk}^{(F)}(h_j - h_j^2)
$$


$$
\text{softmax}(\bf{z})_j = \bf{W}\bf{h} \dfrac{\partial h_j}{\partial }
$$