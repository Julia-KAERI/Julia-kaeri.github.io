---
title: "선형 회귀"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

{{< include ../../../latexmacros.qmd >}}

</br>


## 선형회귀

### 선형회귀와 설계행렬 {#sec-regression_design_matrix}

$n$ 차원 벡터 입력변수 $\bf{x}_1,\ldots,\,\bf{x}_m \in \mathcal{M}_n(\R)$ 에 대한 값 $y_1,\,\ldots,\,y_m$ 을 가장 잘 기술하는 함수

$$
f(\bf{x};\bf{\theta})= \begin{bmatrix} \theta_1 & \cdots & \theta_n\end{bmatrix}^T\bf{x} + \theta_0
$$

를 찾는 문제를 선형 회귀 (linear regression) 문제라고 한다. 이제 $\bf{x}_i \in \mathcal{M}_n(\R)$ 의 $j$ 번째 성분을 $(\bf{x}_i)_j$ 라고 표기하기로 하고 아래와 같이 행렬과 벡터를 정의한다.

$$
\bf{\Phi}:= \begin{bmatrix} 1 & (\bf{x}_1)_1 & \cdots & (\bf{x}_1)_n \\ 1 & (\bf{x}_2)_1 & \cdots & (\bf{x}_2)_n \\ \vdots & & & \vdots \\ 1 & (\bf{x}_m)_1 & \cdots & (\bf{x}_m)_n  \end{bmatrix}, \qquad \bf{\theta} := \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix},\qquad \bf{y} := \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
$$

이 때 $\bf{\Phi}$ 를 **설계 행렬(design matrix)** 이라고 한다. 그리고

$$
\bf{e}:=\bf{y}-\bf{\Phi \theta}
$$

라고 하면 $\bf{e}\in \mathcal{M}_m(\R)$ 의 $i$ 번째 성분 $e_i = y_i - f(\bf{x}_i;\bf{\theta})$ 이다. 결국 선형 회귀 문제는 벡터 $\bf{e}$ 의 크기, 즉 노름을 최소화 하는 $\bf{\theta}^\ast$ 를 찾는 문제로 바뀌며 다음과 같이 기술 될 수 있다.

$$
\bf{\theta}^\ast = \arg\min \left\|\bf{y}- \bf{\Phi \theta}\right\|
$$ {#eq-linear_regression_master_equation}


많은 경우 노름은 유클리드 노름, 즉 $L_2$ 노름을 말하지만 $L_1$ 혹은 $L_\infty$ 노름도 사용 될 수 있다. $L_p$ 노름을 사용했을 경우 @eq-linear_regression_master_equation 는 다음과 같이 표기된다.

$$
\bf{\theta}^\ast = \arg\min \left\|\bf{y}- \bf{\Phi \theta}\right\|_p
$$ {#eq-linear_regression_master_equation_with_p}

벡터 $\bf{v}\in \mathcal{M}_k(\R)$ 의 $L_p$ 노름 $\|\bf{v}\|_p$ 는 다음과 같이 정의된다.

$$
\|\bf{v}\|_p := \left(\sum_{i=1}^k |v_i|^p\right)^{1/p}
$$

즉 $\|\bf{v}\|_1 = \sum |v_i|$ 이며 $\|\bf{v}\|_2 = \displaystyle \sqrt{\sum_{i=1}^k |v_i|^2}$ 는 유클리드 노름이다. $L_\infty$ 는 $p\to \infty$ 극한으로 정의되며 $\|\bf{v}\|_\infty =\displaystyle \max_{i=1,\ldots,\,k} |v_k|$ 이다. 만약 데이터 가운데 소위 튀는 데이터가 있다면 $L_\infty$ 노름이 가장 큰 영향을 받으며 $\|\bf{v}\|_1$ 노름이 가장 영향을 적게 받는다. 일반적으로 $L_2$ 노름을 가장 많이 사용하며 튀는 데이터가 있을 경우 $L_1$ 노름을 사용하기도 한다. $L_2$ 노름의 경우

$$
\bf{\theta}^\ast = \arg \min \|\bf{y}-\bf{\Phi\theta}\|_2 = \arg \min\|\bf{y}-\bf{\Phi\theta}\|_2^2
$$ {#eq-linear_regression_master_equation_with_L2}

이며 $\arg \min\|\bf{y}-\bf{\Phi\theta}\|_2^2$ 가 $\sqrt{\cdots}$ 가 없어 미분 계산이 훨씬 간단해 지기 때문에 노름의 제곱을 많이 사용한다.
 
</br>

### 최소제곱합 {#sec-regression_least_square_sum}

@eq-linear_regression_master_equation_with_L2 은 보통 데이터와 모델값의 오차에 대한 제곱합 오차 함수 $E(\bf{\theta})$ 이다. 

$$
E(\bf{\theta})=\sum_{i=1}^m (y_i - f(\bf{x}_i;\bf{\theta}))^2 = \|\bf{y}-\bf{\Phi \theta}\|_2^2.
$$

이 때 $E(\bf{\theta})$ 가 미분가능하다면 $E(\bf{\theta})$ 를 최소로 하는 $\bf{\theta}^\ast$ 에 대해 $\nabla_\bf{\theta} E(\bf{\theta}^\ast)=\bf{0}$ 이어야 한다. 

$$
\begin{aligned}
\nabla_\bf{\theta}E(\bf{\theta}) &= -2\bf{y}^T\bf{\Phi}-\left(\bf{\theta}^\ast\right)^T(\bf{\Phi}^T\bf{\Phi}) = \bf{0}
\end{aligned}
$$

이로부터

$$
\bf{\theta}^\ast = \left(\bf{\Phi}^T\bf{\Phi}\right)^{-1}\bf{\Phi}^T \bf{y}
$$ {#eq-linear_regression_solution_L2_norm}

를 얻는다.

</br>


## 다양한 선형 회귀

### 1차원 선형 회귀 {#sec-regression_1d_linear}

1차원 선형회귀에서 설계행렬 $\bf{\Phi}$ 는 다음과 같다.

$$
\bf{\Phi} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \\ 1 & x_n\end{bmatrix}.
$$

그리고 

$$
\begin{aligned}
\bf{\theta}=\begin{bmatrix} \theta_0 \\ \theta_1 \end{bmatrix}, 
\qquad \bf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}, \qquad
\bf{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n\end{bmatrix}
\end{aligned}
$$

에 대해

$$
\theta^\ast = \begin{bmatrix} \theta_1^\ast \\ \theta_2^\ast\end{bmatrix} = \arg \min_{\bf{\theta}} \|\bf{y} - \bf{\Phi \theta}\|^2 
$$

이다. $L_2$ 노름의 경우 $\bf{\theta}^\ast$ 는 @eq-linear_regression_solution_L2_norm 로 부터 다음과 같다는 것을 안다.

$$
\bf{\theta}^\ast = \left(\bf{\Phi}^T\bf{\Phi}\right)^{-1}\bf{\Phi}^T \bf{y}.
$$

</br>

### 다항 회귀 {#sec-regression_polynomial_regression}

1 차원 변수 $x$ 에 대한 다항식 $f(x;\bf{\theta}) = \theta_0 + \theta_1 x + \cdots + \theta_m x^m$ 로 데이터를 설명한다고 하자. 우리는 $\R$ 에서 정의된 $\{1,\,x,\,x^2,\ldots,\,x^m\}$ 이 선형독립임을 안다. 즉 다항회귀를 다차원 선형회귀로 간주 할 수 있다. 그렇다면 설계행렬 $\bf{\Phi}$ 는 다음과 같다.

$$
\bf{\Phi} = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^m \\ 1 & x_2 & x_2^2 & \cdots & x_2^m \\ \vdots & & & &\vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^m\end{bmatrix}.
$$

$L_2$ 노름에 대해서라면 역시 $\bf{\theta}^\ast = \left(\bf{\Phi}^T\bf{\Phi}\right)^{-1}\bf{\Phi}^T \bf{y}$ 를 구하면 된다.

</br>

아래 코드와 그림은 데이터를 3차 다항식으로 회귀시킨다.


```julia
using LinearAlgebra, CairoMakie,

xs = 1:0.5:10
f(x, p) = p[1]  + p[2] * x^1 +p[3] * x^2 + p[4] * x^3
p0 = [-9.0, 11.8, -3.0, 0.2]
f(x) = f(x, p0)
ys = f.(xs) .+ 1.0*(rand(length(xs)) .- 0.5);
Φ = [x^k for x in xs, k in 0:3]
θ = inv(Φ'*Φ) * Φ' * ys
f1(x) = f(x, θ)
```

![다항 회귀](notebooks/polyreg.png){#fig-regression_polyreg width=400}

</br>

### 비선형 회귀 {#sec-regression_nonlinear_regression}








