---
title: "Approximation"

number-sections: true
number-depth: 3
crossref:
  chapters: false
---

## 최소 자승법

### 선형 최소 자승법

아래 그림과 같이 $\{(x_i,\,y_i) : i = 1,\ldots, N\}$ 가 주어졌을 때 이 데이터에 가장 근접하는 선형 방정식 $y=ax+b$ 를 찾아야 할 때가 있다.

![선형 최소 자승법](ch13_approximations/images/fig-linear_approximation-1.png){#fig-linear_lieast_seuare width=250}

아래와 같은 함수 $E(a,\,b)$ 를 보자.
$$
E(a, b) = \sum_{i=1}^N \left[ y_i - (ax_i + b)\right]^2
$$

이 함수는 모든 구간에서 미분가능하다는 매우 좋은 성질을 가지고 있다. 또한 $a,\,b$ 값이 상당히 벗어날 경우 매우 커지며, $y_i$ 가 $ax_i +b$ 와 일치할 경우 $0$ 이 된다. 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a} &= -2\sum_{i=1}^N x_i(y_i - ax_i - b), \\
\dfrac{\partial E}{\partial b} &= -2\sum_{i=1}^N (y_i - ax_i -b), \\
\dfrac{\partial^2 E}{\partial a \partial a }& = \sum_{i=1}^N 2{x_i}^2, \\
\dfrac{\partial^2 E}{\partial a \partial b} &= \sum_{i=1}^N 2x_i, \\
\dfrac{\partial^2 E}{\partial b \partial b}&= 2N
\end{aligned}
$$

이며 이로부터 해세 행렬 (Hassian matrix)

$$
\boldsymbol{H} = \begin{bmatrix} \dfrac{\partial^2 E}{\partial a^2 } & \dfrac{\partial^2 E}{\partial a \partial b} \\ \dfrac{\partial^2 E}{\partial a \partial b} & \dfrac{\partial^2 E}{\partial b^2} \end{bmatrix} = \begin{bmatrix} {\displaystyle \sum_{i=1}^N 2{x_i}^2} & {\displaystyle \sum_{i=1}^N 2x_i}, \\ {\displaystyle \sum_{i=1}^N 2x_i} & 2N \end{bmatrix}
$$

을 얻는다. 위 해세 행렬의 행렬식은

$$
D = \det (\boldsymbol{H}) = 4 \left[N\left(\sum_i^N {x_i}^2 \right)- \left(\sum_i^N x_i\right)^2\right]
$$


이며, 코시-슈바르츠 부등식으로 $D\ge 0$ 이며 $D=0$ 일 때는 $x_1 = \cdots = x_i$ 일 때임을 보일 수 있다. 이것을 제외하면 $D>0$ 이며 $E(a,\,b)$ 는 극소값을 가진다는 것을 안다. 극소값을 가질 때는 $\dfrac{\partial E}{\partial a}= \dfrac{\partial E}{\partial b}=0$ 일 때이므로, 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a}= 0 &\implies \sum_i x_i y_i - a \left(\sum_i {x_i}^2\right) - b \left(\sum_{i}x_i\right) = 0, \\
\dfrac{\partial E}{\partial b} = 0 &\implies \sum_i y_i - a\left(\sum_i x_i\right) - b N = 0
\end{aligned}
$$

이다. 위 식은 $a,\,b$ 에 대한 연립방정식이며 이것을 풀면 다음과 같다.

$$
a = \dfrac{N \left( \displaystyle \sum_{i=1}^N x_i y_i\right) - \left(\displaystyle \sum_{i=1}^N x_i\right) \left( \displaystyle \sum_{i=1}^N y_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, \qquad 
b=  \dfrac{\left( \displaystyle \sum_{i=1}^N {x_i}^2\right)  \left( \displaystyle \sum_{i=1}^N y_i\right) - \left(\displaystyle \sum_{i=1}^N x_iy_i\right) \left( \displaystyle \sum_{i=1}^N x_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, 
$$

언뜻 복잡해 보이지만 프로그래밍 입장에서 보면 베열의 합, 베열의 곱의 합, 베열의 제곱의 합, 배열의 합의 제곱에 대한 사칙연산 뿐이며, 쉽게 코딩 할 수 있다. 

</br>

### 다항식 최소 자승법

데이터 $\{(x_k,\,y_k):k=1,\,N\}$ ($x_1<x_2<\cdots < x_N$) 과 $n$ 차 다항식

$$
p_n (x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0
$$

에 대해 

$$
E(a_n,\ldots,\,a_0) = \sum_{k=1}^N \left[ y_k - p_n(x_k)\right]^2
$$

를 최소로 하는 $a_0,\ldots,\,a_n$ 을 찾는다. 이 경우 $N=n+1$ 이면 다항식을 이용한 보간법이 되어 $E=0$ 이 되도록 하는 $a_0,\ldots,\,a_n$ 이 존재한다는 것을 안다. 보통 최소자승법은 $N>n+1$ 일 경우의 가장 좋은 다항식 $p_n$ 을 찾는 것을 말한다. 여기서도 $n<N-1$ 임을 가정하겠다. 

$$
\begin{aligned}
E(a_n,\ldots,\,a_0) &= \sum_{k=1}^N {y_k}^2 -2 \sum_{k=1}^N y_k p_n(x_i) + \sum_{k=1}^N (p_n(x_k))^2\\
&= \sum_{k=1}^N {y_k}^2 - 2 \sum_{k=1}^N \sum_{i=0}^n a_i y_k {x_k}^j + \sum_{k=1}^N \sum_{i,\,j=1}^n a_i a_j {x_k}^{i+j} \\
&= \sum_{k=1}^N {y_k}^2 - 2\sum_{i=0}^n \left(\sum_{k=1}^N (x_k)^i y_k\right) a_i + \sum_{i, j=0}^n \left(\sum_{k=1}^N {x_k}^{i+j}\right) a_i a_j
\end{aligned} 
$$

이며 $E$ 를 최소로 하는 극값은 $i=0,\ldots,n$ 에 대해 

$$
\dfrac{\partial E}{\partial a_i}=0 \implies \sum_{k=1}^N \left[ (x_k)^i y_k- \sum_{j=0}^n  (x_k)^{i+j} a_j\right] = 0
$$ {#eq-least_square_for_polynomial}

를 만족해야 한다. 이제 행렬 $\boldsymbol{X} \in \mathcal{M}_{N\times (n+1)}(\mathbb{F}),\,\boldsymbol{a} \in\mathcal{M}_{n+1}(\mathbb{F}),\,\boldsymbol{y}\in \mathcal{M}_N(\mathbb{F})$ 를 다음과 같이 정의하자.

$$
X_{ij} = (x_i)^{j-1},\qquad \boldsymbol{a} = \begin{bmatrix} a_0 & \cdots & a_n\end{bmatrix}^T, \qquad \boldsymbol{y} = \begin{bmatrix} y_1 & \cdots & y_N \end{bmatrix}
$$

그렇다면,

$$
(\boldsymbol{X}^T\boldsymbol{X})_{ij} = \sum_{k=1}^N X_{ki}X_{kj} = \sum_{k=1}^N x_{k}^{i+j-2}
$$

이므로 @eq-least_square_for_polynomial 는 다음과 같은 선형방정식이 된다.

$$
\boldsymbol{X}^T\boldsymbol{Xa} = \boldsymbol{X}^T\boldsymbol{y}
$$


</br>

::: {#exr-nullity_of_X}

위에서 정의된 $\boldsymbol{X}$ 에 대해 다음을 보여라.

&emsp; ($1$) $\text{nullity} (\boldsymbol{X})=0$.

&emsp; ($2$) $\boldsymbol{X}^T\boldsymbol{X}$ 는 가역행렬이다.

:::

::: {.solution}

($1$) $\text{nullity} (\boldsymbol{x}) > 0$ 이라면 어떤 nontrivial 한 $\boldsymbol{a}$ 에 대해 $\boldsymbol{Xa}=\boldsymbol{0}$ 이어야 한다. 그렇다면, $\sum_{i=0}^n {x_k}^i a_i = 0$ ($i=1,\ldots,\,N$) 인데 $N>n+1$ 조건에서 $n$ 차 방정식이 $N > n+1$ 개의 근을 갖는다는 이야기이므로 모순이다. 우리는 이미 $x_1 < \cdots < x_N$ 을 가정했다. 따라서 $\boldsymbol{Xa}=\boldsymbol{0}$ 을 만족하는 $\boldsymbol{a}$ 는 $\boldsymbol{0}$ 뿐이다.

($2$) $\boldsymbol{X}^T\boldsymbol{X}$ 는 대칭행렬이며 $\text{nullity}(\boldsymbol{X})=0$ 이므로 non-trivial 한 벡터 $\boldsymbol{a}$ 에 대해 $\boldsymbol{a}^T\boldsymbol{X}^T\boldsymbol{Xa} = \|\boldsymbol{Xa}\|^2>0$ 이다. 따라서 $\boldsymbol{X}^T\boldsymbol{X}$ 는 가역이다.
:::

</br>

@exr-nullity_of_X 에 따라 $\boldsymbol{a} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 로 정해진다. 


