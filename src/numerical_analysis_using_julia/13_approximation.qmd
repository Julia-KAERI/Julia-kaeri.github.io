---
title: "Approximation"

number-sections: true
number-depth: 2
crossref:
  chapters: false
---

## 최소 자승법

### 선형 최소 자승법

아래 그림과 같이 $\{(x_i,\,y_i) : i = 1,\ldots, N\}$ 가 주어졌을 때 이 데이터에 가장 근접하는 선형 방정식 $y=ax+b$ 를 찾아야 할 때가 있다.

![선형 최소 자승법](ch13_approximations/images/fig-linear_approximation-1.png){#fig-linear_lieast_seuare width=250}

아래와 같은 함수 $E(a,\,b)$ 를 보자.
$$
E(a, b) = \sum_{i=1}^N \left[ y_i - (ax_i + b)\right]^2
$$

이 함수는 모든 구간에서 미분가능하다는 매우 좋은 성질을 가지고 있다. 또한 $a,\,b$ 값이 상당히 벗어날 경우 매우 커지며, $y_i$ 가 $ax_i +b$ 와 일치할 경우 $0$ 이 된다. 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a} &= -2\sum_{i=1}^N x_i(y_i - ax_i - b), \\
\dfrac{\partial E}{\partial b} &= -2\sum_{i=1}^N (y_i - ax_i -b), \\
\dfrac{\partial^2 E}{\partial a \partial a }& = \sum_{i=1}^N 2{x_i}^2, \\
\dfrac{\partial^2 E}{\partial a \partial b} &= \sum_{i=1}^N 2x_i, \\
\dfrac{\partial^2 E}{\partial b \partial b}&= 2N
\end{aligned}
$$

이며 이로부터 해세 행렬 (Hassian matrix)

$$
\boldsymbol{H} = \begin{bmatrix} \dfrac{\partial^2 E}{\partial a^2 } & \dfrac{\partial^2 E}{\partial a \partial b} \\ \dfrac{\partial^2 E}{\partial a \partial b} & \dfrac{\partial^2 E}{\partial b^2} \end{bmatrix} = \begin{bmatrix} {\displaystyle \sum_{i=1}^N 2{x_i}^2} & {\displaystyle \sum_{i=1}^N 2x_i}, \\ {\displaystyle \sum_{i=1}^N 2x_i} & 2N \end{bmatrix}
$$

을 얻는다. 위 해세 행렬의 행렬식은

$$
D = \det (\boldsymbol{H}) = 4 \left[N\left(\sum_i^N {x_i}^2 \right)- \left(\sum_i^N x_i\right)^2\right]
$$


이며, 코시-슈바르츠 부등식으로 $D\ge 0$ 이며 $D=0$ 일 때는 $x_1 = \cdots = x_i$ 일 때임을 보일 수 있다. 이것을 제외하면 $D>0$ 이며 $E(a,\,b)$ 는 극소값을 가진다는 것을 안다. 극소값을 가질 때는 $\dfrac{\partial E}{\partial a}= \dfrac{\partial E}{\partial b}=0$ 일 때이므로, 

$$
\begin{aligned}
\dfrac{\partial E}{\partial a}= 0 &\implies \sum_i x_i y_i - a \left(\sum_i {x_i}^2\right) - b \left(\sum_{i}x_i\right) = 0, \\
\dfrac{\partial E}{\partial b} = 0 &\implies \sum_i y_i - a\left(\sum_i x_i\right) - b N = 0
\end{aligned}
$$

이다. 위 식은 $a,\,b$ 에 대한 연립방정식이며 이것을 풀면 다음과 같다.

$$
a = \dfrac{N \left( \displaystyle \sum_{i=1}^N x_i y_i\right) - \left(\displaystyle \sum_{i=1}^N x_i\right) \left( \displaystyle \sum_{i=1}^N y_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, \qquad 
b=  \dfrac{\left( \displaystyle \sum_{i=1}^N {x_i}^2\right)  \left( \displaystyle \sum_{i=1}^N y_i\right) - \left(\displaystyle \sum_{i=1}^N x_iy_i\right) \left( \displaystyle \sum_{i=1}^N x_i\right)}{N \left(\displaystyle \sum_{i=1}^N {x_i}^2\right) - \left(\displaystyle \sum_{i=1}^N x_i\right)^2}, 
$$

언뜻 복잡해 보이지만 프로그래밍 입장에서 보면 베열의 합, 베열의 곱의 합, 베열의 제곱의 합, 배열의 합의 제곱에 대한 사칙연산 뿐이며, 쉽게 코딩 할 수 있다. 

</br>

### 다항식 최소 자승법

데이터 $\{(x_k,\,y_k):k=1,\,N\}$ 과 $n$ 차 다항식

$$
p_n (x) = a_n x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0
$$

에 대해 

$$
E(a_n,\ldots,\,a_0) = \sum_{k=1}^N \left[ y_k - p_n(x_k)\right]^2
$$

를 최소로 하는 $a_0,\ldots,\,a_n$ 을 찾는다. 이 경우 $N=n+1$ 이면 다항식을 이용한 보간법이 되어 $E=0$ 이 되도록 하는 $a_0,\ldots,\,a_n$ 이 존재한다는 것을 안다. 보통 최소자승법은 $N>n+1$ 일 경우의 가장 좋은 다항식 $p_n$ 을 찾는 것을 말한다. 여기서도 $n<N-1$ 임을 가정하겠다. 

$$
\begin{aligned}
E(a_n,\ldots,\,a_0) &= \sum_{k=1}^N {y_k}^2 -2 \sum_{k=1}^N y_k p_n(x_i) + \sum_{k=1}^N (p_n(x_k))^2\\
&= \sum_{k=1}^N {y_k}^2 - 2 \sum_{k=1}^N \sum_{i=0}^n a_i y_k {x_k}^j + \sum_{k=1}^N \sum_{i,\,j=1}^n a_i a_j {x_k}^{i+j} \\
&= \sum_{k=1}^N {y_k}^2 - 2\sum_{i=0}^n \left(\sum_{k=1}^N (x_k)^i y_k\right) a_i + \sum_{i, j=0}^n \left(\sum_{k=1}^N {x_k}^{i+j}\right) a_i a_j
\end{aligned} 
$$

이며 $E$ 를 최소로 하는 극값은 $i=0,\ldots,n$ 에 대해 

$$
\dfrac{\partial E}{\partial a_i}=0 \implies \sum_{k=1}^N \left[ (x_k)^i y_k- \sum_{j=0}^n  (x_k)^{i+j} a_j\right] = 0
$$ {#eq-least_square_for_polynomial}

를 만족해야 한다. 행렬 $\boldsymbol{X}$ 와 열벡터 $\boldsymbol{b}$ 를 다음과 같이 정의하자. 행렬의 인덱스는 $1$ 부터 시작하지만 위의 $i,\,j = 0,\,1,\ldots$ 이므로 수식이 약간 번잡스러워진다. 

$$
X_{ij} = \sum_{k=1}^N {x_k}^{(i-1)+(j-1)},\qquad b_{i} = \sum_{k=1}^N (x_k)^{i-1} y_k
$$ {#eq-least_square_for_polynomial_linear_equation_matrix-definition}

또한 $\boldsymbol{a} = \begin{bmatrix} a_0 & a_1 & \cdots & a_n\end{bmatrix}^T$ 라고 하면 @eq-least_square_for_polynomial 은 다음과 같은 선형방정식이 된다.

$$
\boldsymbol{Xa} = \boldsymbol{b}
$$ {#eq-least_square_for_polynomial_linear_equation}

</br>

::: {.callout-tip appearance="minimal" icon="false"}


::: {#prp-least_square_for_polynomial_linear_equation}
@eq-least_square_for_polynomial_linear_equation 의 $\boldsymbol{X}$ 는 가역행렬이다.


::: {.proof}

@eq-least_square_for_polynomial_linear_equation_matrix-definition 에 의해 정의된 $X$ 는 $(n+1)\times (n+1)$ 대칭행렬이다. 

:::

:::

@prp-least_square_for_polynomial_linear_equation 